{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMoa9meGXqAgMQIkk7LAHLU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Step 1: Mounting Google Drive\n"],"metadata":{"id":"jIGC_82kB3Tf"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xw-aYOgzZMMh","executionInfo":{"status":"ok","timestamp":1745469388209,"user_tz":300,"elapsed":15968,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"8e747fd4-c030-4913-870a-58f3b8a63727"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n","data\t\t\t\tLICENSE\t\t qa_pairs   wandb\n","deployment\t\t\tmodels\t\t README.md\n","eval_predictions_baseline.json\tnotebooks\t results\n","gpt4o_judgments_baseline.json\tproject_plan.md  scripts\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Navigate to the repo folder\n","%cd /content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n","\n","# List repo contents\n","!ls"]},{"cell_type":"markdown","source":["## Step 2: Importing Necessary Libraries and Functions"],"metadata":{"id":"RbEuW5A6GRmj"}},{"cell_type":"code","source":["import sys\n","import os, re\n","import json\n","sys.path.append('./scripts')\n","\n","from arxiv_scraper_rag import search_arxiv, filter_papers, download_papers"],"metadata":{"id":"PTRAAReoB6nr","executionInfo":{"status":"ok","timestamp":1745470378775,"user_tz":300,"elapsed":2,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## Step 3: Define Search Queries and Relevance Keywords\n","\n","### Query & Relevance Setup for RAG Corpus Construction\n","\n","Before retrieving papers from arXiv, we define two critical components:\n","\n","1. **Multi-query Search Space (`queries`)**  \n","   These are natural language search phrases used to query the arXiv API.  \n","   Each query represents a high-level concept or subdomain within instruction fine-tuning, parameter-efficient methods, and retrieval-augmented generation (RAG).  \n","   The purpose is to **maximize coverage** across recent papers that might use different terminologies for similar ideas.\n","\n","2. **Keyword List for Relevance Scoring (`keywords`)**  \n","   After retrieving papers via the queries, we **score each paper’s title + abstract** by counting keyword hits.  \n","   The higher the match count, the more relevant the paper is assumed to be.  \n","   This step ensures we prioritize **domain-specific, high-signal documents** for downstream chunking and retrieval.\n","\n","> These two lists control both **breadth of retrieval** (queries) and **precision of filtering** (keywords)."],"metadata":{"id":"z7gdBqamGYRj"}},{"cell_type":"code","source":["# Define your multi‐query search space\n","queries = [\n","    \"instruction tuning\",\n","    \"transformer fine-tuning\",\n","    \"parameter-efficient fine-tuning\"\n","    \"retrieval augmented generation\",\n","    \"LoRA\",\n","    \"prompt tuning\"\n","]"],"metadata":{"id":"CB-6OKiCC3EK","executionInfo":{"status":"ok","timestamp":1745469695408,"user_tz":300,"elapsed":3,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Define the keywords for relevance scoring\n","keywords = [\n","    \"LoRA\", \"QLoRA\", \"parameter-efficient\", \"supervised fine-tuning\",\n","    \"adapter\", \"SFT\", \"instruction tuning\", \"prompt tuning\",\n","    \"RAG\", \"semantic search\", \"vector database\"\n","]"],"metadata":{"id":"iM45mfbqC8Dx","executionInfo":{"status":"ok","timestamp":1745469696271,"user_tz":300,"elapsed":3,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Step 4: Retrieve Papers from arXiv for Each Query\n","\n","For each of the predefined queries, we call the `search_arxiv()` function to retrieve up to 100 recent papers from the arXiv API.\n","\n","- We restrict the search to relevant subfields using arXiv categories:  \n","  - `cs.LG` (Machine Learning)  \n","  - `cs.CL` (Computation and Language)  \n","  - `cs.AI` (Artificial Intelligence)\n","\n","Each query returns a set of papers, which we then append to a master list (`all_papers`).  \n","Note that this step **may introduce duplicates** (e.g., the same paper retrieved by multiple queries), which we’ll address during the filtering phase.\n","\n","> This step prioritizes **recall**—gathering as many potentially relevant documents as possible before scoring and ranking them."],"metadata":{"id":"r_cw8SnrHh2P"}},{"cell_type":"code","source":["# Aggregate raw hits across all queries\n","all_papers = []\n","for q in queries:\n","    papers = search_arxiv(\n","        query=q,\n","        max_results=100,               # fetch up to 100 per query\n","        start=0,\n","        categories=['cs.LG', 'cs.CL', 'cs.AI']  # narrow to ML/NLP/AI\n","    )\n","    print(f\"[QUERY] '{q}' → retrieved {len(papers)} papers\")\n","    all_papers.extend(papers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U1_r7QWmDA8f","executionInfo":{"status":"ok","timestamp":1745469712529,"user_tz":300,"elapsed":14912,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"7577db88-97bf-4846-a9cd-c49a2e0b6c47"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[QUERY] 'instruction tuning' → retrieved 74 papers\n","[QUERY] 'transformer fine-tuning' → retrieved 100 papers\n","[QUERY] 'parameter-efficient fine-tuningretrieval augmented generation' → retrieved 100 papers\n","[QUERY] 'LoRA' → retrieved 100 papers\n","[QUERY] 'prompt tuning' → retrieved 100 papers\n"]}]},{"cell_type":"markdown","source":["## Step 5: Filter, Deduplicate, and Rank Papers by Relevance + Recency\n","\n","Once all papers have been retrieved, we pass them into the `filter_papers()` function, which performs the following steps:\n","\n","1. **Date Filtering**  \n","   - Only include papers published in **2021 or later**, ensuring the corpus reflects **recent advancements** in the field (e.g., LoRA, QLoRA, RAG).\n","\n","2. **Keyword-Based Relevance Scoring**  \n","   - Each paper is scored based on the number of keyword matches in its **title and abstract**.\n","   - This produces a `relevance_score` that captures domain-specific salience.\n","\n","3. **Deduplication by arXiv ID**  \n","   - Ensures that the same paper (possibly retrieved by multiple queries) appears **only once** in the final set.\n","\n","4. **Ranking**  \n","   - Papers are sorted first by `relevance_score`, then by publication year (descending), prioritizing **both relevance and recency**.\n","\n","5. **Top-k Selection**  \n","   - Retain only the **top 75 papers**, forming a high-quality, focused RAG corpus for downstream chunking and embedding.\n","\n","> This stage compresses a noisy, redundant retrieval space into a **dense, curated knowledge base**—tailored for semantic retrieval."],"metadata":{"id":"g9ZNxLpAIC61"}},{"cell_type":"code","source":["# Filter, dedupe & rank by relevance + recency\n","filtered = filter_papers(\n","    papers=all_papers,\n","    keywords=keywords,\n","    year_from=2021,    # only papers 2021+\n","    top_k=75           # keep top 75 most relevant\n",")\n","print(f\"[FILTER] {len(filtered)} papers passed keyword+date filter\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PvBLfupQThkO","executionInfo":{"status":"ok","timestamp":1745470059523,"user_tz":300,"elapsed":11,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"e50d9744-c670-4fc9-c4dc-6bbf55419c2b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[FILTER] 75 papers passed keyword+date filter\n"]}]},{"cell_type":"code","source":["# Exclude any papers you’ve already used for fine-tuning\n","ft_dir = \"./data/QA_corpus/\"\n","existing_ids = {\n","    re.sub(r\"\\.pdf$\", \"\", fname)\n","    for fname in os.listdir(ft_dir)\n","    if fname.endswith(\".pdf\")\n","}\n","before = len(filtered)\n","filtered = [p for p in filtered if p['arxiv_id'] not in existing_ids]\n","print(f\"[EXCLUDE] removed {before-len(filtered)} fine-tuning papers, {len(filtered)} remain\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--lYkkWDVsGI","executionInfo":{"status":"ok","timestamp":1745470172910,"user_tz":300,"elapsed":251,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"d4109408-a07e-464d-abc8-08d4196b782f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[EXCLUDE] removed 0 fine-tuning papers, 75 remain\n"]}]},{"cell_type":"markdown","source":["## Step 6: Downloading the Filtered Papers\n"],"metadata":{"id":"_Ew4cuvrIW8B"}},{"cell_type":"code","source":["# Download the remaining PDFs\n","download_dir = \"./data/rag_corpus/\"\n","os.makedirs(download_dir, exist_ok=True)\n","download_papers(\n","    papers=filtered,\n","    download_dir=download_dir,\n","    sleep_time=1.0\n",")"],"metadata":{"id":"i0Mjx3zOWRx7","executionInfo":{"status":"ok","timestamp":1745470291319,"user_tz":300,"elapsed":116697,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Step 7: Saving Metadata\n","\n","After filtering and ranking, we serialize the final list of selected papers (`filtered`) into a JSON file.\n","\n","- Each entry in this file includes metadata such as:\n","  - `arxiv_id`\n","  - `title`\n","  - `abstract` (summary)\n","  - `publication date`\n","  - `relevance score`\n","  - `PDF download URL`\n","\n","This metadata file will serve as a **canonical reference** in the next notebook, where we:\n","1. Extract text from the downloaded PDFs\n","2. Chunk them into semantically meaningful units\n","3. Embed them for retrieval in our RAG system\n","\n","The JSON is saved to:\n","\n","./data/rag_corpus/metadata.json\n","\n","> This ensures the corpus is **reproducible**, **traceable**, and ready for semantic indexing in FAISS."],"metadata":{"id":"TtzOziVpIpfy"}},{"cell_type":"code","source":["meta_path = \"./data/rag_corpus/metadata.json\"\n","with open(meta_path, \"w\") as f:\n","    json.dump(filtered, f, indent=2)\n","print(f\"[DONE] Saved metadata ({len(filtered)} papers) → {meta_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DWHurqg1Wuka","executionInfo":{"status":"ok","timestamp":1745470329759,"user_tz":300,"elapsed":109,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"57c48c75-9774-43e8-e61f-40a79ebb2bd9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[DONE] Saved metadata (75 papers) → ./data/rag_corpus/metadata.json\n"]}]}]}