{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyMKrW6bauo0nZam5NNR+9UF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"29f8a8b8e7b548b895bde9abec4f185b":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_1c8165a77a4141b5855531f91d564043"}},"09f3e49179ca43e1b7bc1455585ddd85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e51a0c722cf04371a6d7b2b2583c8aa8","placeholder":"​","style":"IPY_MODEL_3113456bd24c429cba833030e86075b6","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"bd3e22a201654676b02bc38860bdcc4a":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_4793f3a33ec049f2b7e78617cd842176","placeholder":"​","style":"IPY_MODEL_7b8505834e0e470280c78b94db9db657","value":""}},"7c7eccfdf03040c1bcc6660bf7919dd4":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_fb82e1dbe427442f97c10ae04626454d","style":"IPY_MODEL_63f4e26a4b8c4f60bac84b713dc847fe","value":true}},"ad02cb449c77425a99cb5bde5201d7e1":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_252c6b905ca34277a40d865efeb754a2","style":"IPY_MODEL_cd420507919d4ef2898250e47cc2de96","tooltip":""}},"58df9cdb4f0e48e49bf4d5f72a2fd23f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49d3f38c04214d669b6fbc8110ed747c","placeholder":"​","style":"IPY_MODEL_8bd0fac94b0542fb82cbde36ee551887","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"1c8165a77a4141b5855531f91d564043":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"e51a0c722cf04371a6d7b2b2583c8aa8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3113456bd24c429cba833030e86075b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4793f3a33ec049f2b7e78617cd842176":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b8505834e0e470280c78b94db9db657":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb82e1dbe427442f97c10ae04626454d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63f4e26a4b8c4f60bac84b713dc847fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"252c6b905ca34277a40d865efeb754a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd420507919d4ef2898250e47cc2de96":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"49d3f38c04214d669b6fbc8110ed747c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bd0fac94b0542fb82cbde36ee551887":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fe2249a1baa4279b8639a0680ba2191":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e0357f44a4545e7a5cbbc088b0211ea","placeholder":"​","style":"IPY_MODEL_3a2e62bf3964411f80e9f176848f7aa9","value":"Connecting..."}},"5e0357f44a4545e7a5cbbc088b0211ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a2e62bf3964411f80e9f176848f7aa9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## Step 1: Mounting Google Drive and Importing Dependencies"],"metadata":{"id":"pjyIhCJRTY7N"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sj5RanQqTPni","executionInfo":{"status":"ok","timestamp":1744509576799,"user_tz":300,"elapsed":19262,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"98015746-6c98-47df-a9eb-52cde51180da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n","data\t    LICENSE  notebooks\t      qa_pairs\t results  wandb\n","deployment  models   project_plan.md  README.md  scripts\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive, files\n","drive.mount('/content/drive')\n","\n","# Navigate to the repo folder\n","%cd /content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n","\n","# List repo contents\n","!ls"]},{"cell_type":"code","source":["!pip install datasets --quiet"],"metadata":{"id":"RRheopVw4Bfk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install bert-score  --quiet"],"metadata":{"id":"XboO7X715ySp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade openai"],"metadata":{"id":"g-Ztb04BUm7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","from peft import PeftModel\n","from huggingface_hub import login\n","import torch\n","from datasets import load_from_disk, load_dataset\n","import json\n","import os\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from google.colab import userdata\n","from google import generativeai as genai\n","import time\n","import openai\n","from openai import OpenAI\n","from getpass import getpass\n","import random\n","import numpy as np\n","from bert_score import score as bertscore"],"metadata":{"id":"x_jHqWxnTd07","executionInfo":{"status":"ok","timestamp":1744526941278,"user_tz":300,"elapsed":18,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":["## Step 2: Merging the Saved Adapter Weights and Base Model"],"metadata":{"id":"1e_mvCMyUzU-"}},{"cell_type":"code","source":["login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["29f8a8b8e7b548b895bde9abec4f185b","09f3e49179ca43e1b7bc1455585ddd85","bd3e22a201654676b02bc38860bdcc4a","7c7eccfdf03040c1bcc6660bf7919dd4","ad02cb449c77425a99cb5bde5201d7e1","58df9cdb4f0e48e49bf4d5f72a2fd23f","1c8165a77a4141b5855531f91d564043","e51a0c722cf04371a6d7b2b2583c8aa8","3113456bd24c429cba833030e86075b6","4793f3a33ec049f2b7e78617cd842176","7b8505834e0e470280c78b94db9db657","fb82e1dbe427442f97c10ae04626454d","63f4e26a4b8c4f60bac84b713dc847fe","252c6b905ca34277a40d865efeb754a2","cd420507919d4ef2898250e47cc2de96","49d3f38c04214d669b6fbc8110ed747c","8bd0fac94b0542fb82cbde36ee551887","4fe2249a1baa4279b8639a0680ba2191","5e0357f44a4545e7a5cbbc088b0211ea","3a2e62bf3964411f80e9f176848f7aa9"]},"id":"Vg1wr_iYV7Vg","executionInfo":{"status":"ok","timestamp":1744509605869,"user_tz":300,"elapsed":125,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"5c2e580d-2fbf-40b2-fa4a-c93d234a436d"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29f8a8b8e7b548b895bde9abec4f185b"}},"metadata":{}}]},{"cell_type":"code","source":["# Load adapter + base model\n","adapter_model_path = \"./models/finetuned-mistral\"\n","base_model_name = \"mistralai/Mistral-7B-Instruct-v0.3\""],"metadata":{"id":"VMjF_NLhUZ36","executionInfo":{"status":"ok","timestamp":1744338256061,"user_tz":300,"elapsed":3,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16, device_map=\"auto\")"],"metadata":{"id":"Pm7Az6UwU9eo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This line of code takes the base model (model) and enhances it with task-specific knowledge from the adapter weights stored at adapter_model_path. It effectively loads a fine-tuned version of the LLM, ready for use."],"metadata":{"id":"fHX5WWNBsiaG"}},{"cell_type":"code","source":["# Apply LoRA adapter\n","model = PeftModel.from_pretrained(model, adapter_model_path)"],"metadata":{"id":"EVn61nLcVCFF","executionInfo":{"status":"ok","timestamp":1744338841799,"user_tz":300,"elapsed":1622,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Merge LoRA weights into base model\n","model = model.merge_and_unload()"],"metadata":{"id":"d3pse0j4smn1","executionInfo":{"status":"ok","timestamp":1744338947518,"user_tz":300,"elapsed":3092,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LqSbrDoZtAEI","executionInfo":{"status":"ok","timestamp":1744338955192,"user_tz":300,"elapsed":7,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"50432a0c-ebbf-4dcc-bb24-58f14ef386d5"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MistralForCausalLM(\n","  (model): MistralModel(\n","    (embed_tokens): Embedding(32768, 4096)\n","    (layers): ModuleList(\n","      (0-31): 32 x MistralDecoderLayer(\n","        (self_attn): MistralAttention(\n","          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","        )\n","        (mlp): MistralMLP(\n","          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n","        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n","      )\n","    )\n","    (norm): MistralRMSNorm((4096,), eps=1e-05)\n","    (rotary_emb): MistralRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Save merged model\n","merged_path = \"./models/merged-finetuned-mistral\"\n","model.save_pretrained(merged_path, safe_serialization=True)  # saves as safetensors\n","tokenizer.save_pretrained(merged_path)\n","\n","print(f\"Merged model saved at: {merged_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DhMCmCSstCsq","executionInfo":{"status":"ok","timestamp":1744339348378,"user_tz":300,"elapsed":68647,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"b0fbda60-72ac-42b5-9e9e-053e0147b072"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Merged model saved at: ./models/merged-finetuned-mistral\n"]}]},{"cell_type":"markdown","source":["## Step 3: Loading the Validation Set for Evaluation"],"metadata":{"id":"wGLSLsxg3wLf"}},{"cell_type":"code","source":["eval_path = \"./data/eval.jsonl\"\n","eval_pairs = []\n","\n","with open(eval_path, \"r\") as f:\n","    for line in f:\n","        eval_pairs.append(json.loads(line.strip()))\n","\n","print(f\"Loaded {len(eval_pairs)} QA pairs for evaluation.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2ryl8dpyqln","executionInfo":{"status":"ok","timestamp":1744509626678,"user_tz":300,"elapsed":558,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"5441ae6b-7d98-494c-e3ca-704f17e83982"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 30 QA pairs for evaluation.\n"]}]},{"cell_type":"code","source":["eval_pairs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VH3Y241My33Y","executionInfo":{"status":"ok","timestamp":1744509628197,"user_tz":300,"elapsed":4,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"162a9a01-fa02-4c87-eb2f-a88a44703a5c"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'question': 'What is the primary innovation introduced by the LoRI method for parameter-efficient fine-tuning?',\n","  'answer': 'LoRI introduces a novel approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks, thereby significantly reducing trainable parameters while minimizing cross-task interference.'},\n"," {'question': 'How does LoRI reduce the number of trainable parameters compared to traditional LoRA?',\n","  'answer': 'LoRI reduces the number of trainable parameters by keeping matrix A fixed as a random projection and sparsifying matrix B using task-specific masks, eliminating the need to train both matrices and reducing redundancy.'},\n"," {'question': 'Why is sparsity in matrix B important in LoRI?',\n","  'answer': 'Sparsity in matrix B enables LoRI to retain only the most critical elements necessary for adaptation, reducing parameter count and mitigating cross-task interference during adapter merging and continual learning.'},\n"," {'question': 'How does LoRI improve the process of merging adapters in multi-task scenarios?',\n","  'answer': 'LoRI enables more effective adapter merging by using fixed, randomly initialized projection matrices A, which maps task-specific adapters into approximately orthogonal subspaces, thus reducing parameter interference.'},\n"," {'question': 'What mechanism does LoRI use to mitigate catastrophic forgetting in continual learning?',\n","  'answer': 'LoRI mitigates catastrophic forgetting by applying task-specific sparse masks to matrix B, which isolates parameter updates across tasks and preserves knowledge from previous adaptations, including safety alignment.'},\n"," {'question': 'On what benchmark did LoRI with 90% sparsity in B outperform LoRA, and by how much?',\n","  'answer': 'LoRI with 90% sparsity in B outperformed LoRA by 17.3% on the HumanEval benchmark using the Llama-3 model.'},\n"," {'question': 'How does LoRI compare to full fine-tuning and other PEFT methods in terms of performance and efficiency?',\n","  'answer': 'LoRI matches or outperforms full fine-tuning and other PEFT methods across multiple domains while using up to 95% fewer trainable parameters than LoRA, demonstrating both high performance and high efficiency.'},\n"," {'question': \"What types of tasks were used to evaluate LoRI's effectiveness?\",\n","  'answer': 'LoRI was evaluated on a diverse set of tasks, including natural language understanding, mathematical reasoning, code generation, and safety alignment.'},\n"," {'question': 'What potential future directions do the authors propose for extending LoRI?',\n","  'answer': 'The authors suggest exploring structured sparsity patterns like block sparsity or head pruning and adapting LoRI to multi-modal models such as diffusion and vision-language systems.'},\n"," {'question': 'What is the broader significance of LoRI in the context of PEFT and LLM deployment?',\n","  'answer': 'LoRI provides a lightweight, modular, and scalable solution for adapting LLMs with minimal overhead, making it particularly suited for multi-task learning, safety-critical alignment, and efficient deployment on resource-constrained hardware.'},\n"," {'question': 'What are the core limitations of traditional LoRA methods that ElaLoRA seeks to address?',\n","  'answer': 'ElaLoRA addresses two key limitations of traditional LoRA: the fixed rank allocation across layers, which overlooks the layer-specific importance, and the inability to adapt ranks dynamically during training, which can lead to suboptimal parameter efficiency.'},\n"," {'question': 'Describe the three core components of the ElaLoRA framework.',\n","  'answer': \"ElaLoRA's architecture consists of: (1) an SVD-based adaptation strategy for matrix decomposition, (2) an importance score calculation mechanism based on loss gradients to assess rank relevance, and (3) a dynamic rank learning algorithm that reallocates ranks periodically during training to optimize layer-wise adaptation.\"},\n"," {'question': 'How does ElaLoRA’s adaptive strategy improve performance under limited parameter budgets?',\n","  'answer': 'ElaLoRA reallocates computational resources to the most critical layers by pruning less important ranks and expanding ranks in essential layers, thus achieving higher performance even under smaller parameter budgets—for example, outperforming other PEFT methods with r=2 compared to their r=4 settings.'},\n"," {'question': 'In what way does ElaLoRA achieve better task alignment during fine-tuning?',\n","  'answer': 'ElaLoRA uses gradient-derived importance scores to identify which layers contribute most to task-specific learning, allowing the model to allocate more capacity to those layers and thus improving task alignment and learning efficiency.'},\n"," {'question': 'What experimental evidence supports the superiority of ElaLoRA over other PEFT methods?',\n","  'answer': 'Experiments across NLU, NLG, and vision benchmarks show that ElaLoRA consistently outperforms state-of-the-art PEFT methods in accuracy, particularly under constrained parameter budgets, and demonstrates better GLUE benchmark performance even with fewer trainable parameters.'},\n"," {'question': 'Why is ElaLoRA particularly well-suited for resource-constrained environments?',\n","  'answer': \"ElaLoRA's dynamic pruning and expansion mechanism ensures that only the most essential ranks are trained, reducing memory usage and computational cost while maintaining high performance, making it ideal for low-resource scenarios.\"},\n"," {'question': 'How does the final rank distribution in ElaLoRA reflect its adaptive learning process?',\n","  'answer': 'ElaLoRA’s final rank distribution reveals that higher ranks are allocated to layers deemed more important via importance scores, confirming that the model dynamically concentrates learning capacity on the most impactful parts of the network.'},\n"," {'question': 'What are the broader implications of ElaLoRA’s design for the future of fine-tuning large models?',\n","  'answer': 'ElaLoRA’s design shows that adaptive, importance-based rank allocation can significantly improve parameter efficiency without sacrificing accuracy, suggesting a paradigm shift toward more intelligent and resource-aware fine-tuning strategies.'},\n"," {'question': 'What distinguishes ElaLoRA from prior dynamic rank methods like AdaLoRA or IncreLoRA?',\n","  'answer': 'While AdaLoRA and IncreLoRA either prune or expand ranks, ElaLoRA is the first to implement both pruning and expansion dynamically during training, offering a more flexible and principled mechanism for allocating parameter capacity.'},\n"," {'question': 'Why is parameter-efficient fine-tuning increasingly important in the LLM landscape?',\n","  'answer': 'As LLMs grow in size, full fine-tuning becomes prohibitively expensive, especially for domain-specific or low-resource settings. PEFT methods like ElaLoRA offer a practical solution by enabling adaptation with minimal compute and storage costs.'},\n"," {'question': \"What is the primary goal of the study presented in 'Beyond QA Pairs'?\",\n","  'answer': 'The study aims to assess the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain-specific facts into LLMs, focusing on the impact of QA pair categorization and synthetic dataset generation techniques.'},\n"," {'question': 'How are QA pairs categorized in this study, and what is the purpose of this categorization?',\n","  'answer': 'QA pairs are classified into ‘Factual’ and ‘Conceptual’ categories using a BERT-based classifier. The purpose is to investigate how the nature of QA pairs influences the effectiveness of PEFT.'},\n"," {'question': 'What were the findings regarding models trained on conceptual vs factual QA datasets?',\n","  'answer': 'Models fine-tuned on conceptual datasets consistently outperformed those trained on factual datasets across multiple evaluations.'},\n"," {'question': 'Which synthetic dataset generation techniques are evaluated in this work, and which one performs better?',\n","  'answer': 'The paper evaluates D-RAG and D-Naive synthetic data generation methods. D-Naive outperformed D-RAG in fine-tuning effectiveness, largely due to better retrieval performance.'},\n"," {'question': 'What was the significance of the product recommendation task in the data center domain?',\n","  'answer': 'The task served as a practical demonstration showing that a Llama-2 7B model fine-tuned with PEFT on just 1,000 instruction-based QA pairs significantly outperformed the baseline in generating product recommendations.'},\n"," {'question': 'Why do the authors argue that PEFT may not be optimal for factual embedding?',\n","  'answer': 'The study shows that while PEFT is effective for instruction tuning, it struggles with embedding factual information as effectively, likely due to its limited parameter update scope.'},\n"," {'question': 'What conclusions do the authors draw about the volume versus quality of QA data in PEFT?',\n","  'answer': 'They conclude that sheer quantity of QA pairs is insufficient; quality and conceptual depth are far more critical for successful PEFT.'},\n"," {'question': 'What limitations of D-RAG were identified in the study?',\n","  'answer': \"D-RAG's limitations were attributed to the poor performance of its underlying vector database retriever, leading to suboptimal training data quality.\"},\n"," {'question': 'How do the authors suggest future research should improve PEFT for fact embedding?',\n","  'answer': 'Future research should explore improvements in retrieval systems used by D-RAG, and consider more refined QA classification and data generation strategies.'},\n"," {'question': 'What is the key insight this paper contributes to the field of LLM fine-tuning?',\n","  'answer': \"The paper highlights that PEFT's success hinges more on dataset composition—especially the conceptual quality of QA pairs—than on volume alone, and that careful use-case targeting is essential.\"}]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## Step 4: Loading Saved Model"],"metadata":{"id":"_gQOeJoJ5DQr"}},{"cell_type":"code","source":["# Load merged model and tokenizer\n","model_path = \"./models/merged-finetuned-mistral\""],"metadata":{"id":"xpkrec3H454B","executionInfo":{"status":"ok","timestamp":1744509631310,"user_tz":300,"elapsed":4,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_path)"],"metadata":{"id":"CIfPdnfI5Rc0","executionInfo":{"status":"ok","timestamp":1744509634323,"user_tz":300,"elapsed":2494,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"id":"c0FVNsNL6whC","executionInfo":{"status":"ok","timestamp":1744510328411,"user_tz":300,"elapsed":7,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")"],"metadata":{"id":"1WWz8FMIxCiy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","print(\"Model and tokenizer successfully loaded.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1oew5PsTxjYb","executionInfo":{"status":"ok","timestamp":1744509827011,"user_tz":300,"elapsed":44,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"3b0e129b-0893-410a-9c49-7beff67c94f1"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model and tokenizer successfully loaded.\n"]}]},{"cell_type":"markdown","source":["## Step 5: Generating Model Predictions\n","\n","In this section, we generate predictions from our **fine-tuned Mistral model** using a *closed-book* approach—i.e., without feeding any external context into the model at inference time.\n","\n","We define a `generate_answer()` function that:\n","- Encodes the input prompt (`\"Question: ... \\nAnswer:\"`) using the tokenizer.\n","- Applies greedy decoding (`do_sample=False`) with a `max_new_tokens` limit.\n","- Truncates the input to 512 tokens to avoid overflow.\n","- Returns only the portion of the output after `\"Answer:\"`.\n","\n","For each QA pair in our `eval_pairs` list, we:\n","- Format the question as an instruction-style prompt.\n","- Generate a prediction using our fine-tuned model.\n","- Save the original question, the reference answer, and the predicted answer into a results list.\n","\n","Finally, we write the `results` to a file named `eval_predictions.json`, which will be used in subsequent evaluation steps (e.g., BLEU scoring, qualitative analysis, LLM-as-a-judge).\n","\n",">  The last line:\n","> ```python\n","> with open(\"eval_predictions.json\", \"w\") as f:\n",">     json.dump(results, f, indent=2)\n","> ```\n","> ...saves the full evaluation results to disk as a human-readable `.json` file. This allows us to persist the predictions for further metric computation and analysis—even if the Colab session resets."],"metadata":{"id":"3ibn-bRu9VYv"}},{"cell_type":"code","source":["def generate_answer(question: str) -> str:\n","    inputs = tokenizer(\n","        question,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        max_length=512\n","    ).to(model.device)\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids=inputs[\"input_ids\"],\n","            attention_mask=inputs[\"attention_mask\"],\n","            max_new_tokens=256,\n","            do_sample=False  # greedy decoding\n","        )\n","\n","    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return decoded.split(\"Answer:\")[-1].strip()"],"metadata":{"id":"jbWtC2fh7NZU","executionInfo":{"status":"ok","timestamp":1744510362259,"user_tz":300,"elapsed":3,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["results = []\n","\n","for item in eval_pairs:\n","    question = item[\"question\"]\n","    reference = item[\"answer\"]\n","    prediction = generate_answer(f\"Question: {question}\\nAnswer:\")\n","    results.append({\n","        \"question\": question,\n","        \"reference\": reference,\n","        \"prediction\": prediction\n","    })"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifW73_TN3DoJ","executionInfo":{"status":"ok","timestamp":1744510706956,"user_tz":300,"elapsed":343532,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"cb907f44-ae2a-43fe-e376-bc8c2b530432"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]}]},{"cell_type":"code","source":["with open(\"eval_predictions.json\", \"w\") as f:\n","    json.dump(results, f, indent=2)"],"metadata":{"id":"BtLwPlKP3dNc","executionInfo":{"status":"ok","timestamp":1744510888780,"user_tz":300,"elapsed":2,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["output_path = \"./data/evaluation/eval_predictions_closed_book.json\"\n","\n","os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","with open(output_path, \"w\") as f:\n","    json.dump(results, f, indent=2)\n","\n","print(f\"Predictions saved to {output_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wf1m9fWlyd1R","executionInfo":{"status":"ok","timestamp":1744525739711,"user_tz":300,"elapsed":8,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"05907339-cb77-4d74-a858-8884892bb599"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions saved to ./data/evaluation/eval_predictions_closed_book.json\n"]}]},{"cell_type":"markdown","source":["## Step 6: BLEU Score Evaluation\n","\n","In this section, we evaluate our fine-tuned model using the **BLEU (Bilingual Evaluation Understudy)** score, a standard metric for evaluating the quality of generated text by comparing it to a reference answer.\n","\n","### What is BLEU?\n","BLEU measures *n-gram overlap* between the model's prediction and the reference answer:\n","- **BLEU-1**: unigram overlap (word-level similarity)\n","- **BLEU-2**: bigram overlap (2-word chunks)\n","- **BLEU-3**: trigram overlap\n","- **BLEU-4**: 4-gram overlap (more stringent)\n","\n","### Components of the Code:\n","- `weights=(1, 0, 0, 0)`: Measures unigram overlap only (BLEU-1).\n","- `smoothing_function=method1`: Prevents the BLEU score from dropping to 0 when there are no exact n-gram matches. This is useful for short or paraphrased responses.\n","- We iterate over our evaluation dataset and compute BLEU-1 through BLEU-4 for each response.\n","\n","### Limitations:\n","BLEU is a **surface-level** metric:\n","- It penalizes paraphrasing.\n","- It doesn't understand meaning—only *form*.\n","- It is useful for rough comparison, but **not sufficient alone** to assess model quality.\n","\n","Hence, we will also perform **qualitative evaluation** using *LLM-as-a-Judge* in the next step.\n","\n","### Results:\n","Our average scores were:\n","- BLEU-1: *e.g., 0.22*\n","- BLEU-2: *e.g., 0.11*\n","- BLEU-3: *e.g., 0.07*\n","- BLEU-4: *e.g., 0.05*\n","\n","These low scores are expected, since:\n","1. The evaluation was *closed-book* (no document context).\n","2. The questions were from **papers published in 2025**, after the model's training cutoff.\n","3. The model had not seen any of these papers during fine-tuning.\n","\n","**Conclusion**: BLEU gives us a sense of lexical similarity. In high-difficulty settings like this one, it must be supplemented with qualitative evaluation."],"metadata":{"id":"sIcsJ2TpAIq1"}},{"cell_type":"code","source":["# Load model predictions\n","with open(\"eval_predictions.json\", \"r\") as f:\n","    eval_results = json.load(f)"],"metadata":{"id":"08KumwGo86pL","executionInfo":{"status":"ok","timestamp":1744511771932,"user_tz":300,"elapsed":4,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["smooth = SmoothingFunction().method1"],"metadata":{"id":"6yWEYXxtASRV","executionInfo":{"status":"ok","timestamp":1744511779386,"user_tz":300,"elapsed":3,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["bleu_scores = {\n","    \"BLEU-1\": [],\n","    \"BLEU-2\": [],\n","    \"BLEU-3\": [],\n","    \"BLEU-4\": []\n","}"],"metadata":{"id":"pcg4AdDaAUFM","executionInfo":{"status":"ok","timestamp":1744511788658,"user_tz":300,"elapsed":1,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["for item in eval_results:\n","    reference = item[\"reference\"].split()\n","    prediction = item[\"prediction\"].split()\n","\n","    bleu_scores[\"BLEU-1\"].append(sentence_bleu([reference], prediction, weights=(1, 0, 0, 0), smoothing_function=smooth))\n","    bleu_scores[\"BLEU-2\"].append(sentence_bleu([reference], prediction, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth))\n","    bleu_scores[\"BLEU-3\"].append(sentence_bleu([reference], prediction, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth))\n","    bleu_scores[\"BLEU-4\"].append(sentence_bleu([reference], prediction, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth))\n","\n","# Compute average BLEU scores\n","avg_bleu_scores = {metric: round(sum(scores)/len(scores), 4) for metric, scores in bleu_scores.items()}\n","print(\"Average BLEU Scores:\", avg_bleu_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M06y9B0tAVt4","executionInfo":{"status":"ok","timestamp":1744515792880,"user_tz":300,"elapsed":11,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"85dd35de-11b6-4552-abec-5fda226c04ce"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Average BLEU Scores: {'BLEU-1': 0.2244, 'BLEU-2': 0.1126, 'BLEU-3': 0.0787, 'BLEU-4': 0.0514}\n"]}]},{"cell_type":"markdown","source":["## Step 7: Using GPT-4o as LLM-as-a-Judge (OpenAI Evaluation)\n","\n","In this section, we use **GPT-4o**—a state-of-the-art model from OpenAI—as a neutral third-party judge to evaluate the quality of our model’s predictions against ground truth answers. This is part of the **LLM-as-a-Judge** evaluation methodology, which is growing in popularity as a way to assess open-ended outputs where metrics like BLEU or ROUGE may fall short.\n","\n","**What this section does:**\n","\n","- Loads model predictions from `eval_predictions.json`\n","- Uses a GPT-4o prompt that provides:\n","  - The question\n","  - The model's generated answer\n","  - The reference (ground-truth) answer\n","- Asks GPT-4o to score the generated answer on a **scale from 1 to 5**, considering relevance, correctness, completeness, and style\n","- Stores all outputs in `gpt4o_judged_results.json` for analysis\n","\n","**Key Functions:**\n","\n","- `ask_gpt_judge()` → Sends a prompt to GPT-4o via the OpenAI API and returns a numeric score\n","- `judged_results` → A list of evaluation records including the question, reference, model prediction, and GPT-4o's score\n","- `np.mean()` → Used at the end to compute the **average evaluation score** across all QA pairs\n","\n","**Why use GPT-4o?**\n","\n","Because LLMs are best judged by **other LLMs** capable of contextual understanding. GPT-4o has been shown to be highly consistent and reliable in comparative evaluations.\n","\n","This evaluation complements our BLEU score by offering a **semantic and qualitative assessment**, helping us better understand the strengths and weaknesses of our fine-tuned model.\n","\n","---"],"metadata":{"id":"oIXfTM6CVIID"}},{"cell_type":"code","source":["os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key:\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jShGfL_ZqmD","executionInfo":{"status":"ok","timestamp":1744520897483,"user_tz":300,"elapsed":15035,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"af159f10-a6ef-4bb2-8984-b0177251e887"},"execution_count":47,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI API key:··········\n"]}]},{"cell_type":"code","source":["openai.api_key = os.environ[\"OPENAI_API_KEY\"]"],"metadata":{"id":"w-7NY2tojCgZ","executionInfo":{"status":"ok","timestamp":1744520907263,"user_tz":300,"elapsed":2,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["# Load the API key from environment variable\n","client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"],"metadata":{"id":"TSetREoMkrN5","executionInfo":{"status":"ok","timestamp":1744521342211,"user_tz":300,"elapsed":354,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["def ask_gpt_judge(question, reference, prediction):\n","    prompt = f\"\"\"\n","You are an expert model evaluator. Given a question, a reference answer, and a model-generated answer, judge how good the model’s answer is on a scale of 1 to 5. Use the following rubric:\n","\n","1 – Completely irrelevant or hallucinated.\n","2 – Partially related but mostly inaccurate.\n","3 – Mostly accurate but missing key details.\n","4 – Accurate and mostly complete.\n","5 – Nearly identical in meaning to the reference.\n","\n","Be strict but fair. Output ONLY the number.\n","\n","Question: {question}\n","Reference Answer: {reference}\n","Model Prediction: {prediction}\n","\n","Score:\"\"\"\n","\n","    try:\n","        response = client.chat.completions.create(\n","            model=\"gpt-4o\",\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            temperature=0,\n","        )\n","        return response.choices[0].message.content.strip()\n","    except Exception as e:\n","        print(\"Error during evaluation:\\n\")\n","        print(e)\n","        return None"],"metadata":{"id":"T9CGf6qjjIk6","executionInfo":{"status":"ok","timestamp":1744521362768,"user_tz":300,"elapsed":2,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["with open(\"eval_predictions.json\") as f:\n","    eval_results = json.load(f)"],"metadata":{"id":"9bCZ6aQ_jMv6","executionInfo":{"status":"ok","timestamp":1744521510382,"user_tz":300,"elapsed":4,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["judged_results = []\n","\n","for i, item in enumerate(results):\n","    print(f\"Evaluating {i+1}/{len(results)}\")\n","    score = ask_gpt_judge(item[\"question\"], item[\"reference\"], item[\"prediction\"])\n","    if score:\n","        judged_results.append({\n","            \"question\": item[\"question\"],\n","            \"reference\": item[\"reference\"],\n","            \"prediction\": item[\"prediction\"],\n","            \"gpt4o_score\": score\n","        })\n","    time.sleep(1.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zNZlU4SnjPgx","executionInfo":{"status":"ok","timestamp":1744521577020,"user_tz":300,"elapsed":50522,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"1023246c-f66d-4000-c4ae-2d73bb5ad8a4"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating 1/30\n","Evaluating 2/30\n","Evaluating 3/30\n","Evaluating 4/30\n","Evaluating 5/30\n","Evaluating 6/30\n","Evaluating 7/30\n","Evaluating 8/30\n","Evaluating 9/30\n","Evaluating 10/30\n","Evaluating 11/30\n","Evaluating 12/30\n","Evaluating 13/30\n","Evaluating 14/30\n","Evaluating 15/30\n","Evaluating 16/30\n","Evaluating 17/30\n","Evaluating 18/30\n","Evaluating 19/30\n","Evaluating 20/30\n","Evaluating 21/30\n","Evaluating 22/30\n","Evaluating 23/30\n","Evaluating 24/30\n","Evaluating 25/30\n","Evaluating 26/30\n","Evaluating 27/30\n","Evaluating 28/30\n","Evaluating 29/30\n","Evaluating 30/30\n"]}]},{"cell_type":"code","source":["with open(\"gpt4o_judgments.json\", \"w\") as f:\n","    json.dump(judged_results, f, indent=2)"],"metadata":{"id":"llc3tVtejRi8","executionInfo":{"status":"ok","timestamp":1744521696530,"user_tz":300,"elapsed":11,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["for sample in judged_results:\n","    print(\" Question:\", sample[\"question\"])\n","    print(\" Reference Answer:\", sample[\"reference\"])\n","    print(\" Model Prediction:\", sample[\"prediction\"])\n","    print(\" GPT-4o Evaluation:\", sample[\"gpt4o_score\"])\n","    print(\"-\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sACw9tRBmJQQ","executionInfo":{"status":"ok","timestamp":1744521795433,"user_tz":300,"elapsed":7,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"81c51c8f-1816-497c-c45d-e78231473320"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":[" Question: What is the primary innovation introduced by the LoRI method for parameter-efficient fine-tuning?\n"," Reference Answer: LoRI introduces a novel approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks, thereby significantly reducing trainable parameters while minimizing cross-task interference.\n"," Model Prediction: LoRI introduces a novel method for parameter-efficient fine-tuning by learning a low-rank matrix that approximates the full weight matrix, enabling efficient adaptation without storing the entire weight matrix.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: How does LoRI reduce the number of trainable parameters compared to traditional LoRA?\n"," Reference Answer: LoRI reduces the number of trainable parameters by keeping matrix A fixed as a random projection and sparsifying matrix B using task-specific masks, eliminating the need to train both matrices and reducing redundancy.\n"," Model Prediction: LoRI reduces the number of trainable parameters by using a single shared matrix for all layers, instead of separate matrices for each layer as in traditional LoRA. This results in a significant reduction in memory usage and computational cost.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: Why is sparsity in matrix B important in LoRI?\n"," Reference Answer: Sparsity in matrix B enables LoRI to retain only the most critical elements necessary for adaptation, reducing parameter count and mitigating cross-task interference during adapter merging and continual learning.\n"," Model Prediction: Sparsity in matrix B allows for efficient computation and storage, as well as improved interpretability by reducing the number of non-zero elements.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: How does LoRI improve the process of merging adapters in multi-task scenarios?\n"," Reference Answer: LoRI enables more effective adapter merging by using fixed, randomly initialized projection matrices A, which maps task-specific adapters into approximately orthogonal subspaces, thus reducing parameter interference.\n"," Model Prediction: LoRI provides a principled method for merging adapters by optimizing a single objective function that balances the performance of each task and the overall model size. This avoids the need for heuristic methods and ensures that the final model is both task-specific and compact.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: What mechanism does LoRI use to mitigate catastrophic forgetting in continual learning?\n"," Reference Answer: LoRI mitigates catastrophic forgetting by applying task-specific sparse masks to matrix B, which isolates parameter updates across tasks and preserves knowledge from previous adaptations, including safety alignment.\n"," Model Prediction: LoRI uses a novel regularization term that encourages the model to maintain a low rank difference between its pre-trained and updated matrices, thereby preserving the original knowledge while adapting to new tasks.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: On what benchmark did LoRI with 90% sparsity in B outperform LoRA, and by how much?\n"," Reference Answer: LoRI with 90% sparsity in B outperformed LoRA by 17.3% on the HumanEval benchmark using the Llama-3 model.\n"," Model Prediction: LoRI with 90% sparsity in B outperformed LoRA on the CIFAR-100 dataset, achieving 86.4% accuracy compared to LoRA's 85.8%.\n"," GPT-4o Evaluation: 1\n","--------------------------------------------------------------------------------\n"," Question: How does LoRI compare to full fine-tuning and other PEFT methods in terms of performance and efficiency?\n"," Reference Answer: LoRI matches or outperforms full fine-tuning and other PEFT methods across multiple domains while using up to 95% fewer trainable parameters than LoRA, demonstrating both high performance and high efficiency.\n"," Model Prediction: LoRI outperforms full fine-tuning in terms of accuracy and efficiency, achieving up to 98% of full fine-tuning performance with only 1% of the data and 1% of the compute. It also outperforms other PEFT methods in terms of accuracy, efficiency, and ease of implementation.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: What types of tasks were used to evaluate LoRI's effectiveness?\n"," Reference Answer: LoRI was evaluated on a diverse set of tasks, including natural language understanding, mathematical reasoning, code generation, and safety alignment.\n"," Model Prediction: The evaluation tasks included text summarization, question answering, and text classification, demonstrating LoRI's versatility and applicability across various NLP domains.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: What potential future directions do the authors propose for extending LoRI?\n"," Reference Answer: The authors suggest exploring structured sparsity patterns like block sparsity or head pruning and adapting LoRI to multi-modal models such as diffusion and vision-language systems.\n"," Model Prediction: The authors suggest exploring LoRI's application in other domains, such as natural language processing, and investigating its performance on larger datasets. They also propose researching ways to improve LoRI's efficiency and scalability, particularly for high-dimensional data.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: What is the broader significance of LoRI in the context of PEFT and LLM deployment?\n"," Reference Answer: LoRI provides a lightweight, modular, and scalable solution for adapting LLMs with minimal overhead, making it particularly suited for multi-task learning, safety-critical alignment, and efficient deployment on resource-constrained hardware.\n"," Model Prediction: LoRI's ability to adapt models without fine-tuning or re-training makes it a powerful tool for deploying LLMs in resource-constrained environments, such as edge devices or low-bandwidth networks. It also enables more efficient and flexible adaptation to changing user needs or task requirements.\n"," GPT-4o Evaluation: 3\n","--------------------------------------------------------------------------------\n"," Question: What are the core limitations of traditional LoRA methods that ElaLoRA seeks to address?\n"," Reference Answer: ElaLoRA addresses two key limitations of traditional LoRA: the fixed rank allocation across layers, which overlooks the layer-specific importance, and the inability to adapt ranks dynamically during training, which can lead to suboptimal parameter efficiency.\n"," Model Prediction: Traditional LoRA methods suffer from high computational complexity, limited adaptability, and poor generalization, especially when applied to large models or complex tasks. ElaLoRA addresses these issues by leveraging efficient low-rank approximations and adaptive regularization, resulting in faster convergence, better generalization, and improved scalability.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: Describe the three core components of the ElaLoRA framework.\n"," Reference Answer: ElaLoRA's architecture consists of: (1) an SVD-based adaptation strategy for matrix decomposition, (2) an importance score calculation mechanism based on loss gradients to assess rank relevance, and (3) a dynamic rank learning algorithm that reallocates ranks periodically during training to optimize layer-wise adaptation.\n"," Model Prediction: The ElaLoRA framework consists of three core components: (1) ElaLoRA-encoder, which adapts the encoder to the target domain, (2) ElaLoRA-decoder, which adapts the decoder to the target domain, and (3) ElaLoRA-adapter, which adapts the intermediate layers to the target domain.\n"," GPT-4o Evaluation: 1\n","--------------------------------------------------------------------------------\n"," Question: How does ElaLoRA’s adaptive strategy improve performance under limited parameter budgets?\n"," Reference Answer: ElaLoRA reallocates computational resources to the most critical layers by pruning less important ranks and expanding ranks in essential layers, thus achieving higher performance even under smaller parameter budgets—for example, outperforming other PEFT methods with r=2 compared to their r=4 settings.\n"," Model Prediction: By dynamically allocating parameters to the most important tasks, ElaLoRA achieves better performance than uniformly distributed methods, even with fewer total parameters.\n"," GPT-4o Evaluation: 3\n","--------------------------------------------------------------------------------\n"," Question: In what way does ElaLoRA achieve better task alignment during fine-tuning?\n"," Reference Answer: ElaLoRA uses gradient-derived importance scores to identify which layers contribute most to task-specific learning, allowing the model to allocate more capacity to those layers and thus improving task alignment and learning efficiency.\n"," Model Prediction: ElaLoRA uses a task-specific loss function that encourages the model to learn task-relevant features, resulting in better task alignment and improved performance.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: What experimental evidence supports the superiority of ElaLoRA over other PEFT methods?\n"," Reference Answer: Experiments across NLU, NLG, and vision benchmarks show that ElaLoRA consistently outperforms state-of-the-art PEFT methods in accuracy, particularly under constrained parameter budgets, and demonstrates better GLUE benchmark performance even with fewer trainable parameters.\n"," Model Prediction: ElaLoRA outperforms other PEFT methods in terms of both accuracy and computational efficiency, as demonstrated by its superior performance on downstream tasks in the LoRA benchmark and its ability to achieve state-of-the-art results with fewer parameters and lower latency.\n"," GPT-4o Evaluation: 3\n","--------------------------------------------------------------------------------\n"," Question: Why is ElaLoRA particularly well-suited for resource-constrained environments?\n"," Reference Answer: ElaLoRA's dynamic pruning and expansion mechanism ensures that only the most essential ranks are trained, reducing memory usage and computational cost while maintaining high performance, making it ideal for low-resource scenarios.\n"," Model Prediction: ElaLoRA is designed to be highly efficient, requiring only a single forward pass and minimal memory footprint, making it ideal for deployment on low-resource devices.\n"," GPT-4o Evaluation: 3\n","--------------------------------------------------------------------------------\n"," Question: How does the final rank distribution in ElaLoRA reflect its adaptive learning process?\n"," Reference Answer: ElaLoRA’s final rank distribution reveals that higher ranks are allocated to layers deemed more important via importance scores, confirming that the model dynamically concentrates learning capacity on the most impactful parts of the network.\n"," Model Prediction: The final rank distribution shows that ElaLoRA effectively adapts to the task by assigning higher ranks to more useful parameters, while less useful parameters receive lower ranks. This demonstrates the effectiveness of ElaLoRA's adaptive learning strategy.\n"," GPT-4o Evaluation: 4\n","--------------------------------------------------------------------------------\n"," Question: What are the broader implications of ElaLoRA’s design for the future of fine-tuning large models?\n"," Reference Answer: ElaLoRA’s design shows that adaptive, importance-based rank allocation can significantly improve parameter efficiency without sacrificing accuracy, suggesting a paradigm shift toward more intelligent and resource-aware fine-tuning strategies.\n"," Model Prediction: ElaLoRA’s success demonstrates the potential of low-rank adaptation methods to reduce memory and computational requirements, making fine-tuning more accessible and sustainable for a wider range of users and applications.\n"," GPT-4o Evaluation: 3\n","--------------------------------------------------------------------------------\n"," Question: What distinguishes ElaLoRA from prior dynamic rank methods like AdaLoRA or IncreLoRA?\n"," Reference Answer: While AdaLoRA and IncreLoRA either prune or expand ranks, ElaLoRA is the first to implement both pruning and expansion dynamically during training, offering a more flexible and principled mechanism for allocating parameter capacity.\n"," Model Prediction: Unlike AdaLoRA and IncreLoRA, ElaLoRA does not require manual tuning of hyperparameters or rank schedules. Instead, it automatically adapts the rank schedule to the task and dataset, making it more flexible and user-friendly.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: Why is parameter-efficient fine-tuning increasingly important in the LLM landscape?\n"," Reference Answer: As LLMs grow in size, full fine-tuning becomes prohibitively expensive, especially for domain-specific or low-resource settings. PEFT methods like ElaLoRA offer a practical solution by enabling adaptation with minimal compute and storage costs.\n"," Model Prediction: As model sizes grow, parameter-efficient fine-tuning allows for adaptation without the computational overhead of full retraining, making it a scalable and cost-effective approach for adapting models to specific tasks or domains.\n"," GPT-4o Evaluation: 4\n","--------------------------------------------------------------------------------\n"," Question: What is the primary goal of the study presented in 'Beyond QA Pairs'?\n"," Reference Answer: The study aims to assess the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain-specific facts into LLMs, focusing on the impact of QA pair categorization and synthetic dataset generation techniques.\n"," Model Prediction: The study aims to demonstrate the potential of instruction-following models in addressing a broader range of tasks, including those that require reasoning, inference, and understanding of context, beyond simple question-answering.\n"," GPT-4o Evaluation: 1\n","--------------------------------------------------------------------------------\n"," Question: How are QA pairs categorized in this study, and what is the purpose of this categorization?\n"," Reference Answer: QA pairs are classified into ‘Factual’ and ‘Conceptual’ categories using a BERT-based classifier. The purpose is to investigate how the nature of QA pairs influences the effectiveness of PEFT.\n"," Model Prediction: QA pairs are categorized into four groups based on their difficulty level and the type of question (factoid, opinion, or inference). This categorization helps in understanding the performance of the model across different types of questions and difficulty levels.\n"," GPT-4o Evaluation: 1\n","--------------------------------------------------------------------------------\n"," Question: What were the findings regarding models trained on conceptual vs factual QA datasets?\n"," Reference Answer: Models fine-tuned on conceptual datasets consistently outperformed those trained on factual datasets across multiple evaluations.\n"," Model Prediction: Models trained on conceptual QA datasets performed better on conceptual reasoning tasks, while models trained on factual QA datasets performed better on factual reasoning tasks.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: Which synthetic dataset generation techniques are evaluated in this work, and which one performs better?\n"," Reference Answer: The paper evaluates D-RAG and D-Naive synthetic data generation methods. D-Naive outperformed D-RAG in fine-tuning effectiveness, largely due to better retrieval performance.\n"," Model Prediction: The study evaluates three synthetic dataset generation techniques: GAN-based, VAE-based, and autoencoder-based. The results show that the GAN-based method outperforms the other two in terms of both quality and diversity.\n"," GPT-4o Evaluation: 1\n","--------------------------------------------------------------------------------\n"," Question: What was the significance of the product recommendation task in the data center domain?\n"," Reference Answer: The task served as a practical demonstration showing that a Llama-2 7B model fine-tuned with PEFT on just 1,000 instruction-based QA pairs significantly outperformed the baseline in generating product recommendations.\n"," Model Prediction: The product recommendation task demonstrated the potential of LLMs to provide actionable insights, even in complex, specialized domains like data center management.\n"," GPT-4o Evaluation: 3\n","--------------------------------------------------------------------------------\n"," Question: Why do the authors argue that PEFT may not be optimal for factual embedding?\n"," Reference Answer: The study shows that while PEFT is effective for instruction tuning, it struggles with embedding factual information as effectively, likely due to its limited parameter update scope.\n"," Model Prediction: The authors argue that PEFT may not be optimal for factual embedding because it does not directly address the problem of factual inconsistency, which is a critical issue in large language models. Instead, it only adjusts the model's behavior without ensuring factual accuracy.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: What conclusions do the authors draw about the volume versus quality of QA data in PEFT?\n"," Reference Answer: They conclude that sheer quantity of QA pairs is insufficient; quality and conceptual depth are far more critical for successful PEFT.\n"," Model Prediction: The authors find that while volume is important, quality is crucial, and even small, high-quality datasets can yield significant improvements in performance.\n"," GPT-4o Evaluation: 5\n","--------------------------------------------------------------------------------\n"," Question: What limitations of D-RAG were identified in the study?\n"," Reference Answer: D-RAG's limitations were attributed to the poor performance of its underlying vector database retriever, leading to suboptimal training data quality.\n"," Model Prediction: D-RAG was found to be less accurate in identifying relevant documents when the query was ambiguous or contained multiple topics, and it struggled to handle queries with complex syntax or long document summaries.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: How do the authors suggest future research should improve PEFT for fact embedding?\n"," Reference Answer: Future research should explore improvements in retrieval systems used by D-RAG, and consider more refined QA classification and data generation strategies.\n"," Model Prediction: Future research should focus on developing more effective PEFT methods for fact embedding, such as incorporating more complex attention mechanisms, using pre-trained language models, and exploring multi-task learning strategies.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"," Question: What is the key insight this paper contributes to the field of LLM fine-tuning?\n"," Reference Answer: The paper highlights that PEFT's success hinges more on dataset composition—especially the conceptual quality of QA pairs—than on volume alone, and that careful use-case targeting is essential.\n"," Model Prediction: The paper demonstrates that fine-tuning LLMs on a single dataset can be as effective as multi-dataset fine-tuning, while significantly reducing the computational cost and data storage requirements.\n"," GPT-4o Evaluation: 2\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["for res in judged_results[:3]:\n","    print(res[\"gpt4o_score\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KtTRoJAkoWE_","executionInfo":{"status":"ok","timestamp":1744522275575,"user_tz":300,"elapsed":80,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"d55844fd-4417-4d01-abce-bb64763caeba"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","2\n","2\n"]}]},{"cell_type":"code","source":["scores = [int(res[\"gpt4o_score\"]) for res in judged_results if res[\"gpt4o_score\"].isdigit()]\n","average_score = np.mean(scores)\n","print(f\"Average GPT-4o Evaluation Score: {average_score:.2f} out of 5\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7piuJ_mmTDB","executionInfo":{"status":"ok","timestamp":1744522300839,"user_tz":300,"elapsed":33,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"c948b9f0-ad1b-4cdb-fa40-12c6415405dd"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Average GPT-4o Evaluation Score: 2.27 out of 5\n"]}]},{"cell_type":"code","source":["output_path = \"./data/evaluation/eval_gpt4o_judgments_closed_book.json\"\n","\n","os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","with open(output_path, \"w\") as f:\n","    json.dump(judged_results, f, indent=2)\n","\n","print(f\"Judged results saved to {output_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pteueJ14nkt3","executionInfo":{"status":"ok","timestamp":1744525688294,"user_tz":300,"elapsed":45,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"6d80899f-9e19-4298-ca5e-d36cd9800301"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Judged results saved to ./data/evaluation/eval_gpt4o_judgments_closed_book.json\n"]}]},{"cell_type":"markdown","source":["## Step 8: Evaluating with BERTScore (Semantic Similarity Metric)\n","\n","In this section, we evaluate the semantic similarity between the model’s predictions and the ground truth answers using **BERTScore**, a metric that leverages contextual embeddings from large pretrained models (like BERT) to assess the *meaning* of the outputs.\n","\n","Unlike BLEU, which only considers surface-level n-gram overlap, BERTScore measures how semantically close the answers are—even when the phrasing differs.\n","\n","### Interpretation:\n","- **BERTScore F1** reflects the degree of **semantic overlap** between model output and human-labeled answer.\n","- A score closer to **1.0** indicates stronger alignment of meaning.\n","- This metric is especially useful in open-ended QA or summarization settings where **exact matching isn't expected**.\n","\n","> Current Result:  \n","> `Average BERTScore (F1): 0.3464`  \n","> *This is modest, and expected for a closed-book setup on unseen 2025 research content. It will likely improve once context is added or RAG is enabled.*"],"metadata":{"id":"0Pmy81OD6Ode"}},{"cell_type":"code","source":["# Replace `results` with `judged_results` if needed\n","predictions = [item[\"prediction\"] for item in results]\n","references = [item[\"reference\"] for item in results]"],"metadata":{"id":"8nmCEqvy6N_g","executionInfo":{"status":"ok","timestamp":1744527024153,"user_tz":300,"elapsed":3,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["P, R, F1 = bertscore(predictions, references, lang=\"en\", rescale_with_baseline=True)"],"metadata":{"id":"ZwKRFwDcxQIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Average Precision: {P.mean().item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9urKUCpn6gGM","executionInfo":{"status":"ok","timestamp":1744527310076,"user_tz":300,"elapsed":5,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"26ebeacc-c28e-46f8-f1ad-a599a189ed4b"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Precision: 0.3634\n"]}]},{"cell_type":"code","source":["print(f\"Average Recall: {R.mean().item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6n9bHVai7X6Q","executionInfo":{"status":"ok","timestamp":1744527307472,"user_tz":300,"elapsed":8,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"feb3f939-408d-4abc-8354-c32f394e91b5"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Recall: 0.3280\n"]}]},{"cell_type":"code","source":["print(f\"Average BERTScore (F1): {F1.mean().item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5szygE-l6sMG","executionInfo":{"status":"ok","timestamp":1744527217279,"user_tz":300,"elapsed":124,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"0e64b5b0-d238-49b7-f45b-669aa9487708"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["Average BERTScore (F1): 0.3464\n"]}]}]}