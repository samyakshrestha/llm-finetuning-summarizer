{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7+XhyzlNsJIGaIgaC7kYH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Step 1: Mounting Google Drive"],"metadata":{"id":"IyTgUNZ_k4w_"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQXz5m3tknsC","executionInfo":{"status":"ok","timestamp":1744564163990,"user_tz":300,"elapsed":25297,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"cb91da11-3517-4623-c068-2c81d38a71b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n","data\t\t       gpt4o_judgments.json  notebooks\t      README.md  wandb\n","deployment\t       LICENSE\t\t     project_plan.md  results\n","eval_predictions.json  models\t\t     qa_pairs\t      scripts\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Navigate to the repo folder\n","%cd /content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n","\n","# List repo contents\n","!ls"]},{"cell_type":"markdown","source":["## Step 2: Importing Libraries and Setting Output Path"],"metadata":{"id":"DUKOCvpek-zd"}},{"cell_type":"code","source":["import os\n","import json\n","from pathlib import Path\n","import re\n","from google.colab import files\n","import pandas as pd\n","import sys"],"metadata":{"id":"CaZmglKQk79v","executionInfo":{"status":"ok","timestamp":1744564175500,"user_tz":300,"elapsed":587,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["sys.path.append(\"./scripts\")\n","from qa_utils import generate_QA_pair\n","from qa_utils_with_context import generate_QA_pair_with_context"],"metadata":{"id":"lDcOqx9dlLUo","executionInfo":{"status":"ok","timestamp":1744564228385,"user_tz":300,"elapsed":521,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["BASE_DIR = \"/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\"\n","PDF_DIR = os.path.join(BASE_DIR, \"data\", \"Evaluation_PDFs\")\n","QA_DIR = os.path.join(BASE_DIR, \"qa_pairs\", \"qa_pairs_eval\")\n","QA_DIR_WITH_CONTEXT = os.path.join(BASE_DIR, \"qa_pairs\", \"qa_pairs_eval_with_context\")\n","os.makedirs(QA_DIR, exist_ok=True)\n","os.makedirs(QA_DIR_WITH_CONTEXT, exist_ok=True)"],"metadata":{"id":"iwcUi6J5x8eF","executionInfo":{"status":"ok","timestamp":1744564304390,"user_tz":300,"elapsed":3,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Step 3: Generating QA pairs\n","\n","### **LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation**\n","\n","**Abstract**\n","\n","Low-Rank Adaptation (LoRA) has emerged as a popular parameter-\n","efficient fine-tuning (PEFT) method for Large Language Models (LLMs),\n","yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n","\n","**Introduction**\n","\n","Large language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n","\n","Recent studies have shown that delta parameters – the\n","differences between fine-tuned and pretrained model weights – exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process\n","to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model’s knowledge during adaptation.\n","\n","Multi-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n","\n","Beyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n","\n","To evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to – or better than – full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by\n","17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n","\n","**Conclusion**\n","\n","In this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across\n","diverse domains – including natural language understanding, mathematical reasoning, code generation, and safety alignment – while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n","\n","Future Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns – such as block sparsity, head pruning, or group-wise masking – which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning.\n"],"metadata":{"id":"va3_7BAophWO"}},{"cell_type":"code","source":["context = \"\"\"\n","Abstract:\n","\n","Low-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n","\n","Introduction:\n","\n","Large language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n","\n","Recent studies have shown that delta parameters – the differences between fine-tuned and pretrained model weights – exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model’s knowledge during adaptation.\n","\n","Multi-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n","\n","Beyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n","\n","To evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to – or better than – full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n","\n","\n","Conclusion:\n","\n","In this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains – including natural language understanding, mathematical reasoning, code generation, and safety alignment – while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n","\n","Future Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns – such as block sparsity, head pruning, or group-wise masking – which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning.\n","\n","\"\"\".strip()"],"metadata":{"id":"PZZWEzptNbty","executionInfo":{"status":"ok","timestamp":1744565622177,"user_tz":300,"elapsed":5,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["qa_pairs = [\n","    {\n","        \"question\": \"What is the primary innovation introduced by the LoRI method for parameter-efficient fine-tuning?\",\n","        \"answer\": \"LoRI introduces a novel approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks, thereby significantly reducing trainable parameters while minimizing cross-task interference.\"\n","    },\n","    {\n","        \"question\": \"How does LoRI reduce the number of trainable parameters compared to traditional LoRA?\",\n","        \"answer\": \"LoRI reduces the number of trainable parameters by keeping matrix A fixed as a random projection and sparsifying matrix B using task-specific masks, eliminating the need to train both matrices and reducing redundancy.\"\n","    },\n","    {\n","        \"question\": \"Why is sparsity in matrix B important in LoRI?\",\n","        \"answer\": \"Sparsity in matrix B enables LoRI to retain only the most critical elements necessary for adaptation, reducing parameter count and mitigating cross-task interference during adapter merging and continual learning.\"\n","    },\n","    {\n","        \"question\": \"How does LoRI improve the process of merging adapters in multi-task scenarios?\",\n","        \"answer\": \"LoRI enables more effective adapter merging by using fixed, randomly initialized projection matrices A, which maps task-specific adapters into approximately orthogonal subspaces, thus reducing parameter interference.\"\n","    },\n","    {\n","        \"question\": \"What mechanism does LoRI use to mitigate catastrophic forgetting in continual learning?\",\n","        \"answer\": \"LoRI mitigates catastrophic forgetting by applying task-specific sparse masks to matrix B, which isolates parameter updates across tasks and preserves knowledge from previous adaptations, including safety alignment.\"\n","    },\n","    {\n","        \"question\": \"On what benchmark did LoRI with 90% sparsity in B outperform LoRA, and by how much?\",\n","        \"answer\": \"LoRI with 90% sparsity in B outperformed LoRA by 17.3% on the HumanEval benchmark using the Llama-3 model.\"\n","    },\n","    {\n","        \"question\": \"How does LoRI compare to full fine-tuning and other PEFT methods in terms of performance and efficiency?\",\n","        \"answer\": \"LoRI matches or outperforms full fine-tuning and other PEFT methods across multiple domains while using up to 95% fewer trainable parameters than LoRA, demonstrating both high performance and high efficiency.\"\n","    },\n","    {\n","        \"question\": \"What types of tasks were used to evaluate LoRI's effectiveness?\",\n","        \"answer\": \"LoRI was evaluated on a diverse set of tasks, including natural language understanding, mathematical reasoning, code generation, and safety alignment.\"\n","    },\n","    {\n","        \"question\": \"What potential future directions do the authors propose for extending LoRI?\",\n","        \"answer\": \"The authors suggest exploring structured sparsity patterns like block sparsity or head pruning and adapting LoRI to multi-modal models such as diffusion and vision-language systems.\"\n","    },\n","    {\n","        \"question\": \"What is the broader significance of LoRI in the context of PEFT and LLM deployment?\",\n","        \"answer\": \"LoRI provides a lightweight, modular, and scalable solution for adapting LLMs with minimal overhead, making it particularly suited for multi-task learning, safety-critical alignment, and efficient deployment on resource-constrained hardware.\"\n","    }\n","]"],"metadata":{"id":"pYncSCvVoVrF","executionInfo":{"status":"ok","timestamp":1744565861456,"user_tz":300,"elapsed":4,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["title = \"LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation\""],"metadata":{"id":"b2QqpYwAxtWs","executionInfo":{"status":"ok","timestamp":1744565853238,"user_tz":300,"elapsed":2,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["generate_QA_pair(\"01\", 2025, \"lori\", title, qa_pairs, QA_DIR)"],"metadata":{"id":"rtNRZbkFOjaZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate_QA_pair_with_context(\n","    paper_number=\"01\",\n","    publication_year=2025,\n","    short_id=\"lori\",\n","    title=title,\n","    qa_pairs=qa_pairs,\n","    context=context,\n","    save_dir= QA_DIR_WITH_CONTEXT\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"5yy4pPohOQxL","executionInfo":{"status":"ok","timestamp":1744565864386,"user_tz":300,"elapsed":139,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"80501194-fb37-40b7-8958-19db9fdd8e91"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/qa_pairs/qa_pairs_eval_with_context/01_2025_lori.json'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["### **ElaLoRA: Elastic & Learnable Low-Rank Adaptation for Efficient Model Fine-Tuning**\n","\n","\n","**Abstract**\n","\n","Low-Rank Adaptation (LoRA) has become a widely adopted technique for fine-tuning large-scale pre-trained models with minimal parameter updates. However, existing methods rely on fixed ranks or focus solely on either rank pruning or expansion, failing to adapt ranks dynamically to match the importance of different layers during training. In this work, we propose ElaLoRA, an adaptive low-rank adaptation framework that dynamically prunes and expands ranks based on gradient-derived importance scores. To the best of our knowledge, ElaLoRA is the first method that enables both rank pruning and expansion during fine-tuning. Experiments across multiple benchmarks demonstrate that ElaLoRA consistently outperforms existing PEFT methods across different parameter budgets. Furthermore, our studies validate that layers receiving higher rank allocations contribute more significantly to model performance, providing theoretical justification for our adaptive strategy. By introducing a principled and adaptive rank allocation mechanism, ElaLoRA offers a scalable and efficient fine-tuning solution, particularly suited for resource-constrained environments.\n","\n","**Introduction**\n","\n","Scaling laws of transformer-based Pre-trained Language Models (PLMs) suggest that increasing model size leads to improved generalization and task performance, which has driven the rapid expansion of model architectures, from 330M parameters in BERT to 1.5B in GPT-2, 175B in GPT-3, and 671B in DeepSeek, highlighting the trend toward ever-larger pretrained models. Despite\n","these advances, Large Language Models (LLMs) remain constrained by their knowledge boundaries, requiring fine-tuning to specialize in domain-specific applications and adapt to evolving datasets. Traditionally, full fine-tuning has been the standard approach, which is nevertheless prohibitively expensive in terms of memory and computation.\n","\n","To address the computational burden of full fine-tuning, Parameter-efficient fine-tuning (PEFT) methods have been developed, with Low-Rank Adaptation (LoRA) being a widely used approach that reduces trainable parameters\n","without increasing inference latency. However, LoRA’s fixed rank allocation leads to suboptimal performance by failing to account for layer-specific importance (Zhang et al., 2023b). Dynamic rank allocation methods like AdaLoRA and SaLoRA decompose a matrix using singular value decomposition (SVD) and selectively prune its singular values to control the rank of the matrix, but these methods are computationally inefficient as they begin with a high rank. IncreLoRA (Zhang et al., 2023a) mitigates this by starting with a minimal rank and increasing it heuristically. However, early training samples may not be effectively learned or utilized when the rank is small.\n","\n","To overcome these limitations, we propose ElaLoRA, a novel adaptive and dynamic LoRA framework that simultaneously prunes and expands ranks (as shown in Figure 1). By dynamically reallocating computational resources to the most critical layers, ElaLoRA ensures that essential layers receive more capacity while redundant ranks are removed. ElaLoRA operates through three key components: 1) SVD-based adaptation strategy; 2) importance score calculation to quantify the significance of each rank based on loss gradients; and 3) a dynamic rank learning algorithm that reallocates ranks at scheduled intervals.\n","Experimental results across multiple Natural Language Understanding (NLU), Natural Language Generation (NLG), and Visual Task benchmarks demonstrate that ElaLoRA consistently outperforms existing PEFT methods under various parameter budgets. Notably, ElaLoRA achieves better average GLUE results with r = 2 than other PEFT methods at r = 4, making it particularly well-suited for resource-constrained environments. Our key contributions include:\n","\n","• We introduce ElaLoRA, the first method to the best of our knowledge that enables both rank pruning and expansion simultaneously during fine-tuning. Comparisons are shown in Table 1.\n","\n","• We conduct extensive experiments across multiple benchmarks under different parameter budgets. Our results consistently demonstrate the effectiveness of ElaLoRA, outperforming existing PEFT methods in performance.\n","\n","• We conduct analysis to verify that the layers and matrices identified as highly important for a specific task are indeed significant for that task, providing a principled validation of our adaptive rank allocation method.\n","\n","**Conclusion**\n","\n","In this work, we introduced ElaLoRA, a novel parameter-efficient fine-tuning (PEFT) method that dynamically prunes and expands ranks based on importance scores, ensuring that the most impactful layers receive additional capacity while removing redundant ranks. This adaptive rank learning mechanism enables more efficient model adaptation across diverse NLP and Vision tasks. Our empirical results demonstrate that ElaLoRA outperforms other state-of-the-art methods, achieving superior accuracy across multiple benchmark datasets while maintaining a lower or comparable parameter budget. Beyond performance improvements, our analysis of final rank distributions and importance score distributions confirms that ElaLoRA’s rank allocation decisions align with the layers that contribute most to task-specific learning.\n","\n"],"metadata":{"id":"z8ipFG-H2bqH"}},{"cell_type":"code","source":["context = \"\"\"\n","Abstract:\n","Low-Rank Adaptation (LoRA) has become a widely adopted technique for fine-tuning large-scale pre-trained models with minimal parameter updates. However, existing methods rely on fixed ranks or focus solely on either rank pruning or expansion, failing to adapt ranks dynamically to match the importance of different layers during training. In this work, we propose ElaLoRA, an adaptive low-rank adaptation framework that dynamically prunes and expands ranks based on gradient-derived importance scores. To the best of our knowledge, ElaLoRA is the first method that enables both rank pruning and expansion during fine-tuning. Experiments across multiple benchmarks demonstrate that ElaLoRA consistently outperforms existing PEFT methods across different parameter budgets. Furthermore, our studies validate that layers receiving higher rank allocations contribute more significantly to model performance, providing theoretical justification for our adaptive strategy. By introducing a principled and adaptive rank allocation mechanism, ElaLoRA offers a scalable and efficient fine-tuning solution, particularly suited for resource-constrained environments.\n","\n","Introduction:\n","Scaling laws of transformer-based Pre-trained Language Models (PLMs) suggest that increasing model size leads to improved generalization and task performance, which has driven the rapid expansion of model architectures, from 330M parameters in BERT to 1.5B in GPT-2, 175B in GPT-3, and 671B in DeepSeek, highlighting the trend toward ever-larger pretrained models. Despite these advances, Large Language Models (LLMs) remain constrained by their knowledge boundaries, requiring fine-tuning to specialize in domain-specific applications and adapt to evolving datasets. Traditionally, full fine-tuning has been the standard approach, which is nevertheless prohibitively expensive in terms of memory and computation.\n","\n","To address the computational burden of full fine-tuning, Parameter-efficient fine-tuning (PEFT) methods have been developed, with Low-Rank Adaptation (LoRA) being a widely used approach that reduces trainable parameters without increasing inference latency. However, LoRA’s fixed rank allocation leads to suboptimal performance by failing to account for layer-specific importance (Zhang et al., 2023b). Dynamic rank allocation methods like AdaLoRA and SaLoRA decompose a matrix using singular value decomposition (SVD) and selectively prune its singular values to control the rank of the matrix, but these methods are computationally inefficient as they begin with a high rank. IncreLoRA (Zhang et al., 2023a) mitigates this by starting with a minimal rank and increasing it heuristically. However, early training samples may not be effectively learned or utilized when the rank is small.\n","\n","To overcome these limitations, we propose ElaLoRA, a novel adaptive and dynamic LoRA framework that simultaneously prunes and expands ranks (as shown in Figure 1). By dynamically reallocating computational resources to the most critical layers, ElaLoRA ensures that essential layers receive more capacity while redundant ranks are removed. ElaLoRA operates through three key components: 1) SVD-based adaptation strategy; 2) importance score calculation to quantify the significance of each rank based on loss gradients; and 3) a dynamic rank learning algorithm that reallocates ranks at scheduled intervals. Experimental results across multiple Natural Language Understanding (NLU), Natural Language Generation (NLG), and Visual Task benchmarks demonstrate that ElaLoRA consistently outperforms existing PEFT methods under various parameter budgets. Notably, ElaLoRA achieves better average GLUE results with r = 2 than other PEFT methods at r = 4, making it particularly well-suited for resource-constrained environments. Our key contributions include:\n","\n","• We introduce ElaLoRA, the first method to the best of our knowledge that enables both rank pruning and expansion simultaneously during fine-tuning. Comparisons are shown in Table 1.\n","\n","• We conduct extensive experiments across multiple benchmarks under different parameter budgets. Our results consistently demonstrate the effectiveness of ElaLoRA, outperforming existing PEFT methods in performance.\n","\n","• We conduct analysis to verify that the layers and matrices identified as highly important for a specific task are indeed significant for that task, providing a principled validation of our adaptive rank allocation method.\n","\n","Conclusion:\n","In this work, we introduced ElaLoRA, a novel parameter-efficient fine-tuning (PEFT) method that dynamically prunes and expands ranks based on importance scores, ensuring that the most impactful layers receive additional capacity while removing redundant ranks. This adaptive rank learning mechanism enables more efficient model adaptation across diverse NLP and Vision tasks. Our empirical results demonstrate that ElaLoRA outperforms other state-of-the-art methods, achieving superior accuracy across multiple benchmark datasets while maintaining a lower or comparable parameter budget. Beyond performance improvements, our analysis of final rank distributions and importance score distributions confirms that ElaLoRA’s rank allocation decisions align with the layers that contribute most to task-specific learning.\n","\"\"\".strip()"],"metadata":{"id":"ICouAH_rQRdp","executionInfo":{"status":"ok","timestamp":1744566363025,"user_tz":300,"elapsed":4,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["qa_pairs = [\n","  {\n","    \"question\": \"What are the core limitations of traditional LoRA methods that ElaLoRA seeks to address?\",\n","    \"answer\": \"ElaLoRA addresses two key limitations of traditional LoRA: the fixed rank allocation across layers, which overlooks the layer-specific importance, and the inability to adapt ranks dynamically during training, which can lead to suboptimal parameter efficiency.\"\n","  },\n","  {\n","    \"question\": \"Describe the three core components of the ElaLoRA framework.\",\n","    \"answer\": \"ElaLoRA's architecture consists of: (1) an SVD-based adaptation strategy for matrix decomposition, (2) an importance score calculation mechanism based on loss gradients to assess rank relevance, and (3) a dynamic rank learning algorithm that reallocates ranks periodically during training to optimize layer-wise adaptation.\"\n","  },\n","  {\n","    \"question\": \"How does ElaLoRA’s adaptive strategy improve performance under limited parameter budgets?\",\n","    \"answer\": \"ElaLoRA reallocates computational resources to the most critical layers by pruning less important ranks and expanding ranks in essential layers, thus achieving higher performance even under smaller parameter budgets—for example, outperforming other PEFT methods with r=2 compared to their r=4 settings.\"\n","  },\n","  {\n","    \"question\": \"In what way does ElaLoRA achieve better task alignment during fine-tuning?\",\n","    \"answer\": \"ElaLoRA uses gradient-derived importance scores to identify which layers contribute most to task-specific learning, allowing the model to allocate more capacity to those layers and thus improving task alignment and learning efficiency.\"\n","  },\n","  {\n","    \"question\": \"What experimental evidence supports the superiority of ElaLoRA over other PEFT methods?\",\n","    \"answer\": \"Experiments across NLU, NLG, and vision benchmarks show that ElaLoRA consistently outperforms state-of-the-art PEFT methods in accuracy, particularly under constrained parameter budgets, and demonstrates better GLUE benchmark performance even with fewer trainable parameters.\"\n","  },\n","  {\n","    \"question\": \"Why is ElaLoRA particularly well-suited for resource-constrained environments?\",\n","    \"answer\": \"ElaLoRA's dynamic pruning and expansion mechanism ensures that only the most essential ranks are trained, reducing memory usage and computational cost while maintaining high performance, making it ideal for low-resource scenarios.\"\n","  },\n","  {\n","    \"question\": \"How does the final rank distribution in ElaLoRA reflect its adaptive learning process?\",\n","    \"answer\": \"ElaLoRA’s final rank distribution reveals that higher ranks are allocated to layers deemed more important via importance scores, confirming that the model dynamically concentrates learning capacity on the most impactful parts of the network.\"\n","  },\n","  {\n","    \"question\": \"What are the broader implications of ElaLoRA’s design for the future of fine-tuning large models?\",\n","    \"answer\": \"ElaLoRA’s design shows that adaptive, importance-based rank allocation can significantly improve parameter efficiency without sacrificing accuracy, suggesting a paradigm shift toward more intelligent and resource-aware fine-tuning strategies.\"\n","  },\n","  {\n","    \"question\": \"What distinguishes ElaLoRA from prior dynamic rank methods like AdaLoRA or IncreLoRA?\",\n","    \"answer\": \"While AdaLoRA and IncreLoRA either prune or expand ranks, ElaLoRA is the first to implement both pruning and expansion dynamically during training, offering a more flexible and principled mechanism for allocating parameter capacity.\"\n","  },\n","  {\n","    \"question\": \"Why is parameter-efficient fine-tuning increasingly important in the LLM landscape?\",\n","    \"answer\": \"As LLMs grow in size, full fine-tuning becomes prohibitively expensive, especially for domain-specific or low-resource settings. PEFT methods like ElaLoRA offer a practical solution by enabling adaptation with minimal compute and storage costs.\"\n","  }\n","]"],"metadata":{"id":"l7HmTyQG2bML","executionInfo":{"status":"ok","timestamp":1744566363574,"user_tz":300,"elapsed":3,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["title = \"ElaLoRA: Elastic & Learnable Low-Rank Adaptation for Efficient Model Fine-Tuning\""],"metadata":{"id":"L9ACBXLOQkC8","executionInfo":{"status":"ok","timestamp":1744566424508,"user_tz":300,"elapsed":2,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["generate_QA_pair(\"02\", 2025, \"elalora\", title, qa_pairs, QA_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"E9ugWNsk0_yL","executionInfo":{"status":"ok","timestamp":1744445139121,"user_tz":300,"elapsed":155,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"30d6ff17-de05-496a-ee02-ec38eddc73f8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/qa_pairs/qa_pairs_eval/02_2025_elalora.json'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["generate_QA_pair_with_context(\n","    paper_number=\"02\",\n","    publication_year=2025,\n","    short_id=\"elalora\",\n","    title=title,\n","    qa_pairs=qa_pairs,\n","    context=context,\n","    save_dir= QA_DIR_WITH_CONTEXT\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"E0s6q5I0Qpuu","executionInfo":{"status":"ok","timestamp":1744566453621,"user_tz":300,"elapsed":110,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"997ea3a9-c1dc-489e-c33f-9230d6f53d89"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/qa_pairs/qa_pairs_eval_with_context/02_2025_elalora.json'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["### **Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs**\n","\n","**Abstract**\n","\n","This paper presents an extensive examination of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain specific facts into Large Language Models (LLMs), focusing on improving the fine-tuning process by categorizing question-answer (QA) pairs into ‘Factual’ and ‘Conceptual’ classes using a BERT-based classifier. Two distinct Llama-2 models are fine-tuned based on these classifications and evaluated using larger models like GPT-3.5 Turbo and Gemini. Our results indicate that models trained on conceptual datasets outperform those trained on factual datasets. Additionally, we compare the efficiency of two synthetic fine-tuning dataset generation techniques, D-RAG and D-Naive, with D-Naive demonstrating superior performance. Although PEFT has shown effectiveness, our research indicates that it may not be the most optimal method for embedding facts into LLMs. However, it has demonstrated exceptional performance in instruction-based tasks. Our findings are reinforced by a 1000-sample dataset in the data center domain, where the fine-tuned Llama-2 7B model significantly outperforms the baseline model in generating product recommendations. Our study highlights the importance of QA pair categorization and synthetic dataset generation techniques in enhancing the performance of LLMs in specific domains.\n","\n","\n","**Introduction**\n","\n","Parameter-Efficient Fine-Tuning (PEFT) has emerged as a highly effective strategy for refining Large Language Models (LLMs) on domain-specific data, thanks to its reduced computational and time requirements compared to full fine-tuning. This technique has seen widespread adoption in the industry for embedding domain knowledge into LLMs. Platforms like Azure, Google Cloud Platform, Mistral, AWS, and Lamini offer fine-tuning as a service using methods like Low Rank Adaptation (LoRA), making PEFT accessible and user-friendly (Hu et al. 2021). These low code/no code solutions have become popular among developers due to their simplicity. However, the ease of use of these platforms can create a misconception that merely having a large quantity of question-answer (QA) pairs is sufficient for effective domain adaptation. This misunderstanding may lead to the utilization of low-quality datasets, compromising the effectiveness of the fine-tuning process. In this paper, we address this issue by proposing a set of metrics to assess the quality and appropriateness of QA datasets for PEFT. We introduce a novel method for categorizing QA pairs into ‘Factual’ and ‘Conceptual’ classes using a BERT-based classifier. By separating the original dataset based on these categories, we fine-tune two distinct sets of Llama-2 models\n","using LoRA. Our evaluation, conducted with larger models such as GPT-3.5 Turbo, Gemini 1.5 Pro, and Prometheus, reveals that models trained on conceptual datasets significantly outperform those trained on factual datasets. Furthermore, we investigate the effectiveness of two synthetic dataset generation techniques, D-RAG and D-Naive (depicted in Figure 1). Our results show that the D-Naive approach produces superior fine-tuning datasets compared to D-RAG. Additionally, we suggest that while PEFT is highly effective, it may not be optimal for embedding factual information into LLMs. Instead, it excels in instruction-based tasks. To support our assertion, we conducted an experiment using a 1000-sample dataset for sales product recommendation in the data center domain. The results clearly demonstrate that the fine-tuned Llama-2 7B model outperforms the baseline model.\n","\n","**Conclusions**\n","\n","Our research highlights the paramount importance of the quality and categorization of QA pairs in PEFT, providing profound insights into optimizing the fine-tuning process of LLMs for domain-specific applications. The outcomes of our fine-tuning experiments reveal that PEFT is particularly\n","advantageous for scenarios requiring minimal factual information embedding into LLMs. Notably, the LLM trained on a conceptual dataset significantly outperformed the one trained on a factual dataset. This trend was consistently observed across all three proctor models, underscoring that the sheer volume of QA pairs is insufficient for the effective deployment of PEFT in developing domain-specific QA bots. It is crucial to judiciously select the use-case when leveraging PEFT. Our product recommendation experiment further illustrates that for instruction-based applications, even a dataset as modest as 1,000 prompt-response pairs can yield a high-quality fine-tuned model.\n","\n","Although our experiments with D-RAG and D-Naive did not demonstrate that the D-RAG technique for synthetic training data generation is more efficient, we believe that this avenue warrants further exploration. The potential of D-RAG to generate more comprehensive and complete answers remains promising. In this particular instance, the technique’s shortcomings were primarily due to the suboptimal performance of the vector database retriever. By addressing these retrieval inefficiencies, future research could unlock the full potential of D-RAG, thereby contributing to more effective and nuanced fine-tuning methodologies for LLMs. Thus, while current findings emphasize the importance of careful use-case selection and QA pair quality in PEFT, they also open the door for continued innovation in synthetic data generation techniques."],"metadata":{"id":"8X5nmq3X47TP"}},{"cell_type":"code","source":["context = \"\"\"\n","Abstract:\n","This paper presents an extensive examination of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain specific facts into Large Language Models (LLMs), focusing on improving the fine-tuning process by categorizing question-answer (QA) pairs into ‘Factual’ and ‘Conceptual’ classes using a BERT-based classifier. Two distinct Llama-2 models are fine-tuned based on these classifications and evaluated using larger models like GPT-3.5 Turbo and Gemini. Our results indicate that models trained on conceptual datasets outperform those trained on factual datasets. Additionally, we compare the efficiency of two synthetic fine-tuning dataset generation techniques, D-RAG and D-Naive, with D-Naive demonstrating superior performance. Although PEFT has shown effectiveness, our research indicates that it may not be the most optimal method for embedding facts into LLMs. However, it has demonstrated exceptional performance in instruction-based tasks. Our findings are reinforced by a 1000-sample dataset in the data center domain, where the fine-tuned Llama-2 7B model significantly outperforms the baseline model in generating product recommendations. Our study highlights the importance of QA pair categorization and synthetic dataset generation techniques in enhancing the performance of LLMs in specific domains.\n","\n","Introduction:\n","Parameter-Efficient Fine-Tuning (PEFT) has emerged as a highly effective strategy for refining Large Language Models (LLMs) on domain-specific data, thanks to its reduced computational and time requirements compared to full fine-tuning. This technique has seen widespread adoption in the industry for embedding domain knowledge into LLMs. Platforms like Azure, Google Cloud Platform, Mistral, AWS, and Lamini offer fine-tuning as a service using methods like Low Rank Adaptation (LoRA), making PEFT accessible and user-friendly (Hu et al. 2021). These low code/no code solutions have become popular among developers due to their simplicity. However, the ease of use of these platforms can create a misconception that merely having a large quantity of question-answer (QA) pairs is sufficient for effective domain adaptation. This misunderstanding may lead to the utilization of low-quality datasets, compromising the effectiveness of the fine-tuning process. In this paper, we address this issue by proposing a set of metrics to assess the quality and appropriateness of QA datasets for PEFT. We introduce a novel method for categorizing QA pairs into ‘Factual’ and ‘Conceptual’ classes using a BERT-based classifier. By separating the original dataset based on these categories, we fine-tune two distinct sets of Llama-2 models using LoRA. Our evaluation, conducted with larger models such as GPT-3.5 Turbo, Gemini 1.5 Pro, and Prometheus, reveals that models trained on conceptual datasets significantly outperform those trained on factual datasets. Furthermore, we investigate the effectiveness of two synthetic dataset generation techniques, D-RAG and D-Naive (depicted in Figure 1). Our results show that the D-Naive approach produces superior fine-tuning datasets compared to D-RAG. Additionally, we suggest that while PEFT is highly effective, it may not be optimal for embedding factual information into LLMs. Instead, it excels in instruction-based tasks. To support our assertion, we conducted an experiment using a 1000-sample dataset for sales product recommendation in the data center domain. The results clearly demonstrate that the fine-tuned Llama-2 7B model outperforms the baseline model.\n","\n","Conclusion:\n","Our research highlights the paramount importance of the quality and categorization of QA pairs in PEFT, providing profound insights into optimizing the fine-tuning process of LLMs for domain-specific applications. The outcomes of our fine-tuning experiments reveal that PEFT is particularly advantageous for scenarios requiring minimal factual information embedding into LLMs. Notably, the LLM trained on a conceptual dataset significantly outperformed the one trained on a factual dataset. This trend was consistently observed across all three proctor models, underscoring that the sheer volume of QA pairs is insufficient for the effective deployment of PEFT in developing domain-specific QA bots. It is crucial to judiciously select the use-case when leveraging PEFT. Our product recommendation experiment further illustrates that for instruction-based applications, even a dataset as modest as 1,000 prompt-response pairs can yield a high-quality fine-tuned model.\n","\n","Although our experiments with D-RAG and D-Naive did not demonstrate that the D-RAG technique for synthetic training data generation is more efficient, we believe that this avenue warrants further exploration. The potential of D-RAG to generate more comprehensive and complete answers remains promising. In this particular instance, the technique’s shortcomings were primarily due to the suboptimal performance of the vector database retriever. By addressing these retrieval inefficiencies, future research could unlock the full potential of D-RAG, thereby contributing to more effective and nuanced fine-tuning methodologies for LLMs. Thus, while current findings emphasize the importance of careful use-case selection and QA pair quality in PEFT, they also open the door for continued innovation in synthetic data generation techniques.\n","\"\"\".strip()"],"metadata":{"id":"eCY5Q99KRDMc","executionInfo":{"status":"ok","timestamp":1744566538654,"user_tz":300,"elapsed":22,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["qa_pairs = [\n","    {\n","        \"question\": \"What is the primary goal of the study presented in 'Beyond QA Pairs'?\",\n","        \"answer\": \"The study aims to assess the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain-specific facts into LLMs, focusing on the impact of QA pair categorization and synthetic dataset generation techniques.\"\n","    },\n","    {\n","        \"question\": \"How are QA pairs categorized in this study, and what is the purpose of this categorization?\",\n","        \"answer\": \"QA pairs are classified into ‘Factual’ and ‘Conceptual’ categories using a BERT-based classifier. The purpose is to investigate how the nature of QA pairs influences the effectiveness of PEFT.\"\n","    },\n","    {\n","        \"question\": \"What were the findings regarding models trained on conceptual vs factual QA datasets?\",\n","        \"answer\": \"Models fine-tuned on conceptual datasets consistently outperformed those trained on factual datasets across multiple evaluations.\"\n","    },\n","    {\n","        \"question\": \"Which synthetic dataset generation techniques are evaluated in this work, and which one performs better?\",\n","        \"answer\": \"The paper evaluates D-RAG and D-Naive synthetic data generation methods. D-Naive outperformed D-RAG in fine-tuning effectiveness, largely due to better retrieval performance.\"\n","    },\n","    {\n","        \"question\": \"What was the significance of the product recommendation task in the data center domain?\",\n","        \"answer\": \"The task served as a practical demonstration showing that a Llama-2 7B model fine-tuned with PEFT on just 1,000 instruction-based QA pairs significantly outperformed the baseline in generating product recommendations.\"\n","    },\n","    {\n","        \"question\": \"Why do the authors argue that PEFT may not be optimal for factual embedding?\",\n","        \"answer\": \"The study shows that while PEFT is effective for instruction tuning, it struggles with embedding factual information as effectively, likely due to its limited parameter update scope.\"\n","    },\n","    {\n","        \"question\": \"What conclusions do the authors draw about the volume versus quality of QA data in PEFT?\",\n","        \"answer\": \"They conclude that sheer quantity of QA pairs is insufficient; quality and conceptual depth are far more critical for successful PEFT.\"\n","    },\n","    {\n","        \"question\": \"What limitations of D-RAG were identified in the study?\",\n","        \"answer\": \"D-RAG's limitations were attributed to the poor performance of its underlying vector database retriever, leading to suboptimal training data quality.\"\n","    },\n","    {\n","        \"question\": \"How do the authors suggest future research should improve PEFT for fact embedding?\",\n","        \"answer\": \"Future research should explore improvements in retrieval systems used by D-RAG, and consider more refined QA classification and data generation strategies.\"\n","    },\n","    {\n","        \"question\": \"What is the key insight this paper contributes to the field of LLM fine-tuning?\",\n","        \"answer\": \"The paper highlights that PEFT's success hinges more on dataset composition—especially the conceptual quality of QA pairs—than on volume alone, and that careful use-case targeting is essential.\"\n","    }\n","]"],"metadata":{"id":"mkpocIiR4uk3","executionInfo":{"status":"ok","timestamp":1744566540092,"user_tz":300,"elapsed":1,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["title = \"Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs\""],"metadata":{"id":"975H6brdROYE","executionInfo":{"status":"ok","timestamp":1744566548808,"user_tz":300,"elapsed":4,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["generate_QA_pair(\"03\", 2025, \"beyond_qa_pairs\", title, qa_pairs, QA_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"62jUMFcn6kVi","executionInfo":{"status":"ok","timestamp":1744445144699,"user_tz":300,"elapsed":108,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"644c7b5f-fea9-440b-b01f-691813262e71"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/qa_pairs/qa_pairs_eval/03_2025_beyond_qa_pairs.json'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["generate_QA_pair_with_context(\n","    paper_number=\"03\",\n","    publication_year=2025,\n","    short_id=\"beyond_qa_pairs\",\n","    title=title,\n","    qa_pairs=qa_pairs,\n","    context=context,\n","    save_dir= QA_DIR_WITH_CONTEXT\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"u9L193foRRCX","executionInfo":{"status":"ok","timestamp":1744566579291,"user_tz":300,"elapsed":110,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"2da29697-c113-4abd-9a26-d913d46257f9"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/qa_pairs/qa_pairs_eval_with_context/03_2025_beyond_qa_pairs.json'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["## Step 4: Combining All QA Pairs"],"metadata":{"id":"6DOb1XpIBk4K"}},{"cell_type":"code","source":["qa_folder = './qa_pairs/qa_pairs_eval'\n","output_path = './data/eval.jsonl'\n","\n","all_qa_pairs = []\n","\n","for filename in os.listdir(qa_folder):\n","    if filename.endswith('.json'):\n","        with open(os.path.join(qa_folder, filename), 'r') as f:\n","            data = json.load(f)\n","            for pair in data['qa_pairs']:\n","                all_qa_pairs.append({\n","                    \"question\": pair['question'].strip(),\n","                    \"answer\": pair['answer'].strip()\n","                })"],"metadata":{"id":"Nj3yk55DBh6W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_qa_pairs[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQc5GTN-6wHB","executionInfo":{"status":"ok","timestamp":1744445277540,"user_tz":300,"elapsed":9,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"ef89aad6-6dea-465b-f8ff-0993b2516c06"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'What is the primary innovation introduced by the LoRI method for parameter-efficient fine-tuning?',\n"," 'answer': 'LoRI introduces a novel approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks, thereby significantly reducing trainable parameters while minimizing cross-task interference.'}"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["with open(output_path, 'w') as f:\n","    for pair in all_qa_pairs:\n","        json.dump(pair, f)\n","        f.write('\\n')"],"metadata":{"id":"mOPXH5CRCoP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_json('data/eval.jsonl', lines=True)"],"metadata":{"id":"gVmqQsYtCsj1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":979},"id":"U-od_tKsCxfc","executionInfo":{"status":"ok","timestamp":1744445735058,"user_tz":300,"elapsed":154,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"4cdffd10-18cc-4849-a944-711c685f69d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             question  \\\n","0   What is the primary innovation introduced by t...   \n","1   How does LoRI reduce the number of trainable p...   \n","2      Why is sparsity in matrix B important in LoRI?   \n","3   How does LoRI improve the process of merging a...   \n","4   What mechanism does LoRI use to mitigate catas...   \n","5   On what benchmark did LoRI with 90% sparsity i...   \n","6   How does LoRI compare to full fine-tuning and ...   \n","7   What types of tasks were used to evaluate LoRI...   \n","8   What potential future directions do the author...   \n","9   What is the broader significance of LoRI in th...   \n","10  What are the core limitations of traditional L...   \n","11  Describe the three core components of the ElaL...   \n","12  How does ElaLoRA’s adaptive strategy improve p...   \n","13  In what way does ElaLoRA achieve better task a...   \n","14  What experimental evidence supports the superi...   \n","15  Why is ElaLoRA particularly well-suited for re...   \n","16  How does the final rank distribution in ElaLoR...   \n","17  What are the broader implications of ElaLoRA’s...   \n","18  What distinguishes ElaLoRA from prior dynamic ...   \n","19  Why is parameter-efficient fine-tuning increas...   \n","20  What is the primary goal of the study presente...   \n","21  How are QA pairs categorized in this study, an...   \n","22  What were the findings regarding models traine...   \n","23  Which synthetic dataset generation techniques ...   \n","24  What was the significance of the product recom...   \n","25  Why do the authors argue that PEFT may not be ...   \n","26  What conclusions do the authors draw about the...   \n","27  What limitations of D-RAG were identified in t...   \n","28  How do the authors suggest future research sho...   \n","29  What is the key insight this paper contributes...   \n","\n","                                               answer  \n","0   LoRI introduces a novel approach that freezes ...  \n","1   LoRI reduces the number of trainable parameter...  \n","2   Sparsity in matrix B enables LoRI to retain on...  \n","3   LoRI enables more effective adapter merging by...  \n","4   LoRI mitigates catastrophic forgetting by appl...  \n","5   LoRI with 90% sparsity in B outperformed LoRA ...  \n","6   LoRI matches or outperforms full fine-tuning a...  \n","7   LoRI was evaluated on a diverse set of tasks, ...  \n","8   The authors suggest exploring structured spars...  \n","9   LoRI provides a lightweight, modular, and scal...  \n","10  ElaLoRA addresses two key limitations of tradi...  \n","11  ElaLoRA's architecture consists of: (1) an SVD...  \n","12  ElaLoRA reallocates computational resources to...  \n","13  ElaLoRA uses gradient-derived importance score...  \n","14  Experiments across NLU, NLG, and vision benchm...  \n","15  ElaLoRA's dynamic pruning and expansion mechan...  \n","16  ElaLoRA’s final rank distribution reveals that...  \n","17  ElaLoRA’s design shows that adaptive, importan...  \n","18  While AdaLoRA and IncreLoRA either prune or ex...  \n","19  As LLMs grow in size, full fine-tuning becomes...  \n","20  The study aims to assess the effectiveness of ...  \n","21  QA pairs are classified into ‘Factual’ and ‘Co...  \n","22  Models fine-tuned on conceptual datasets consi...  \n","23  The paper evaluates D-RAG and D-Naive syntheti...  \n","24  The task served as a practical demonstration s...  \n","25  The study shows that while PEFT is effective f...  \n","26  They conclude that sheer quantity of QA pairs ...  \n","27  D-RAG's limitations were attributed to the poo...  \n","28  Future research should explore improvements in...  \n","29  The paper highlights that PEFT's success hinge...  "],"text/html":["\n","  <div id=\"df-3f4c485a-3edb-4923-b689-7b0e75d17c38\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What is the primary innovation introduced by t...</td>\n","      <td>LoRI introduces a novel approach that freezes ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>How does LoRI reduce the number of trainable p...</td>\n","      <td>LoRI reduces the number of trainable parameter...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Why is sparsity in matrix B important in LoRI?</td>\n","      <td>Sparsity in matrix B enables LoRI to retain on...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>How does LoRI improve the process of merging a...</td>\n","      <td>LoRI enables more effective adapter merging by...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>What mechanism does LoRI use to mitigate catas...</td>\n","      <td>LoRI mitigates catastrophic forgetting by appl...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>On what benchmark did LoRI with 90% sparsity i...</td>\n","      <td>LoRI with 90% sparsity in B outperformed LoRA ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>How does LoRI compare to full fine-tuning and ...</td>\n","      <td>LoRI matches or outperforms full fine-tuning a...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>What types of tasks were used to evaluate LoRI...</td>\n","      <td>LoRI was evaluated on a diverse set of tasks, ...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>What potential future directions do the author...</td>\n","      <td>The authors suggest exploring structured spars...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>What is the broader significance of LoRI in th...</td>\n","      <td>LoRI provides a lightweight, modular, and scal...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>What are the core limitations of traditional L...</td>\n","      <td>ElaLoRA addresses two key limitations of tradi...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Describe the three core components of the ElaL...</td>\n","      <td>ElaLoRA's architecture consists of: (1) an SVD...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>How does ElaLoRA’s adaptive strategy improve p...</td>\n","      <td>ElaLoRA reallocates computational resources to...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>In what way does ElaLoRA achieve better task a...</td>\n","      <td>ElaLoRA uses gradient-derived importance score...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>What experimental evidence supports the superi...</td>\n","      <td>Experiments across NLU, NLG, and vision benchm...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Why is ElaLoRA particularly well-suited for re...</td>\n","      <td>ElaLoRA's dynamic pruning and expansion mechan...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>How does the final rank distribution in ElaLoR...</td>\n","      <td>ElaLoRA’s final rank distribution reveals that...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>What are the broader implications of ElaLoRA’s...</td>\n","      <td>ElaLoRA’s design shows that adaptive, importan...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>What distinguishes ElaLoRA from prior dynamic ...</td>\n","      <td>While AdaLoRA and IncreLoRA either prune or ex...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Why is parameter-efficient fine-tuning increas...</td>\n","      <td>As LLMs grow in size, full fine-tuning becomes...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>What is the primary goal of the study presente...</td>\n","      <td>The study aims to assess the effectiveness of ...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>How are QA pairs categorized in this study, an...</td>\n","      <td>QA pairs are classified into ‘Factual’ and ‘Co...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>What were the findings regarding models traine...</td>\n","      <td>Models fine-tuned on conceptual datasets consi...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Which synthetic dataset generation techniques ...</td>\n","      <td>The paper evaluates D-RAG and D-Naive syntheti...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>What was the significance of the product recom...</td>\n","      <td>The task served as a practical demonstration s...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Why do the authors argue that PEFT may not be ...</td>\n","      <td>The study shows that while PEFT is effective f...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>What conclusions do the authors draw about the...</td>\n","      <td>They conclude that sheer quantity of QA pairs ...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>What limitations of D-RAG were identified in t...</td>\n","      <td>D-RAG's limitations were attributed to the poo...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>How do the authors suggest future research sho...</td>\n","      <td>Future research should explore improvements in...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>What is the key insight this paper contributes...</td>\n","      <td>The paper highlights that PEFT's success hinge...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f4c485a-3edb-4923-b689-7b0e75d17c38')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3f4c485a-3edb-4923-b689-7b0e75d17c38 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3f4c485a-3edb-4923-b689-7b0e75d17c38');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-8ce281a6-5f5e-4cfe-bff0-bcf1b56ce718\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8ce281a6-5f5e-4cfe-bff0-bcf1b56ce718')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-8ce281a6-5f5e-4cfe-bff0-bcf1b56ce718 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"What limitations of D-RAG were identified in the study?\",\n          \"Why is ElaLoRA particularly well-suited for resource-constrained environments?\",\n          \"Which synthetic dataset generation techniques are evaluated in this work, and which one performs better?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"D-RAG's limitations were attributed to the poor performance of its underlying vector database retriever, leading to suboptimal training data quality.\",\n          \"ElaLoRA's dynamic pruning and expansion mechanism ensures that only the most essential ranks are trained, reducing memory usage and computational cost while maintaining high performance, making it ideal for low-resource scenarios.\",\n          \"The paper evaluates D-RAG and D-Naive synthetic data generation methods. D-Naive outperformed D-RAG in fine-tuning effectiveness, largely due to better retrieval performance.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["## Step 5: Combining All QA Pairs With Context"],"metadata":{"id":"EKA1byvzYFnK"}},{"cell_type":"code","source":["# Define paths\n","qa_folder = './qa_pairs/qa_pairs_eval_with_context'  # This should contain your new JSON files with 'context'\n","output_path = './data/eval_with_context.jsonl'"],"metadata":{"id":"8iVyfcEVCzIP","executionInfo":{"status":"ok","timestamp":1744568531775,"user_tz":300,"elapsed":1,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Aggregate all QA pairs\n","all_qa_pairs = []\n","\n","for filename in os.listdir(qa_folder):\n","    if filename.endswith('.json'):\n","        with open(os.path.join(qa_folder, filename), 'r') as f:\n","            data = json.load(f)\n","            for pair in data['qa_pairs']:\n","                all_qa_pairs.append({\n","                    \"question\": pair['question'].strip(),\n","                    \"answer\": pair['answer'].strip(),\n","                    \"context\": pair['context'].strip()\n","                })\n","\n","# Save to .jsonl file\n","with open(output_path, 'w') as f:\n","    for pair in all_qa_pairs:\n","        json.dump(pair, f)\n","        f.write('\\n')\n","\n","print(f\"uccessfully saved {len(all_qa_pairs)} QA pairs with context to {output_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JHqiwZ3cYeef","executionInfo":{"status":"ok","timestamp":1744568536104,"user_tz":300,"elapsed":23,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"0b60a45c-8894-45bc-cd55-48aa228a0e40"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["uccessfully saved 30 QA pairs with context to ./data/eval_with_context.jsonl\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hydmkzPVYmxE"},"execution_count":null,"outputs":[]}]}