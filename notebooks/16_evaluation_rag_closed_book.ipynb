{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5P83WGBXHRce"
   },
   "source": [
    "## Step 1: Mounting Google Drive and Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFCIzkr0HSED"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Navigate to the repo folder\n",
    "%cd /content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n",
    "\n",
    "# List repo contents\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70999,
     "status": "ok",
     "timestamp": 1747349096061,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "xiwoNZbeG4VE",
    "outputId": "2bb8e65d-1e54-4224-bad8-cade3b286d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate datasets openai sentence-transformers faiss-cpu bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 19160,
     "status": "ok",
     "timestamp": 1747349138176,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "pSxxurWtHjDE"
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "from typing import List\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# HF / model-specific imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# BERTScore + OpenAI Eval\n",
    "from bert_score import score as bertscore\n",
    "from openai import OpenAI\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFC7qGOaICO0"
   },
   "source": [
    "## Step 2: Loading the Validation Set for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1747349169917,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "SHKhC3Y7H5_A",
    "outputId": "25ef0e45-3beb-4a31-cc15-a67f19bb0116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 QA pairs for RAG evaluation.\n"
     ]
    }
   ],
   "source": [
    "eval_path = \"./data/eval_with_context.jsonl\"\n",
    "\n",
    "eval_pairs = []\n",
    "with open(eval_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        eval_pairs.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"Loaded {len(eval_pairs)} QA pairs for RAG evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-kUyvWPJ5hO"
   },
   "source": [
    "## Step 3: Load the Fine-Tuned RAG Model and FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1747349673707,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "Mr5B0SeRIGPQ"
   },
   "outputs": [],
   "source": [
    "EMBED_MODEL       = \"BAAI/bge-base-en-v1.5\"\n",
    "MODEL_PATH        = \"./models/merged-finetuned-mistral\"\n",
    "FAISS_INDEX_PATH  = \"./data/rag_corpus/faiss_index.bin\"\n",
    "METADATA_PATH     = \"./data/rag_corpus/chunk_metadata.json\"\n",
    "\n",
    "DEVICE            = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CTX_TOKEN_LIMIT   = 2048\n",
    "MAX_NEW_TOKENS    = 256\n",
    "TOP_K             = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1095,
     "status": "ok",
     "timestamp": 1747349692034,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "m30TPOmAKBZP"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747349711070,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "4bhHlFrAKFnX"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token # to avoid padding error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "17d5591cd18d40149a4f665984746e09",
      "769c343e3cb74fcaaf9a4ebc3d1a782c",
      "3f6af54428aa46c682b727038233be08",
      "19fbe0edb6aa4369885457dba7662c79",
      "526c245eab834e6bb1346db681fa0838",
      "f337bab483cc49e9b1908ceb31865132",
      "46da6b0527b94037ab5fe2dbe0a18a4d",
      "bc116993b7824259b1a02f9df56c79e5",
      "3d3945fe41f142a4a82b45301675425d",
      "a4923672500b48d1837bc242054eb1fb",
      "584642cd86a4411d83b59b1b63fc9cf0"
     ]
    },
    "executionInfo": {
     "elapsed": 250010,
     "status": "ok",
     "timestamp": 1747349974944,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "7gR_qWthKKiF",
    "outputId": "9a59138e-8730-4ed8-a537-45c1ab0369de"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d5591cd18d40149a4f665984746e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747350014810,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "rv8TRFwSKN6o",
    "outputId": "3cc6534b-5e7e-410b-bfdb-1a24ccb980b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2977,
     "status": "ok",
     "timestamp": 1747350085770,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "omsWMAqqLUrn"
   },
   "outputs": [],
   "source": [
    "# Load FAISS + Metadata\n",
    "index = faiss.read_index(str(FAISS_INDEX_PATH))\n",
    "with open(METADATA_PATH) as f:\n",
    "    chunk_metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ra5OGq9ZLlRm"
   },
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(EMBED_MODEL, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747350125391,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "FDeHMDhALndJ"
   },
   "outputs": [],
   "source": [
    "def retrieve_chunks(query: str, k: int = TOP_K) -> List[dict]:\n",
    "    \"\"\"Return top-k chunks (dicts with 'title' & 'text' fields).\"\"\"\n",
    "    q_emb = embedder.encode([query], normalize_embeddings=True)\n",
    "    _, idxs = index.search(q_emb, k)\n",
    "    return [chunk_metadata[int(i)] for i in idxs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747350152401,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "8hQ4PQjLLvrn"
   },
   "outputs": [],
   "source": [
    "def build_prompt_rag(question: str,\n",
    "                     k: int = TOP_K,\n",
    "                     ctx_limit: int = CTX_TOKEN_LIMIT) -> str:\n",
    "    \"\"\"\n",
    "    Compose prompt using ONLY RAG-retrieved chunks.\n",
    "    Stops adding chunks when token budget (`ctx_limit`) would be exceeded.\n",
    "    \"\"\"\n",
    "    blocks, n_tokens = [], 0\n",
    "    for ch in retrieve_chunks(question, k):\n",
    "        blk = f\"[{ch['title']}]\\n{ch['text']}\\n\"\n",
    "        t   = len(tokenizer.tokenize(blk))\n",
    "        if n_tokens + t <= ctx_limit:\n",
    "            blocks.append(blk)\n",
    "            n_tokens += t\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    context = \"\\n\\n\".join(blocks)\n",
    "    prompt  = (\n",
    "        \"You are an expert scientific assistant. Use the excerpts to answer.\\n\\n\"\n",
    "        f\"Excerpts:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\nAnswer:\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747350177716,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "cqo_kWujL2Rx"
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_answer_rag(question: str) -> str:\n",
    "    \"\"\"Generate answer using *only* RAG context (closed-book).\"\"\"\n",
    "    prompt = build_prompt_rag(question)\n",
    "    inputs = tokenizer(prompt,\n",
    "                       return_tensors=\"pt\",\n",
    "                       padding=True,\n",
    "                       truncation=True,\n",
    "                       max_length=CTX_TOKEN_LIMIT).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    # keep only the newly-generated tokens\n",
    "    gen_ids   = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    answer = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q07h63CEMCUG"
   },
   "source": [
    "## Step 4: Generate predictions using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHDw4Rg4L8dK"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, item in enumerate(eval_pairs):\n",
    "    question  = item[\"question\"]\n",
    "    reference = item[\"answer\"]\n",
    "\n",
    "    prediction = generate_answer_rag(question)\n",
    "\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"prediction\": prediction,\n",
    "    })\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"[{i}/{len(eval_pairs)}] Question: {question}\\n→ {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747351693099,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "tRCDteyMRqxl"
   },
   "outputs": [],
   "source": [
    "# Save for evaluation\n",
    "with open(\"eval_predictions_closed_book_rag.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1747351521872,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "42FqmhMJM2Lc",
    "outputId": "8605cac1-7768-4078-d2b9-e824fbf357c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to: ./data/evaluation/eval_predictions_closed_book_rag.json\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./data/evaluation/eval_predictions_closed_book_rag.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Saved predictions to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4kpJJYYRON1"
   },
   "source": [
    "## Step 5: BLEU Score Evaluation\n",
    "\n",
    "In this section, we evaluate our fine-tuned model using the **BLEU (Bilingual Evaluation Understudy)** score, a standard metric for evaluating the quality of generated text by comparing it to a reference answer.\n",
    "\n",
    "### What is BLEU?\n",
    "BLEU measures *n-gram overlap* between the model's prediction and the reference answer:\n",
    "- **BLEU-1**: unigram overlap (word-level similarity)\n",
    "- **BLEU-2**: bigram overlap (2-word chunks)\n",
    "- **BLEU-3**: trigram overlap\n",
    "- **BLEU-4**: 4-gram overlap (more stringent)\n",
    "\n",
    "### Components of the Code:\n",
    "- `weights=(1, 0, 0, 0)`: Measures unigram overlap only (BLEU-1).\n",
    "- `smoothing_function=method1`: Prevents the BLEU score from dropping to 0 when there are no exact n-gram matches. This is useful for short or paraphrased responses.\n",
    "- We iterate over our evaluation dataset and compute BLEU-1 through BLEU-4 for each response.\n",
    "\n",
    "### Limitations:\n",
    "BLEU is a **surface-level** metric:\n",
    "- It penalizes paraphrasing.\n",
    "- It doesn't understand meaning—only *form*.\n",
    "- It is useful for rough comparison, but **not sufficient alone** to assess model quality.\n",
    "\n",
    "Hence, we will also perform **qualitative evaluation** using *LLM-as-a-Judge* in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747351696551,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "lsj2uXOJREgP"
   },
   "outputs": [],
   "source": [
    "# Load predictions with context\n",
    "with open(\"eval_predictions_closed_book_rag.json\", \"r\") as f:\n",
    "    eval_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747351722162,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "V378PalNReME"
   },
   "outputs": [],
   "source": [
    "# Initialize smoothing function and score containers\n",
    "smooth = SmoothingFunction().method1\n",
    "bleu_scores = {f\"BLEU-{n}\": [] for n in range(1, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1747351730191,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "_G-3kj4OR1g8",
    "outputId": "1561eb73-7563-446a-ae0a-3bf503a7141e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Scores: {'BLEU-1': 0.2327, 'BLEU-2': 0.1221, 'BLEU-3': 0.0804, 'BLEU-4': 0.0595}\n"
     ]
    }
   ],
   "source": [
    "# Iterate over predictions and compute BLEU-1 to BLEU-4\n",
    "for item in eval_results:\n",
    "    reference = item[\"reference\"].split()\n",
    "    prediction = item[\"prediction\"].split()\n",
    "\n",
    "    bleu_scores[\"BLEU-1\"].append(\n",
    "        sentence_bleu([reference], prediction, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "    )\n",
    "    bleu_scores[\"BLEU-2\"].append(\n",
    "        sentence_bleu([reference], prediction, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
    "    )\n",
    "    bleu_scores[\"BLEU-3\"].append(\n",
    "        sentence_bleu([reference], prediction, weights=(1/3, 1/3, 1/3, 0), smoothing_function=smooth)\n",
    "    )\n",
    "    bleu_scores[\"BLEU-4\"].append(\n",
    "        sentence_bleu([reference], prediction, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "    )\n",
    "\n",
    "# Compute and display average scores\n",
    "avg_bleu_scores = {metric: round(sum(scores)/len(scores), 4) for metric, scores in bleu_scores.items()}\n",
    "print(\"Average BLEU Scores:\", avg_bleu_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAOeluczSBm6"
   },
   "source": [
    "## Step 6: Using GPT-4o as LLM-as-a-Judge (OpenAI Evaluation)\n",
    "\n",
    "In this section, we use **GPT-4o**—a state-of-the-art model from OpenAI—as a neutral third-party judge to evaluate the quality of our model’s predictions against ground truth answers. This is part of the **LLM-as-a-Judge** evaluation methodology, which is growing in popularity as a way to assess open-ended outputs where metrics like BLEU or ROUGE may fall short.\n",
    "\n",
    "**What this section does:**\n",
    "\n",
    "- Loads model predictions from `eval_openbook_predictions.json`\n",
    "- Uses a GPT-4o prompt that provides:\n",
    "  - The question\n",
    "  - The model's generated answer\n",
    "  - The reference (ground-truth) answer\n",
    "- Asks GPT-4o to score the generated answer on a **scale from 1 to 5**, considering relevance, correctness, completeness, and style\n",
    "- Stores all outputs in `gpt4o_judgments_openbook.json` for analysis\n",
    "\n",
    "**Key Functions:**\n",
    "\n",
    "- `ask_gpt_judge()` → Sends a prompt to GPT-4o via the OpenAI API and returns a numeric score\n",
    "- `judged_results` → A list of evaluation records including the question, reference, model prediction, and GPT-4o's score\n",
    "- `np.mean()` → Used at the end to compute the **average evaluation score** across all QA pairs\n",
    "\n",
    "**Why use GPT-4o?**\n",
    "\n",
    "Because LLMs are best judged by **other LLMs** capable of contextual understanding. GPT-4o has been shown to be highly consistent and reliable in comparative evaluations.\n",
    "\n",
    "This evaluation complements our BLEU score by offering a **semantic and qualitative assessment**, helping us better understand the strengths and weaknesses of our fine-tuned model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13533,
     "status": "ok",
     "timestamp": 1747351851321,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "3agQCrvWR3dn",
    "outputId": "3a50c17d-c115-416c-f701-b6e4b5b8dc5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:··········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747351869910,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "YtPNvazcSRwC"
   },
   "outputs": [],
   "source": [
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1747351877803,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "JLrQwODySZln"
   },
   "outputs": [],
   "source": [
    "# Load the API key from environment variable\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747351897780,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "p1ulMoApSbfy"
   },
   "outputs": [],
   "source": [
    "def ask_gpt_judge(question, reference, prediction):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert model evaluator. Given a question, a reference answer, and a model-generated answer that was generated with access to a relevant excerpt from a scientific paper, judge how good the model’s answer is on a scale of 1 to 5. Use the following rubric:\n",
    "\n",
    "1 – Completely irrelevant or hallucinated.\n",
    "2 – Partially related but mostly inaccurate.\n",
    "3 – Mostly accurate but missing key details.\n",
    "4 – Accurate and mostly complete.\n",
    "5 – Nearly identical in meaning to the reference.\n",
    "\n",
    "Be strict but fair. Output ONLY the number.\n",
    "\n",
    "Question: {question}\n",
    "Reference Answer: {reference}\n",
    "Model Prediction: {prediction}\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(\"Error during evaluation:\\n\")\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747351930041,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "aWqHii5lSgZJ"
   },
   "outputs": [],
   "source": [
    "with open(\"eval_predictions_closed_book_rag.json\") as f:\n",
    "    eval_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ce4IbhFzSoRR"
   },
   "outputs": [],
   "source": [
    "judged_results = []\n",
    "\n",
    "for i, item in enumerate(eval_results):\n",
    "    print(f\"Evaluating {i+1}/{len(eval_results)}\")\n",
    "    score = ask_gpt_judge(item[\"question\"], item[\"reference\"], item[\"prediction\"])\n",
    "    if score:\n",
    "        judged_results.append({\n",
    "            \"question\": item[\"question\"],\n",
    "            \"reference\": item[\"reference\"],\n",
    "            \"prediction\": item[\"prediction\"],\n",
    "            \"gpt4o_score\": score\n",
    "        })\n",
    "    time.sleep(1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747352376857,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "q808sy34SsOu"
   },
   "outputs": [],
   "source": [
    "with open(\"gpt4o_judgments_closedbook_rag.json\", \"w\") as f:\n",
    "    json.dump(judged_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1747352378301,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "ZP83R8j7S-98",
    "outputId": "c26f3455-e6a9-43e8-bd42-9c2993b149db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: What is the primary innovation introduced by the LoRI method for parameter-efficient fine-tuning?\n",
      " Reference Answer: LoRI introduces a novel approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks, thereby significantly reducing trainable parameters while minimizing cross-task interference.\n",
      " Model Prediction: LoRI introduces a novel approach to parameter-efficient fine-tuning by leveraging low-rank approximations of the pre-trained weight matrix, updating only the low-rank components while keeping the majority of pre-trained parameters frozen.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does LoRI reduce the number of trainable parameters compared to traditional LoRA?\n",
      " Reference Answer: LoRI reduces the number of trainable parameters by keeping matrix A fixed as a random projection and sparsifying matrix B using task-specific masks, eliminating the need to train both matrices and reducing redundancy.\n",
      " Model Prediction: LoRI reduces the number of trainable parameters by freezing matrices A and applying 90% sparsity to matrices B, achieving a 95% reduction in trainable parameters compared to LoRA. Despite this, LoRI-D and LoRI-S achieve performance comparable to – and even better than – LoRA and DoRA on various tasks.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why is sparsity in matrix B important in LoRI?\n",
      " Reference Answer: Sparsity in matrix B enables LoRI to retain only the most critical elements necessary for adaptation, reducing parameter count and mitigating cross-task interference during adapter merging and continual learning.\n",
      " Model Prediction: Sparsity in matrix B is crucial in LoRI because it enables selective adaptation, allowing the model to focus on modifying only the most critical parameters needed for specific tasks while preserving the foundational knowledge encoded in the pretrained base model. This selective adaptation reduces the risk of overfitting and improves generalization.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does LoRI improve the process of merging adapters in multi-task scenarios?\n",
      " Reference Answer: LoRI enables more effective adapter merging by using fixed, randomly initialized projection matrices A, which maps task-specific adapters into approximately orthogonal subspaces, thus reducing parameter interference.\n",
      " Model Prediction: LoRI enables adapter merging without manual selection of merging methods by using fixed, randomly initialized projection A. This reduces parameter interference when merging multiple LoRIs, thereby improving performance in the merged LoRA compared to single-task LoRAs.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What mechanism does LoRI use to mitigate catastrophic forgetting in continual learning?\n",
      " Reference Answer: LoRI mitigates catastrophic forgetting by applying task-specific sparse masks to matrix B, which isolates parameter updates across tasks and preserves knowledge from previous adaptations, including safety alignment.\n",
      " Model Prediction: LoRI leverages sparse updates and orthogonalization to maintain the orthogonality between adapter subspaces, thereby reducing cross-task interference and mitigating catastrophic forgetting.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: On what benchmark did LoRI with 90% sparsity in B outperform LoRA, and by how much?\n",
      " Reference Answer: LoRI with 90% sparsity in B outperformed LoRA by 17.3% on the HumanEval benchmark using the Llama-3 model.\n",
      " Model Prediction: On the NLU benchmark, LoRI with 90% sparsity in B outperformed LoRA by 1.2 points, achieving 57.1 compared to LoRA's 55.9.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does LoRI compare to full fine-tuning and other PEFT methods in terms of performance and efficiency?\n",
      " Reference Answer: LoRI matches or outperforms full fine-tuning and other PEFT methods across multiple domains while using up to 95% fewer trainable parameters than LoRA, demonstrating both high performance and high efficiency.\n",
      " Model Prediction: LoRA outperforms full fine-tuning and other PEFT methods in terms of performance, while maintaining or even reducing the number of trainable parameters.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What types of tasks were used to evaluate LoRI's effectiveness?\n",
      " Reference Answer: LoRI was evaluated on a diverse set of tasks, including natural language understanding, mathematical reasoning, code generation, and safety alignment.\n",
      " Model Prediction: LoRI was evaluated on four types of tasks: Natural Language Understanding (NLU), Safety, and two benchmarks, HumanEval and HEx-PHI. The NLU tasks include BoolQ, PIQA, SocialIQA, ARC-Challenge, ARC-Easy, OpenBookQA, and GSM8K.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What potential future directions do the authors propose for extending LoRI?\n",
      " Reference Answer: The authors suggest exploring structured sparsity patterns like block sparsity or head pruning and adapting LoRI to multi-modal models such as diffusion and vision-language systems.\n",
      " Model Prediction: The authors suggest exploring the modularization of LoRA for applications such as federated learning, where strategies to minimize communication overhead and expedite model convergence through sharing and aggregating MSUs could be developed.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What is the broader significance of LoRI in the context of PEFT and LLM deployment?\n",
      " Reference Answer: LoRI provides a lightweight, modular, and scalable solution for adapting LLMs with minimal overhead, making it particularly suited for multi-task learning, safety-critical alignment, and efficient deployment on resource-constrained hardware.\n",
      " Model Prediction: LoRI represents a significant advancement in PEFT, demonstrating that it is possible to achieve strong single-task performance across diverse domains while using up to 95% fewer trainable parameters than LoRA. Furthermore, LoRI enables training-free adapter merging and supports continual learning with reduced catastrophic forgetting, making it a promising approach for building safety adapters and deploying LLMs in resource-constrained environments.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What are the core limitations of traditional LoRA methods that ElaLoRA seeks to address?\n",
      " Reference Answer: ElaLoRA addresses two key limitations of traditional LoRA: the fixed rank allocation across layers, which overlooks the layer-specific importance, and the inability to adapt ranks dynamically during training, which can lead to suboptimal parameter efficiency.\n",
      " Model Prediction: Traditional LoRA methods assume that the rank of the updated parameters is low, which may break down in cases of pre-training or when there is a large amount of fine-tuning data. Additionally, LoRA requires more tasks to ramp up cross-task generalization than adapter and finetuning, and it suffers from training instability when the number of training tasks is low.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Describe the three core components of the ElaLoRA framework.\n",
      " Reference Answer: ElaLoRA's architecture consists of: (1) an SVD-based adaptation strategy for matrix decomposition, (2) an importance score calculation mechanism based on loss gradients to assess rank relevance, and (3) a dynamic rank learning algorithm that reallocates ranks periodically during training to optimize layer-wise adaptation.\n",
      " Model Prediction: The ElaLoRA framework consists of three core components: (1) Warm-up, where ranks remain fixed for the first two iterations, allowing the model to initialize its representations; (2) Dynamic Rank Adjustment, where ranks are adjusted every tadjust iterations by pruning and expanding based on calculated importance scores; and (3) Stabilization, where rank updates are frozen for the final tstabilize iterations, allowing the model to converge smoothly.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does ElaLoRA’s adaptive strategy improve performance under limited parameter budgets?\n",
      " Reference Answer: ElaLoRA reallocates computational resources to the most critical layers by pruning less important ranks and expanding ranks in essential layers, thus achieving higher performance even under smaller parameter budgets—for example, outperforming other PEFT methods with r=2 compared to their r=4 settings.\n",
      " Model Prediction: ElaLoRA dynamically adjusts ranks based on gradient-derived importance scores, enabling both rank pruning and expansion during fine-tuning. This adaptive strategy allows ElaLoRA to effectively allocate resources, ensuring that even low-rank components contribute significantly. This results in improved performance under limited parameter budgets compared to fixed-rank methods like LoRA.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: In what way does ElaLoRA achieve better task alignment during fine-tuning?\n",
      " Reference Answer: ElaLoRA uses gradient-derived importance scores to identify which layers contribute most to task-specific learning, allowing the model to allocate more capacity to those layers and thus improving task alignment and learning efficiency.\n",
      " Model Prediction: ElaLoRA dynamically prunes and expands ranks based on importance scores, ensuring that the most impactful layers receive additional capacity while removing redundant ranks. This adaptive rank learning mechanism enables more efficient model adaptation across diverse NLP and Vision tasks.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What experimental evidence supports the superiority of ElaLoRA over other PEFT methods?\n",
      " Reference Answer: Experiments across NLU, NLG, and vision benchmarks show that ElaLoRA consistently outperforms state-of-the-art PEFT methods in accuracy, particularly under constrained parameter budgets, and demonstrates better GLUE benchmark performance even with fewer trainable parameters.\n",
      " Model Prediction: ElaLoRA consistently outperforms existing PEFT methods across various parameter budgets, even achieving better average GLUE results with r = 2 than other methods at r = 4. This makes it particularly well-suited for resource-constrained environments.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why is ElaLoRA particularly well-suited for resource-constrained environments?\n",
      " Reference Answer: ElaLoRA's dynamic pruning and expansion mechanism ensures that only the most essential ranks are trained, reducing memory usage and computational cost while maintaining high performance, making it ideal for low-resource scenarios.\n",
      " Model Prediction: ElaLoRA achieves better average GLUE results with r = 2 than other PEFT methods at r = 4, making it particularly well-suited for resource-constrained environments.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does the final rank distribution in ElaLoRA reflect its adaptive learning process?\n",
      " Reference Answer: ElaLoRA’s final rank distribution reveals that higher ranks are allocated to layers deemed more important via importance scores, confirming that the model dynamically concentrates learning capacity on the most impactful parts of the network.\n",
      " Model Prediction: The final rank distribution in ElaLoRA reflects its adaptive learning process by prioritizing the most important ranks and pruning the least important ones. This ensures that resources are allocated efficiently while preserving expressivity and avoiding computational redundancy.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What are the broader implications of ElaLoRA’s design for the future of fine-tuning large models?\n",
      " Reference Answer: ElaLoRA’s design shows that adaptive, importance-based rank allocation can significantly improve parameter efficiency without sacrificing accuracy, suggesting a paradigm shift toward more intelligent and resource-aware fine-tuning strategies.\n",
      " Model Prediction: ElaLoRA’s adaptive low-rank adaptation framework, which dynamically prunes and expands ranks based on gradient-derived importance scores, represents a significant step forward in the fine-tuning of large models. By enabling both rank pruning and expansion, ElaLoRA outperforms existing PEFT methods across different parameter budgets, demonstrating its potential to become a cornerstone technique for efficient fine-tuning in the future.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What distinguishes ElaLoRA from prior dynamic rank methods like AdaLoRA or IncreLoRA?\n",
      " Reference Answer: While AdaLoRA and IncreLoRA either prune or expand ranks, ElaLoRA is the first to implement both pruning and expansion dynamically during training, offering a more flexible and principled mechanism for allocating parameter capacity.\n",
      " Model Prediction: Unlike AdaLoRA and IncreLoRA, which either prune or expand ranks statically, ElaLoRA dynamically adjusts ranks based on gradient-derived importance scores, ensuring efficient resource allocation throughout fine-tuning.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why is parameter-efficient fine-tuning increasingly important in the LLM landscape?\n",
      " Reference Answer: As LLMs grow in size, full fine-tuning becomes prohibitively expensive, especially for domain-specific or low-resource settings. PEFT methods like ElaLoRA offer a practical solution by enabling adaptation with minimal compute and storage costs.\n",
      " Model Prediction: Parameter-efficient fine-tuning (PEFT) is crucial in the LLM landscape due to the enormous size of models, which poses significant challenges in terms of computational complexity and resource requirements during fine-tuning. PEFT methods adapt LLMs to downstream tasks while minimizing the number of trainable parameters, thereby reducing computational and memory overhead.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What is the primary goal of the study presented in 'Beyond QA Pairs'?\n",
      " Reference Answer: The study aims to assess the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain-specific facts into LLMs, focusing on the impact of QA pair categorization and synthetic dataset generation techniques.\n",
      " Model Prediction: The primary goal of the study is to develop a method for generating instruction-response pairs that can be used to train large language models in a more effective and efficient manner.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How are QA pairs categorized in this study, and what is the purpose of this categorization?\n",
      " Reference Answer: QA pairs are classified into ‘Factual’ and ‘Conceptual’ categories using a BERT-based classifier. The purpose is to investigate how the nature of QA pairs influences the effectiveness of PEFT.\n",
      " Model Prediction: The QA pairs are categorized based on the defined RAG scenarios, which include single-hop and multi-hop QA. This categorization helps explore the performance of RAG methods across different scenarios.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What were the findings regarding models trained on conceptual vs factual QA datasets?\n",
      " Reference Answer: Models fine-tuned on conceptual datasets consistently outperformed those trained on factual datasets across multiple evaluations.\n",
      " Model Prediction: Models trained on conceptual QA datasets performed better on reasoning tasks, while models trained on factual QA datasets performed better on factual recall tasks.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Which synthetic dataset generation techniques are evaluated in this work, and which one performs better?\n",
      " Reference Answer: The paper evaluates D-RAG and D-Naive synthetic data generation methods. D-Naive outperformed D-RAG in fine-tuning effectiveness, largely due to better retrieval performance.\n",
      " Model Prediction: The work evaluates both distillation and self-improvement techniques for synthetic data generation. However, the results suggest that distillation, specifically LoRA, outperforms self-improvement in terms of computational efficiency and expressiveness.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What was the significance of the product recommendation task in the data center domain?\n",
      " Reference Answer: The task served as a practical demonstration showing that a Llama-2 7B model fine-tuned with PEFT on just 1,000 instruction-based QA pairs significantly outperformed the baseline in generating product recommendations.\n",
      " Model Prediction: The product recommendation task was crucial in the data center domain as it helped customers make informed decisions about their hardware purchases, ultimately improving overall system performance and efficiency.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why do the authors argue that PEFT may not be optimal for factual embedding?\n",
      " Reference Answer: The study shows that while PEFT is effective for instruction tuning, it struggles with embedding factual information as effectively, likely due to its limited parameter update scope.\n",
      " Model Prediction: The authors argue that PEFT methods like LoRA may not be optimal for factual embedding because they do not update the entire model, potentially leading to inconsistencies and inaccuracies in the output. This is particularly problematic when the input contains factual information that the model is not trained to handle.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What conclusions do the authors draw about the volume versus quality of QA data in PEFT?\n",
      " Reference Answer: They conclude that sheer quantity of QA pairs is insufficient; quality and conceptual depth are far more critical for successful PEFT.\n",
      " Model Prediction: The authors conclude that while a large volume of QA data is not necessary for PEFT, high-quality data is crucial for achieving optimal performance.\n",
      " GPT-4o Evaluation: 5\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What limitations of D-RAG were identified in the study?\n",
      " Reference Answer: D-RAG's limitations were attributed to the poor performance of its underlying vector database retriever, leading to suboptimal training data quality.\n",
      " Model Prediction: The study found that D-RAG struggled with long-term memory and context retention, particularly in multi-turn dialogues. It also had issues with handling out-of-scope queries and maintaining coherence in generated responses.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How do the authors suggest future research should improve PEFT for fact embedding?\n",
      " Reference Answer: Future research should explore improvements in retrieval systems used by D-RAG, and consider more refined QA classification and data generation strategies.\n",
      " Model Prediction: The authors suggest future research should focus on improving the quality of fact embeddings by incorporating more diverse and accurate information, and developing more sophisticated methods for selecting and updating relevant parameters.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What is the key insight this paper contributes to the field of LLM fine-tuning?\n",
      " Reference Answer: The paper highlights that PEFT's success hinges more on dataset composition—especially the conceptual quality of QA pairs—than on volume alone, and that careful use-case targeting is essential.\n",
      " Model Prediction: The paper introduces a novel parameter-efficient fine-tuning method called Extreme Gradient Boost Rank-1 Adaption (EGBRA), which significantly reduces the computational and memory overhead of fine-tuning large language models (LLMs) without sacrificing performance.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sample in judged_results:\n",
    "    print(\" Question:\", sample[\"question\"])\n",
    "    print(\" Reference Answer:\", sample[\"reference\"])\n",
    "    print(\" Model Prediction:\", sample[\"prediction\"])\n",
    "    print(\" GPT-4o Evaluation:\", sample[\"gpt4o_score\"])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1747352329317,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "h7qszNFvTEP2",
    "outputId": "6b3d2302-7a55-43ce-c659-7895f1601108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-4o Evaluation Score: 2.60 out of 5\n"
     ]
    }
   ],
   "source": [
    "# Calculating the average score\n",
    "scores = [int(res[\"gpt4o_score\"]) for res in judged_results if res[\"gpt4o_score\"].isdigit()]\n",
    "average_score = np.mean(scores)\n",
    "print(f\"Average GPT-4o Evaluation Score: {average_score:.2f} out of 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747352465504,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "Gk8MyhmdTJio",
    "outputId": "17d662b8-f01a-4cf3-80dd-604509815ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judged results saved to ./data/evaluation/eval_gpt4o_judgments_closed_book_rag.json\n"
     ]
    }
   ],
   "source": [
    "# Saving the results\n",
    "\n",
    "output_path = \"./data/evaluation/eval_gpt4o_judgments_closed_book_rag.json\"\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(judged_results, f, indent=2)\n",
    "\n",
    "print(f\"Judged results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UozFz7BjUxXa"
   },
   "source": [
    "## Step 7: Evaluating with BERTScore (Semantic Similarity Metric)\n",
    "\n",
    "In this section, we evaluate the semantic similarity between the model’s predictions and the ground truth answers using **BERTScore**, a metric that leverages contextual embeddings from large pretrained models (like BERT) to assess the *meaning* of the outputs.\n",
    "\n",
    "Unlike BLEU, which only considers surface-level n-gram overlap, BERTScore measures how semantically close the answers are—even when the phrasing differs.\n",
    "\n",
    "### Interpretation:\n",
    "- **BERTScore F1** reflects the degree of **semantic overlap** between model output and human-labeled answer.\n",
    "- A score closer to **1.0** indicates stronger alignment of meaning.\n",
    "- This metric is especially useful in open-ended QA or summarization settings where **exact matching isn't expected**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1747352510794,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "caLk_nz-Uq_c"
   },
   "outputs": [],
   "source": [
    "# Replace `results` with `judged_results` if needed\n",
    "predictions = [item[\"prediction\"] for item in results]\n",
    "references = [item[\"reference\"] for item in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILSr9TrJU2Cz"
   },
   "outputs": [],
   "source": [
    "P, R, F1 = bertscore(predictions, references, lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1747352535372,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "JkR-U2e5U30p",
    "outputId": "d4996138-6477-4529-f23c-50aa828bcb04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.2910\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Precision: {P.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747352544527,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "7Dd0ZfbQU8Da",
    "outputId": "106f3c46-af87-456b-e37d-7c81f297a23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall: 0.3422\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Recall: {R.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747352551806,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "nV3mpU33U-Sf",
    "outputId": "97f89187-33fb-46c8-84d9-123a12d4553f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall: 0.3422\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Recall: {R.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWTPxAA3VgOG"
   },
   "source": [
    "## Step 8: Fixing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4234,
     "status": "ok",
     "timestamp": 1747352735612,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "LItrsGcOVAEY"
   },
   "outputs": [],
   "source": [
    "pip install nbformat --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13654,
     "status": "ok",
     "timestamp": 1747352766863,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "-MUEnmpbVqb6",
    "outputId": "b412c1e1-9bc1-41cf-a9a1-8cd2b2a8065f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4bEORB3VxPI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPspG++xLnR+yLZaDjZkenY",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
