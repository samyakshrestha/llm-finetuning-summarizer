{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY-hVizBkSW1"
   },
   "source": [
    "## Step 1: Mounting Google Drive and Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23031,
     "status": "ok",
     "timestamp": 1747289489429,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "jb5M8dVdfB4s",
    "outputId": "c9b4bcd6-43c8-4b99-a9c6-7f1076c0be63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n",
      "data\t\t\t\tLICENSE\t\t qa_pairs   wandb\n",
      "deployment\t\t\tmodels\t\t README.md\n",
      "eval_predictions_baseline.json\tnotebooks\t results\n",
      "gpt4o_judgments_baseline.json\tproject_plan.md  scripts\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Navigate to the repo folder\n",
    "%cd /content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n",
    "\n",
    "# List repo contents\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxZcau8NkUOX"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate datasets openai sentence-transformers faiss-cpu bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747300946888,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "_nENU2jJkeGK"
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "from typing import List\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# HF / model-specific imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# BERTScore + OpenAI Eval\n",
    "from bert_score import score as bertscore\n",
    "from openai import OpenAI\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XpJm2Pdk5MO"
   },
   "source": [
    "## Step 2: Loading the Validation Set for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1747289652865,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "Myz8FnZZkzTs",
    "outputId": "a9774bae-1f51-4716-d580-5f682b6a149d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 QA pairs for RAG evaluation.\n"
     ]
    }
   ],
   "source": [
    "eval_path = \"./data/eval_with_context.jsonl\"\n",
    "\n",
    "eval_pairs = []\n",
    "with open(eval_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        eval_pairs.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"Loaded {len(eval_pairs)} QA pairs for RAG evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEZw7PWcpAFv"
   },
   "source": [
    "## Step 3: Load the Fine-Tuned RAG Model and FAISS Index\n",
    "\n",
    "In this step, we load the LoRA-fine-tuned Mistral-7B model along with the FAISS index and chunk metadata used for retrieval.\n",
    "\n",
    "Components loaded:\n",
    "- **Tokenizer**: For tokenizing prompts and decoding model output.\n",
    "- **Fine-tuned model**: Our LoRA-adapted Mistral model, loaded with 4-bit quantization if available.\n",
    "- **FAISS index**: A pre-computed vector store of document chunks, used to retrieve the most relevant contexts for a given question.\n",
    "- **Chunk metadata**: Contains the actual content and titles of the document chunks associated with each FAISS vector.\n",
    "\n",
    "This setup allows the model to perform Retrieval-Augmented Generation (RAG) by grounding answers in semantically retrieved chunks at inference time. The model is now ready to generate context-informed answers for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1747291307081,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "O64HCxV_lDtn"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBED_MODEL = \"BAAI/bge-base-en-v1.5\"\n",
    "MODEL_PATH = \"./models/merged-finetuned-mistral\"\n",
    "FAISS_INDEX_PATH = \"./data/rag_corpus/faiss_index.bin\"\n",
    "METADATA_PATH = \"./data/rag_corpus/chunk_metadata.json\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CTX_TOKEN_LIMIT = 2048\n",
    "MAX_NEW_TOKENS = 256\n",
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1636,
     "status": "ok",
     "timestamp": 1747290839638,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "9L2JQr2opD11"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1747296188350,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "C2cXuikV9-nu"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9e7101dd40c24d0db13c68b9ee06eecf",
      "9133139571f04510a7917d32759f5bf4",
      "fc18d1dbffc1421ebfd4ddb2a24215e8",
      "32fb5bd1f64b4e02b6bdc849065af2e1",
      "091faa1e29944bddbb5536820167f08f",
      "f10dedd615074538ac3016067e59abef",
      "132bce8382a04d91ad679f2d2b9d7deb",
      "7e4b71dd304d459498822570d39f68a7",
      "80885d72d1b8489eae27288f3b850a00",
      "6682fcb3aec4450e863fe9522b814ef1",
      "83dab6159bed43d4a9e65415e66aa6bd"
     ]
    },
    "executionInfo": {
     "elapsed": 238874,
     "status": "ok",
     "timestamp": 1747291108868,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "zShuiX3QplOH",
    "outputId": "8a0c0ff4-c705-45ce-dc4f-570e29a88a65"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7101dd40c24d0db13c68b9ee06eecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4029,
     "status": "ok",
     "timestamp": 1747291319909,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "LEIibeD3ptCE"
   },
   "outputs": [],
   "source": [
    "# Load FAISS index and metadata\n",
    "index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "with open(METADATA_PATH) as f:\n",
    "    chunk_metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZ-zw_QCrZ4l"
   },
   "outputs": [],
   "source": [
    "# Load embedder\n",
    "embedder = SentenceTransformer(EMBED_MODEL, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1747299574789,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "S2wmMFNsrdkN"
   },
   "outputs": [],
   "source": [
    "def retrieve_chunks(query: str, k: int = TOP_K) -> List[dict]:\n",
    "    \"\"\"Return top-k chunks (dicts with 'title' & 'text').\"\"\"\n",
    "    q_emb = embedder.encode([query], normalize_embeddings=True)\n",
    "    _, idxs = index.search(q_emb, k)\n",
    "    return [chunk_metadata[int(i)] for i in idxs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747299590822,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "KmxZHG5jrk0T"
   },
   "outputs": [],
   "source": [
    "def build_prompt(question: str,\n",
    "                 gt_context: str,\n",
    "                 k: int = TOP_K,\n",
    "                 ctx_limit: int = CTX_TOKEN_LIMIT) -> str:\n",
    "    \"\"\"\n",
    "    1. Keep the full ground-truth context.\n",
    "    2. Add RAG chunks until we approach `ctx_limit` tokens.\n",
    "    \"\"\"\n",
    "    # ----- ground-truth block -------------------------------------------------\n",
    "    gt_block = f\"[Ground Truth]\\n{gt_context.strip()}\\n\"\n",
    "    gt_tokens = len(tokenizer.tokenize(gt_block))\n",
    "\n",
    "    # ----- RAG retrieval ------------------------------------------------------\n",
    "    rag_blocks, rag_tokens = [], 0\n",
    "    for ch in retrieve_chunks(question, k=k):\n",
    "        blk = f\"[{ch['title']}]\\n{ch['text']}\\n\"\n",
    "        t   = len(tokenizer.tokenize(blk))\n",
    "        # will we still fit?\n",
    "        if gt_tokens + rag_tokens + t <= ctx_limit:\n",
    "            rag_blocks.append(blk)\n",
    "            rag_tokens += t\n",
    "        else:\n",
    "            break                            # stop when limit reached\n",
    "\n",
    "    # concatenate (GT first, then RAG)\n",
    "    context = gt_block + \"\\n\".join(rag_blocks)\n",
    "\n",
    "    # final prompt\n",
    "    prompt = (\n",
    "        \"You are an expert scientific assistant. Use the excerpts to answer.\\n\\n\"\n",
    "        f\"Excerpts:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\nAnswer:\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1747299606077,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "ZHn1uUfprnNB"
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_answer_rag_plus_gt(question: str, gt_context: str) -> str:\n",
    "    prompt  = build_prompt(question, gt_context)\n",
    "    inputs  = tokenizer(prompt, return_tensors=\"pt\",\n",
    "                        padding=True, truncation=True,\n",
    "                        max_length=CTX_TOKEN_LIMIT).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    # strip the prompt part → keep only newly generated tokens\n",
    "    gen_ids   = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    prediction = tokenizer.decode(gen_ids,\n",
    "                                  skip_special_tokens=True).strip()\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hY-8xAhu8Ql"
   },
   "source": [
    "## Step 4: Generate predictions using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qH4CXGpurqjB"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, item in enumerate(eval_pairs):\n",
    "    question  = item[\"question\"]\n",
    "    reference = item[\"answer\"]\n",
    "    context   = item[\"context\"]  # ground-truth context\n",
    "\n",
    "    prediction = generate_answer_rag_plus_gt(question, context)\n",
    "\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"context\": context,\n",
    "        \"prediction\": prediction\n",
    "    })\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"[{i}/{len(eval_pairs)}] Question: {question}\\n→ {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747300010923,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "i2yK97lL3eXp",
    "outputId": "5a592f90-7455-4235-8748-ed6732aac894"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Why is sparsity in matrix B important in LoRI?',\n",
       " 'reference': 'Sparsity in matrix B enables LoRI to retain only the most critical elements necessary for adaptation, reducing parameter count and mitigating cross-task interference during adapter merging and continual learning.',\n",
       " 'context': 'Abstract:\\n\\nLow-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\\n\\nIntroduction:\\n\\nLarge language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\\n\\nRecent studies have shown that delta parameters – the differences between fine-tuned and pretrained model weights – exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model’s knowledge during adaptation.\\n\\nMulti-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\\n\\nBeyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\\n\\nTo evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to – or better than – full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\\n\\n\\nConclusion:\\n\\nIn this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains – including natural language understanding, mathematical reasoning, code generation, and safety alignment – while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\\n\\nFuture Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns – such as block sparsity, head pruning, or group-wise masking – which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning.',\n",
       " 'prediction': 'Sparsity in matrix B is crucial in LoRI because it reduces the number of trainable parameters while preserving the most critical elements of the adapter. This strategy significantly reduces the computational overhead and memory requirements, making LoRI more practical for large-scale models and multi-task scenarios.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1747300668376,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "oC1ftgCWO-if"
   },
   "outputs": [],
   "source": [
    "# Save for evaluation\n",
    "with open(\"eval_predictions_openbook_rag_plus_gt.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1747300682668,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "_eM-mVRMCBzK"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_path = \"./data/evaluation/eval_predictions_openbook_rag_plus_gt.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747300733061,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "yWc2YsghNX6z",
    "outputId": "60500290-08eb-4388-b334-de21c173246b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ./data/evaluation/eval_predictions_openbook_rag_plus_gt.json\n"
     ]
    }
   ],
   "source": [
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvCNBTe9OeM1"
   },
   "source": [
    "## Step 5: BLEU Score Evaluation\n",
    "\n",
    "In this section, we evaluate our fine-tuned model using the **BLEU (Bilingual Evaluation Understudy)** score, a standard metric for evaluating the quality of generated text by comparing it to a reference answer.\n",
    "\n",
    "### What is BLEU?\n",
    "BLEU measures *n-gram overlap* between the model's prediction and the reference answer:\n",
    "- **BLEU-1**: unigram overlap (word-level similarity)\n",
    "- **BLEU-2**: bigram overlap (2-word chunks)\n",
    "- **BLEU-3**: trigram overlap\n",
    "- **BLEU-4**: 4-gram overlap (more stringent)\n",
    "\n",
    "### Components of the Code:\n",
    "- `weights=(1, 0, 0, 0)`: Measures unigram overlap only (BLEU-1).\n",
    "- `smoothing_function=method1`: Prevents the BLEU score from dropping to 0 when there are no exact n-gram matches. This is useful for short or paraphrased responses.\n",
    "- We iterate over our evaluation dataset and compute BLEU-1 through BLEU-4 for each response.\n",
    "\n",
    "### Limitations:\n",
    "BLEU is a **surface-level** metric:\n",
    "- It penalizes paraphrasing.\n",
    "- It doesn't understand meaning—only *form*.\n",
    "- It is useful for rough comparison, but **not sufficient alone** to assess model quality.\n",
    "\n",
    "Hence, we will also perform **qualitative evaluation** using *LLM-as-a-Judge* in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1747300697785,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "HeLq1S9XOftp"
   },
   "outputs": [],
   "source": [
    "# Load predictions with context\n",
    "with open(\"eval_predictions_openbook_rag_plus_gt.json\", \"r\") as f:\n",
    "    eval_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1747300790496,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "b2An18WZOrzn"
   },
   "outputs": [],
   "source": [
    "# Initialize smoothing function and score containers\n",
    "smooth = SmoothingFunction().method1\n",
    "bleu_scores = {f\"BLEU-{n}\": [] for n in range(1, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1747300797263,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "pLmTMeE2PjBG",
    "outputId": "b7eae531-40c8-4a75-aada-3c6a652a80e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Scores: {'BLEU-1': 0.2571, 'BLEU-2': 0.1757, 'BLEU-3': 0.1345, 'BLEU-4': 0.108}\n"
     ]
    }
   ],
   "source": [
    "# Iterate over predictions and compute BLEU-1 to BLEU-4\n",
    "for item in eval_results:\n",
    "    reference = item[\"reference\"].split()\n",
    "    prediction = item[\"prediction\"].split()\n",
    "\n",
    "    bleu_scores[\"BLEU-1\"].append(\n",
    "        sentence_bleu([reference], prediction, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "    )\n",
    "    bleu_scores[\"BLEU-2\"].append(\n",
    "        sentence_bleu([reference], prediction, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
    "    )\n",
    "    bleu_scores[\"BLEU-3\"].append(\n",
    "        sentence_bleu([reference], prediction, weights=(1/3, 1/3, 1/3, 0), smoothing_function=smooth)\n",
    "    )\n",
    "    bleu_scores[\"BLEU-4\"].append(\n",
    "        sentence_bleu([reference], prediction, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "    )\n",
    "\n",
    "# Compute and display average scores\n",
    "avg_bleu_scores = {metric: round(sum(scores)/len(scores), 4) for metric, scores in bleu_scores.items()}\n",
    "print(\"Average BLEU Scores:\", avg_bleu_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SNcjCWxP1Qo"
   },
   "source": [
    "## Step 6: Using GPT-4o as LLM-as-a-Judge (OpenAI Evaluation)\n",
    "\n",
    "In this section, we use **GPT-4o**—a state-of-the-art model from OpenAI—as a neutral third-party judge to evaluate the quality of our model’s predictions against ground truth answers. This is part of the **LLM-as-a-Judge** evaluation methodology, which is growing in popularity as a way to assess open-ended outputs where metrics like BLEU or ROUGE may fall short.\n",
    "\n",
    "**What this section does:**\n",
    "\n",
    "- Loads model predictions from `eval_openbook_predictions.json`\n",
    "- Uses a GPT-4o prompt that provides:\n",
    "  - The question\n",
    "  - The model's generated answer\n",
    "  - The reference (ground-truth) answer\n",
    "- Asks GPT-4o to score the generated answer on a **scale from 1 to 5**, considering relevance, correctness, completeness, and style\n",
    "- Stores all outputs in `gpt4o_judgments_openbook.json` for analysis\n",
    "\n",
    "**Key Functions:**\n",
    "\n",
    "- `ask_gpt_judge()` → Sends a prompt to GPT-4o via the OpenAI API and returns a numeric score\n",
    "- `judged_results` → A list of evaluation records including the question, reference, model prediction, and GPT-4o's score\n",
    "- `np.mean()` → Used at the end to compute the **average evaluation score** across all QA pairs\n",
    "\n",
    "**Why use GPT-4o?**\n",
    "\n",
    "Because LLMs are best judged by **other LLMs** capable of contextual understanding. GPT-4o has been shown to be highly consistent and reliable in comparative evaluations.\n",
    "\n",
    "This evaluation complements our BLEU score by offering a **semantic and qualitative assessment**, helping us better understand the strengths and weaknesses of our fine-tuned model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19114,
     "status": "ok",
     "timestamp": 1747300915130,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "52ApEP9cPkrK",
    "outputId": "f927720f-6335-4eff-8a2b-a0eede70bedd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:··········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1747300951631,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "Zqg6itwZP8yl"
   },
   "outputs": [],
   "source": [
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1747300964838,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "X9yYs-FgQECQ"
   },
   "outputs": [],
   "source": [
    "# Load the API key from environment variable\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1747300996512,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "0-TnGl-0QNlG"
   },
   "outputs": [],
   "source": [
    "def ask_gpt_judge(question, reference, prediction):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert model evaluator. Given a question, a reference answer, and a model-generated answer that was generated with access to a relevant excerpt from a scientific paper, judge how good the model’s answer is on a scale of 1 to 5. Use the following rubric:\n",
    "\n",
    "1 – Completely irrelevant or hallucinated.\n",
    "2 – Partially related but mostly inaccurate.\n",
    "3 – Mostly accurate but missing key details.\n",
    "4 – Accurate and mostly complete.\n",
    "5 – Nearly identical in meaning to the reference.\n",
    "\n",
    "Be strict but fair. Output ONLY the number.\n",
    "\n",
    "Question: {question}\n",
    "Reference Answer: {reference}\n",
    "Model Prediction: {prediction}\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(\"Error during evaluation:\\n\")\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747301038909,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "ml0aAaFdQVUC"
   },
   "outputs": [],
   "source": [
    "with open(\"eval_predictions_openbook_rag_plus_gt.json\") as f:\n",
    "    eval_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_0hnUjEQfrS"
   },
   "outputs": [],
   "source": [
    "judged_results = []\n",
    "\n",
    "for i, item in enumerate(eval_results):\n",
    "    print(f\"Evaluating {i+1}/{len(eval_results)}\")\n",
    "    score = ask_gpt_judge(item[\"question\"], item[\"reference\"], item[\"prediction\"])\n",
    "    if score:\n",
    "        judged_results.append({\n",
    "            \"question\": item[\"question\"],\n",
    "            \"reference\": item[\"reference\"],\n",
    "            \"prediction\": item[\"prediction\"],\n",
    "            \"gpt4o_score\": score\n",
    "        })\n",
    "    time.sleep(1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1747301166378,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "joQSBeG5Qit2"
   },
   "outputs": [],
   "source": [
    "with open(\"gpt4o_judgments_openbook_rag_plus_gt.json\", \"w\") as f:\n",
    "    json.dump(judged_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1747301178846,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "_DYLICm7Q-yS",
    "outputId": "020b6ab7-d9d6-4a50-8801-baa780cbfebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: What is the primary innovation introduced by the LoRI method for parameter-efficient fine-tuning?\n",
      " Reference Answer: LoRI introduces a novel approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks, thereby significantly reducing trainable parameters while minimizing cross-task interference.\n",
      " Model Prediction: The primary innovation of LoRI is the use of fixed, randomly initialized projection matrices A and sparse, task-specific matrices B to reduce trainable parameters while minimizing cross-task interference. This design enables effective single-task performance, training-free adapter merging, and continual learning with minimal catastrophic forgetting.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does LoRI reduce the number of trainable parameters compared to traditional LoRA?\n",
      " Reference Answer: LoRI reduces the number of trainable parameters by keeping matrix A fixed as a random projection and sparsifying matrix B using task-specific masks, eliminating the need to train both matrices and reducing redundancy.\n",
      " Model Prediction: LoRI reduces the number of trainable parameters by freezing the projection matrices A as random projections and sparsifying the matrices B using task-specific masks. This design allows LoRI to achieve strong task performance while using up to 95% fewer trainable parameters than LoRA.\n",
      " GPT-4o Evaluation: 5\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why is sparsity in matrix B important in LoRI?\n",
      " Reference Answer: Sparsity in matrix B enables LoRI to retain only the most critical elements necessary for adaptation, reducing parameter count and mitigating cross-task interference during adapter merging and continual learning.\n",
      " Model Prediction: Sparsity in matrix B is crucial in LoRI because it reduces the number of trainable parameters while preserving the most critical elements of the adapter. This strategy significantly reduces the computational overhead and memory requirements, making LoRI more practical for large-scale models and multi-task scenarios.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does LoRI improve the process of merging adapters in multi-task scenarios?\n",
      " Reference Answer: LoRI enables more effective adapter merging by using fixed, randomly initialized projection matrices A, which maps task-specific adapters into approximately orthogonal subspaces, thus reducing parameter interference.\n",
      " Model Prediction: LoRI enables training-free adapter merging with minimal performance degradation by using fixed, randomly initialized projection A, which maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What mechanism does LoRI use to mitigate catastrophic forgetting in continual learning?\n",
      " Reference Answer: LoRI mitigates catastrophic forgetting by applying task-specific sparse masks to matrix B, which isolates parameter updates across tasks and preserves knowledge from previous adaptations, including safety alignment.\n",
      " Model Prediction: LoRI mitigates catastrophic forgetting by leveraging the sparsity of matrices B through task-specific masks, which isolates parameter updates across tasks, thereby preserving both safety and task effectiveness.\n",
      " GPT-4o Evaluation: 5\n",
      "--------------------------------------------------------------------------------\n",
      " Question: On what benchmark did LoRI with 90% sparsity in B outperform LoRA, and by how much?\n",
      " Reference Answer: LoRI with 90% sparsity in B outperformed LoRA by 17.3% on the HumanEval benchmark using the Llama-3 model.\n",
      " Model Prediction: LoRI with 90% sparsity outperformed LoRA on the HumanEval benchmark by 17.3%.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does LoRI compare to full fine-tuning and other PEFT methods in terms of performance and efficiency?\n",
      " Reference Answer: LoRI matches or outperforms full fine-tuning and other PEFT methods across multiple domains while using up to 95% fewer trainable parameters than LoRA, demonstrating both high performance and high efficiency.\n",
      " Model Prediction: LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What types of tasks were used to evaluate LoRI's effectiveness?\n",
      " Reference Answer: LoRI was evaluated on a diverse set of tasks, including natural language understanding, mathematical reasoning, code generation, and safety alignment.\n",
      " Model Prediction: LoRI was evaluated on natural language understanding, mathematical reasoning, code generation, and safety alignment tasks.\n",
      " GPT-4o Evaluation: 5\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What potential future directions do the authors propose for extending LoRI?\n",
      " Reference Answer: The authors suggest exploring structured sparsity patterns like block sparsity or head pruning and adapting LoRI to multi-modal models such as diffusion and vision-language systems.\n",
      " Model Prediction: The authors suggest exploring structured sparsity patterns, such as block sparsity, head pruning, or group-wise masking, for better hardware compatibility. They also propose extending LoRI to multi-modal models, such as diffusion and vision-language models, given the growing impact of adapter-based fine-tuning.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What is the broader significance of LoRI in the context of PEFT and LLM deployment?\n",
      " Reference Answer: LoRI provides a lightweight, modular, and scalable solution for adapting LLMs with minimal overhead, making it particularly suited for multi-task learning, safety-critical alignment, and efficient deployment on resource-constrained hardware.\n",
      " Model Prediction: LoRI represents a significant advancement in PEFT, offering a computationally efficient approach to fine-tuning LLMs without compromising performance. By reducing trainable parameters and minimizing cross-task interference, LoRI enables more efficient deployment of LLMs in diverse applications, including multi-task scenarios and safety-critical environments.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What are the core limitations of traditional LoRA methods that ElaLoRA seeks to address?\n",
      " Reference Answer: ElaLoRA addresses two key limitations of traditional LoRA: the fixed rank allocation across layers, which overlooks the layer-specific importance, and the inability to adapt ranks dynamically during training, which can lead to suboptimal parameter efficiency.\n",
      " Model Prediction: Traditional LoRA methods suffer from suboptimal performance due to fixed rank allocation, which fails to account for layer-specific importance. ElaLoRA addresses this by dynamically reallocating computational resources to the most critical layers, ensuring that essential layers receive more capacity while redundant ranks are removed.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Describe the three core components of the ElaLoRA framework.\n",
      " Reference Answer: ElaLoRA's architecture consists of: (1) an SVD-based adaptation strategy for matrix decomposition, (2) an importance score calculation mechanism based on loss gradients to assess rank relevance, and (3) a dynamic rank learning algorithm that reallocates ranks periodically during training to optimize layer-wise adaptation.\n",
      " Model Prediction: The ElaLoRA framework consists of three core components: 1) SVD-based adaptation strategy, 2) importance score calculation, and 3) a dynamic rank learning algorithm that reallocates ranks at scheduled intervals.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does ElaLoRA’s adaptive strategy improve performance under limited parameter budgets?\n",
      " Reference Answer: ElaLoRA reallocates computational resources to the most critical layers by pruning less important ranks and expanding ranks in essential layers, thus achieving higher performance even under smaller parameter budgets—for example, outperforming other PEFT methods with r=2 compared to their r=4 settings.\n",
      " Model Prediction: By dynamically reallocating computational resources to the most critical layers, ElaLoRA ensures that essential layers receive more capacity while redundant ranks are removed. This adaptive rank learning mechanism enables more efficient model adaptation across diverse NLP and Vision tasks, particularly in resource-constrained environments.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: In what way does ElaLoRA achieve better task alignment during fine-tuning?\n",
      " Reference Answer: ElaLoRA uses gradient-derived importance scores to identify which layers contribute most to task-specific learning, allowing the model to allocate more capacity to those layers and thus improving task alignment and learning efficiency.\n",
      " Model Prediction: ElaLoRA dynamically reallocates computational resources to the most critical layers, ensuring that essential layers receive more capacity while redundant ranks are removed. This adaptive rank learning mechanism enables more efficient model adaptation across diverse NLP and Vision tasks, resulting in better task alignment during fine-tuning.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What experimental evidence supports the superiority of ElaLoRA over other PEFT methods?\n",
      " Reference Answer: Experiments across NLU, NLG, and vision benchmarks show that ElaLoRA consistently outperforms state-of-the-art PEFT methods in accuracy, particularly under constrained parameter budgets, and demonstrates better GLUE benchmark performance even with fewer trainable parameters.\n",
      " Model Prediction: ElaLoRA consistently outperforms existing PEFT methods across multiple Natural Language Understanding (NLU), Natural Language Generation (NLG), and Visual Task benchmarks, demonstrating its effectiveness in performance. Notably, ElaLoRA achieves better average GLUE results with r = 2 than other PEFT methods at r = 4, making it particularly well-suited for resource-constrained environments.\n",
      " GPT-4o Evaluation: 5\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why is ElaLoRA particularly well-suited for resource-constrained environments?\n",
      " Reference Answer: ElaLoRA's dynamic pruning and expansion mechanism ensures that only the most essential ranks are trained, reducing memory usage and computational cost while maintaining high performance, making it ideal for low-resource scenarios.\n",
      " Model Prediction: ElaLoRA achieves better average GLUE results with rank 2 than other PEFT methods at rank 4, making it particularly well-suited for resource-constrained environments.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does the final rank distribution in ElaLoRA reflect its adaptive learning process?\n",
      " Reference Answer: ElaLoRA’s final rank distribution reveals that higher ranks are allocated to layers deemed more important via importance scores, confirming that the model dynamically concentrates learning capacity on the most impactful parts of the network.\n",
      " Model Prediction: The final rank distribution in ElaLoRA reflects the adaptive learning process by allocating more capacity to the most impactful layers while removing redundant ranks, ensuring efficient model adaptation across diverse NLP and Vision tasks.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What are the broader implications of ElaLoRA’s design for the future of fine-tuning large models?\n",
      " Reference Answer: ElaLoRA’s design shows that adaptive, importance-based rank allocation can significantly improve parameter efficiency without sacrificing accuracy, suggesting a paradigm shift toward more intelligent and resource-aware fine-tuning strategies.\n",
      " Model Prediction: ElaLoRA’s adaptive rank allocation mechanism offers a scalable and efficient fine-tuning solution, particularly suited for resource-constrained environments. By dynamically reallocating computational resources to the most critical layers, ElaLoRA ensures that essential layers receive more capacity while redundant ranks are removed. This approach could become a standard method for fine-tuning large models in the future, as it addresses the computational burden of full fine-tuning while maintaining or improving performance.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What distinguishes ElaLoRA from prior dynamic rank methods like AdaLoRA or IncreLoRA?\n",
      " Reference Answer: While AdaLoRA and IncreLoRA either prune or expand ranks, ElaLoRA is the first to implement both pruning and expansion dynamically during training, offering a more flexible and principled mechanism for allocating parameter capacity.\n",
      " Model Prediction: Unlike AdaLoRA and IncreLoRA, which only prune or expand ranks, ElaLoRA simultaneously prunes and expands ranks based on gradient-derived importance scores. This adaptive approach ensures that essential layers receive more capacity while redundant ranks are removed, leading to more efficient model adaptation.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why is parameter-efficient fine-tuning increasingly important in the LLM landscape?\n",
      " Reference Answer: As LLMs grow in size, full fine-tuning becomes prohibitively expensive, especially for domain-specific or low-resource settings. PEFT methods like ElaLoRA offer a practical solution by enabling adaptation with minimal compute and storage costs.\n",
      " Model Prediction: Parameter-efficient fine-tuning (PEFT) is crucial in the LLM landscape due to the enormous size of these models, which poses significant challenges in terms of computational complexity and resource requirements during fine-tuning. PEFT methods adapt LLMs to downstream tasks while minimizing the number of trainable parameters, thereby reducing computational and memory overhead.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What is the primary goal of the study presented in 'Beyond QA Pairs'?\n",
      " Reference Answer: The study aims to assess the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain-specific facts into LLMs, focusing on the impact of QA pair categorization and synthetic dataset generation techniques.\n",
      " Model Prediction: performance.\n",
      "\n",
      "[RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n",
      "  Instructions]\n",
      "\n",
      "[RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n",
      "  Generation]\n",
      "\n",
      "[Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n",
      "  Conflicts for Large Language Models]\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How are QA pairs categorized in this study, and what is the purpose of this categorization?\n",
      " Reference Answer: QA pairs are classified into ‘Factual’ and ‘Conceptual’ categories using a BERT-based classifier. The purpose is to investigate how the nature of QA pairs influences the effectiveness of PEFT.\n",
      " Model Prediction: 7 (Multi-hop QA). The final data volume for each subset is shown in Table 9. B.2 Prompts for synthesizing data for five RAG scenarios. We construct five RAG paradigms as described in Figure 8, Figure 9, Figure 10, Figure 11, and Fig- Method ARC PQA OBQA WQA AVG.(↑) VAR.(↓) Self-RAG (Llama2-7B) + DuckDuckGo 72.1 56.7 76.4 48.1 62.9 1.9 + WIKI 73.1 55.8 78.0 49.0 + BingSearch 68.6 53.2 76.8 46.4 RQ-RAG (Llama2-7B) + DuckDuckGo 69.0 68.0 70.0 67.0 + WIKI 70.0 68.0 71.0 68.0 + B\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What were the findings regarding models trained on conceptual vs factual QA datasets?\n",
      " Reference Answer: Models fine-tuned on conceptual datasets consistently outperformed those trained on factual datasets across multiple evaluations.\n",
      " Model Prediction: for LLMs, and the potential of fine-tuning and CoT reasoning to further improve performance.\n",
      "\n",
      "[Understand What LLM Needs: Dual Preference Alignment for\n",
      "  Retrieval-Augmented Generation]\n",
      "Open-Domain QA, represented by NaturalQuestions (NQ) [84] and TriviaQA (TQA) [85]; (2) Multi-Hop QA, represented by HotpotQA (HQA) [86]; and (3) Knowledge Base QA, represented by WebQues- tionsSP (WebQSP) [87]. Table 1 illustrate the statistics of them. For evaluation metrics, we use Hit@1 for the accuracy of the top-ranked response and F1 score to assess the quality and similarity to the ground-truth. More details of the experimental setup are listed in Appendix B. 4.2 Main Results The experimental results are shown in Table 2. In general, our DPA-RAG significantly outperforms all baselines across four datasets in different setups. This clearly highlights the superiority of our approach. We further have the following observations: (1\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Which synthetic dataset generation techniques are evaluated in this work, and which one performs better?\n",
      " Reference Answer: The paper evaluates D-RAG and D-Naive synthetic data generation methods. D-Naive outperformed D-RAG in fine-tuning effectiveness, largely due to better retrieval performance.\n",
      " Model Prediction: and what are their relative performances?\n",
      "\n",
      "Answer: The study compares D-RAG and D-Naive techniques for synthetic dataset generation. The results indicate that D-Naive outperforms D-RAG in terms of fine-tuning efficiency and quality.\n",
      " GPT-4o Evaluation: 5\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What was the significance of the product recommendation task in the data center domain?\n",
      " Reference Answer: The task served as a practical demonstration showing that a Llama-2 7B model fine-tuned with PEFT on just 1,000 instruction-based QA pairs significantly outperformed the baseline in generating product recommendations.\n",
      " Model Prediction: s dataset, MTL-LoRA outperforms the baseline by a significant margin, achieving an AUC-ROC of 0.97 compared to 0.89. This improvement demonstrates the effectiveness of MTL-LoRA in capturing task correlations and improving performance across multiple tasks.\n",
      "\n",
      "[MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning]\n",
      "relevance dataset (referred to as the Ads dataset). The text Ads relevance task involves determining whether a query is semantically relevant to a given Ad. This dataset encompasses 14 tasks, covering various production scenar- ios. The query and Ad pairs are collected from a commercial sponsored search engine, with relevance labels provided by experts. For evaluation, we treat the task as a binary clas- sification problem and report AUC-ROC metrics, follow- ing production practices. The dataset consists of 13 mil- lion examples in the training set and 2 million examples in the test set, with data collected in multiple languages from global markets. This benchmark is particularly challenging because, while all tasks are from the ad domain, they span different product scenarios. Effectively modeling the corre\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why do the authors argue that PEFT may not be optimal for factual embedding?\n",
      " Reference Answer: The study shows that while PEFT is effective for instruction tuning, it struggles with embedding factual information as effectively, likely due to its limited parameter update scope.\n",
      " Model Prediction: PEFT may not be optimal for embedding factual information into LLMs?\n",
      "\n",
      "Answer: The authors argue that PEFT may not be optimal for embedding factual information into LLMs because it excels in instruction-based tasks rather than factual information embedding. They suggest that for factual information embedding, other methods may be more suitable.\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What conclusions do the authors draw about the volume versus quality of QA data in PEFT?\n",
      " Reference Answer: They conclude that sheer quantity of QA pairs is insufficient; quality and conceptual depth are far more critical for successful PEFT.\n",
      " Model Prediction: different fine-tuning methods and model sizes.\n",
      "\n",
      "[GPT vs RETRO: Exploring the Intersection of Retrieval and\n",
      "  Parameter-Efficient Fine-Tuning]\n",
      "tuning methods, including P-tuning, Adapters, and LoRA, across six datasets and five model sizes. The results show that GPT consistently outperforms RETRO in the zero-shot setting, but the gap narrows significantly when PEFT is applied. In the P-tuning scenario, GPT achieves an average score of 24.74, while RETRO scores 47.18, indicating that GPT requires less fine-tuning to achieve comparable performance. However, in the Adapters scenario, RETRO outperforms GPT, with an average score of 52.75 compared to GPT’s 22.64. The LoRA method shows similar results, with RETRO achieving an average score of 48.00 compared to GPT’s 23.25. These findings suggest that while GPT is generally superior in the zero-shot setting, RETRO may be more effective when fine-tuning is applied.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What limitations of D-RAG were identified in the study?\n",
      " Reference Answer: D-RAG's limitations were attributed to the poor performance of its underlying vector database retriever, leading to suboptimal training data quality.\n",
      " Model Prediction: Performance comparison of various RAG methods and RAG-Instruct on the Multi-Turn Instructional Dialogue (MTID) dataset.\n",
      "\n",
      "[RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n",
      "  Instructions]\n",
      "b 71.0 48.0 17.1 51.2 21.2 Llama3-8b 76.4 51.0 20.2 61.4 21.4 Self-RAG (2-7b) 77.3 42.4 14.7 45.1 16.6 RQ-RAG (2-7b) 80.9 52.6 18.7 57.9 24.0 ChatQA-1.5 (3-8b) 83.5 54.9 21.4 65.1 23.9 ChatQA-2.0 (3-8b) 82.4 51.5 20.1 6\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How do the authors suggest future research should improve PEFT for fact embedding?\n",
      " Reference Answer: Future research should explore improvements in retrieval systems used by D-RAG, and consider more refined QA classification and data generation strategies.\n",
      " Model Prediction: A survey on content moderation in large language models. arXiv preprint arXiv:2304.08848.\n",
      "\n",
      "[LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content\n",
      "  Moderation of Large Language Models]\n",
      "tuning. arXiv preprint arXiv:2308.08747. Sourab Mangrulkar, Sylvain Gugger, Lysandre De- but, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022. Peft: State-of-the-art parameter- efficient fine-tuning methods. https://github. com/huggingface/peft. Todor Markov, Chong Zhang, Sandhini Agarwal, Flo- rentine Eloundou Nekoul, Theodore Lee, Steven Adler, Angela Jiang, and Lilian Weng. 2023. A holis- tic approach to undesired content detection in the real world. In Proceedings of the AAAI Conference on Ar- tificial Intelligence, volume 37\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What is the key insight this paper contributes to the field of LLM fine-tuning?\n",
      " Reference Answer: The paper highlights that PEFT's success hinges more on dataset composition—especially the conceptual quality of QA pairs—than on volume alone, and that careful use-case targeting is essential.\n",
      " Model Prediction: tuning?\n",
      "\n",
      "Answer: The paper emphasizes the importance of categorizing QA pairs into ‘Factual’ and ‘Conceptual’ classes for effective domain-specific fine-tuning of LLMs. It also highlights the superior performance of the D-Naive synthetic dataset generation technique over D-RAG.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sample in judged_results:\n",
    "    print(\" Question:\", sample[\"question\"])\n",
    "    print(\" Reference Answer:\", sample[\"reference\"])\n",
    "    print(\" Model Prediction:\", sample[\"prediction\"])\n",
    "    print(\" GPT-4o Evaluation:\", sample[\"gpt4o_score\"])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747301212575,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "5KY1_aHwRB1l",
    "outputId": "9f037a89-4771-4a72-c51f-4b9ead906ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-4o Evaluation Score: 3.20 out of 5\n"
     ]
    }
   ],
   "source": [
    "# Calculating the average score\n",
    "scores = [int(res[\"gpt4o_score\"]) for res in judged_results if res[\"gpt4o_score\"].isdigit()]\n",
    "average_score = np.mean(scores)\n",
    "print(f\"Average GPT-4o Evaluation Score: {average_score:.2f} out of 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1747301261357,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "4k5OU4WVRKE0",
    "outputId": "91ac3d92-5114-4637-f052-2a8d6a7afe5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judged results saved to ./data/evaluation/eval_gpt4o_judgments_open_book_rag_plus_gt.json\n"
     ]
    }
   ],
   "source": [
    "# Saving the results\n",
    "\n",
    "output_path = \"./data/evaluation/eval_gpt4o_judgments_open_book_rag_plus_gt.json\"\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(judged_results, f, indent=2)\n",
    "\n",
    "print(f\"Judged results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiPaB4AZRZBe"
   },
   "source": [
    "## Step 7: Evaluating with BERTScore (Semantic Similarity Metric)\n",
    "\n",
    "In this section, we evaluate the semantic similarity between the model’s predictions and the ground truth answers using **BERTScore**, a metric that leverages contextual embeddings from large pretrained models (like BERT) to assess the *meaning* of the outputs.\n",
    "\n",
    "Unlike BLEU, which only considers surface-level n-gram overlap, BERTScore measures how semantically close the answers are—even when the phrasing differs.\n",
    "\n",
    "### Interpretation:\n",
    "- **BERTScore F1** reflects the degree of **semantic overlap** between model output and human-labeled answer.\n",
    "- A score closer to **1.0** indicates stronger alignment of meaning.\n",
    "- This metric is especially useful in open-ended QA or summarization settings where **exact matching isn't expected**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1747301296146,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "w4ROMMawRV-K"
   },
   "outputs": [],
   "source": [
    "# Replace `results` with `judged_results` if needed\n",
    "predictions = [item[\"prediction\"] for item in results]\n",
    "references = [item[\"reference\"] for item in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263,
     "referenced_widgets": [
      "20617ff50ae14d54a6cb1c82389856dd",
      "52bcbac6c8bd4e15b3aa4e385b679b34",
      "ec4d0f0e02df44158689a80c2cb0c3b7",
      "7f10f650506a4a0a9604bf1686e46fe1",
      "258ebadd213f4af9a6f446ede36da132",
      "6289fd741b8841a8903605033ba89b94",
      "2033debb202c43b096ef25c0fb7d7d70",
      "6334f98bd262407ca429d331b07da34a",
      "827c8e34d8224a85835e285da5246443",
      "4d9114c15fc84a5a8d27a5c1919575cb",
      "c07b5a5c56f242d38a9ce6ee4a5b88fd",
      "983f981a08514294bef9202809e0d100",
      "c8f872202a2a442ca15c3b5c6f620c97",
      "b8c35516e4a84301b304b1bfb5e2058f",
      "b35a6777d84b4f48ae920b947252ffc2",
      "ffafc952479345bb81db6343789c9ec1",
      "3e3226d8a51b4c42a007cd0682970ec8",
      "4f395e5c8cca4831825478b563ac24a5",
      "96f4798ae9a5421a9782e5cc48c63f19",
      "bd836c9c7d1a4efc9ff2d328e284e987",
      "f3dc1055a29e4196a9cdefad16106f49",
      "b7352823b67648e5870fb3b410f01164",
      "ff3d9f7f485641f8ac53258d32b67e60",
      "d58e6a265b184bc69ae9bf821f62e413",
      "1a7c6f3fc24c46618e0cce7862f8bd0b",
      "7729710dbb324c0594bb3c5f711aded0",
      "597370eb257444a09fd2f18c15d409b6",
      "d6ae515ffc794857ab834859598a20da",
      "3d15abed37e84e3a97a40be87838a49f",
      "df5fefa4894f4dd794b1c7ebe24a8ba4",
      "dd91d354d7864ebfb6a02f84f1e885ec",
      "7719b7a9ebe84ab7a4be04873995dfff",
      "fa85e632a63945fbb9610933cb246a01",
      "c0cd9a5a2b4a43cabf06948ac0a51607",
      "ce4217dc1ed1486295f5964013d96379",
      "1cd79b4fc982442d94cb8d63b581d848",
      "1ca6061af5714ddbabfd603cb3922392",
      "945aebb762c247a7b8425ace1bdd5c1b",
      "1c7b486457dd486f9a0b2f5dde0eecb4",
      "f2b17daf94524f57a5e34aac1cef3690",
      "4b7a7ba636bb4a0eb98238d5c7f9f70f",
      "e5c0cfb4092e447f9a551b71c5580379",
      "329a9c271003406aac048774e5ccd2ba",
      "e17aa769dfac490f8c4f6d0a2cfb0d9d",
      "0233d75880074ea1adc2a69d9416ab26",
      "687e8b2380b54fdabad695452e5d53c9",
      "4b5732e79aa94e89b92fa2218e9e14ef",
      "2b1d4cf88fed47ceb4a8abe871c867f3",
      "05849abe990348618aef4de97cf31958",
      "96e76744f72d4197b15e2fe40395fe2b",
      "50289de0fe66406ab25c1eb8055ae96f",
      "e1c9764659104e969dee1a72adaa1e13",
      "57abeaa18aa940a7bb3f2bda8acab9fb",
      "2d4336ca593e4d4b810f8a08a26d68cc",
      "ffff42b9f42147458a0a86bdba1fbd36",
      "c76652259dcf49449d8a7fdaa4616a6b",
      "6ae95d2a8b114e209ee7ad871cad178b",
      "24ca9ada3b914facae90959ba82336e8",
      "a873b8abbdc74df2babfe1dcc881ca8e",
      "a5bae54bcf0b45c69b14a510ac0782e4",
      "cb5e8c4c05a54b5381a95d1f3a964c6f",
      "37c539c191684760bac82baaa4fc7f45",
      "cd34556eb33f4ee68070c025b68c9ce4",
      "f427f0250fbc441ea778504ccc5a6fd0",
      "85e54602b2164f6c8559a3719b1fa83b",
      "91ed52b3d77a43608baa45e952d659df"
     ]
    },
    "executionInfo": {
     "elapsed": 12013,
     "status": "ok",
     "timestamp": 1747301317221,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "XUNGELHhRed8",
    "outputId": "aee1c30a-40d0-448f-eec9-9285c27de2a7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20617ff50ae14d54a6cb1c82389856dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983f981a08514294bef9202809e0d100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3d9f7f485641f8ac53258d32b67e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cd9a5a2b4a43cabf06948ac0a51607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0233d75880074ea1adc2a69d9416ab26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76652259dcf49449d8a7fdaa4616a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = bertscore(predictions, references, lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1747301324211,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "Bm5lePQ2RgsK",
    "outputId": "80741df7-fddd-4017-ea8b-0ca6b35ee0c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.2728\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Precision: {P.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747301331589,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "w_AMGHI8RlUz",
    "outputId": "2efc4f09-44b7-4a33-b344-9d324273604a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall: 0.3940\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Recall: {R.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747301337890,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "9uNjp688RnIR",
    "outputId": "6ddb65db-2f1c-4538-916d-1d0ab1a0c7f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BERTScore (F1): 0.3318\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average BERTScore (F1): {F1.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sH5iy_D3UEep"
   },
   "source": [
    "## Step 8: Fixing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3421,
     "status": "ok",
     "timestamp": 1747302041010,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "OZxNPfitRoqz"
   },
   "outputs": [],
   "source": [
    "pip install nbformat --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14862,
     "status": "ok",
     "timestamp": 1747302055879,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "nobRGN1PUSKv",
    "outputId": "0e90d6ef-ec7a-4c2b-d83f-c29a19755b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1428,
     "status": "ok",
     "timestamp": 1747302058934,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "FtQeK5XfUTl_",
    "outputId": "46b08452-417f-4e7d-94fe-8bddc1ef9bd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.keep',\n",
       " '00_colab_setup.ipynb',\n",
       " '01_arxiv_scraper.ipynb',\n",
       " '02_pdf_downloader.ipynb',\n",
       " '04_prepare_finetuning_corpus.ipynb',\n",
       " '05_tokenization.ipynb',\n",
       " '03_qa_curation.ipynb',\n",
       " '07_eval_qa_curation.ipynb',\n",
       " '08_evaluation_closed_book.ipynb',\n",
       " '06_finetuning.ipynb',\n",
       " '10_evaluation_baseline_open_book.ipynb',\n",
       " '11_evaluation_baseline_closed_book.ipynb',\n",
       " '12_pdf_downloader_for_rag.ipynb',\n",
       " '14_rag_retrieval_and_inference.ipynb',\n",
       " '13_chunk_and_embed.ipynb',\n",
       " '09_evaluation_open_book.ipynb',\n",
       " '15_evaluation_rag_open_book.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# List the notebook directory to confirm the file exists\n",
    "os.listdir(\"/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUO7N02nUYW5"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = \"/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/notebooks/09_evaluation_open_book.ipynb\"\n",
    "\n",
    "with open(notebook_path, \"r\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "with open(notebook_path, \"w\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Notebook fixed and saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMmFUOmMPec8BABy6UmO23Z",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
