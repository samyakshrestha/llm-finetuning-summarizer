{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJ5jqeHRvoyI65yHuy24Vf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Step 1: Mounting Google Drive"],"metadata":{"id":"Fx3fmbJyIXen"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wx77uvGhIOCR","executionInfo":{"status":"ok","timestamp":1743379359755,"user_tz":300,"elapsed":16942,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"3ab7efa8-f9a0-4bbb-b913-bd7c52662760"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n","data  deployment  LICENSE  notebooks  project_plan.md  README.md  scripts\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Navigate to the repo folder\n","%cd /content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n","\n","# List repo contents\n","!ls"]},{"cell_type":"markdown","source":["## Step 2: Downlading LLM Fine-Tuning Papers\n","\n","This block runs the full pipeline to query, filter, and download relevant papers for our **fine-tuning QA corpus**.\n","\n","### Functionality Overview:\n","\n","- **Import custom utilities** from `arxiv_scraper.py`:\n","  - `search_arxiv`: queries arXiv's Atom API for papers\n","  - `filter_papers`: keeps only those whose title/summary contain key phrases\n","  - `download_papers`: downloads the PDFs to the target directory\n","\n","- **Query string**: `\"\"large language model OR llm OR fine-tuning\"\n","- **Keywords used for filtering**:\n","  - `[\"LoRA\", \"QLoRA\", \"parameter-efficient\", \"supervised fine-tuning\",\"adapter\", \"SFT\", \"instruction tuning\", \"continued pretraining\"]`\n","\n","- **Download directory**:  \n","  `./data/QA_corpus`  \n","  (i.e., inside the repo's `data/` folder)\n","\n","This yields a curated set of recent, relevant PDFs that will serve as the foundation for crafting our supervised QA pairs. These papers are assumed to be recent, though not necessarily high-impact (arXiv does not provide citation metadata).\n","\n","You may increase `max_results` or refine `keywords` to adjust the yield."],"metadata":{"id":"bmFS3ZYIMeGI"}},{"cell_type":"code","source":["import sys\n","sys.path.append('./scripts')\n","\n","from arxiv_scraper import search_arxiv, filter_papers, download_papers"],"metadata":{"id":"e4eWzSD2IeX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"large language model OR llm OR fine-tuning\"\n","keywords = [\"LoRA\", \"QLoRA\", \"parameter-efficient\", \"supervised fine-tuning\", \"adapter\", \"SFT\", \"instruction tuning\", \"continued pretraining\"]\n","\n","papers = search_arxiv(query=query, max_results=50)\n","print(f\"Retrieved {len(papers)} papers.\")\n","\n","filtered = filter_papers(papers, keywords)\n","print(f\"{len(filtered)} papers matched the keywords.\")\n","\n","download_dir = \"./data/QA_corpus\"\n","download_papers(filtered, download_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qyw-wQtPI_E2","executionInfo":{"status":"ok","timestamp":1743325489604,"user_tz":300,"elapsed":6185,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"02a91de4-292f-4da5-da6d-1db363569042"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Retrieved 50 papers.\n","15 papers matched the keywords.\n","Downloading: CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting\n","  Mitigation\n","Downloading: Balancing Continuous Pre-Training and Instruction Fine-Tuning:\n","  Optimizing Instruction-Following in LLMs\n","Downloading: Revisiting Zeroth-Order Optimization for Memory-Efficient LLM\n","  Fine-Tuning: A Benchmark\n","Downloading: Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over\n","  Aligned Large Language Models\n","Downloading: DELIFT: Data Efficient Language model Instruction Fine Tuning\n","Downloading: Non-instructional Fine-tuning: Enabling Instruction-Following\n","  Capabilities in Pre-trained Language Models without Instruction-Following\n","  Data\n","Downloading: Targeted Efficient Fine-tuning: Optimizing Parameter Updates with\n","  Data-Driven Sample Selection\n","Downloading: Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n","  on Audio Question Answering\n","Downloading: Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific\n","  Training Tasks\n","Downloading: Safety Alignment Backfires: Preventing the Re-emergence of Suppressed\n","  Concepts in Fine-tuned Text-to-Image Diffusion Models\n","Downloading: Language-Specific Neurons: The Key to Multilingual Capabilities in Large\n","  Language Models\n","Downloading: FATE-LLM: A Industrial Grade Federated Learning Framework for Large\n","  Language Models\n","Downloading: LoRA vs Full Fine-tuning: An Illusion of Equivalence\n","Downloading: Exploring Advanced Large Language Models with LLMsuite\n","Downloading: Exploring Design Choices for Building Language-Specific LLMs\n"]}]},{"cell_type":"markdown","source":["## Step 3: Doing a More Refined Search\n","\n","This cell performs a **more exhaustive and precise sweep** of arXiv to collect highly relevant fine-tuning papers.\n","\n","- **Query:** `\"large language model OR llm OR fine-tuning\"`\n","- **Max Results:** 100 (to ensure wider sampling)\n","- **Keywords:** Expanded list including `\"LoRA\"`, `\"QLoRA\"`, `\"PEFT\"`, `\"instruction tuning\"`, etc.\n","- **Final Filter:** Only papers with \"LLM\" or \"Large Language Model\" in title are kept\n","\n","This will form the *core dataset* for QA pair generation, ensuring every paper is semantically rich and technically focused on **LLM fine-tuning**."],"metadata":{"id":"NdWL0zx2X-Zs"}},{"cell_type":"code","source":["query = \"large language model OR llm OR fine-tuning\"\n","keywords = [\n","    \"LoRA\", \"QLoRA\", \"low-rank adaptation\", \"parameter-efficient\",\n","    \"efficient fine-tuning\", \"supervised fine-tuning\", \"adapter\",\n","    \"SFT\", \"instruction tuning\", \"continued pretraining\",\n","    \"PEFT\", \"alignment tuning\"\n","]\n","\n","papers = search_arxiv(query=query, max_results=100)\n","print(f\"Retrieved {len(papers)} papers.\")\n","\n","filtered = filter_papers(papers, keywords)\n","print(f\"{len(filtered)} papers matched the keywords.\")\n","\n","# Keep only those where title mentions LLM or Large Language Model\n","filtered = [\n","    paper for paper in filtered\n","    if (\"llm\" in paper['title'].lower()) or (\"large language model\" in paper['title'].lower())\n","]\n","print(f\"{len(filtered)} papers have LLM in title.\")\n","\n","download_dir = \"./data/QA_corpus\"\n","download_papers(filtered, download_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"er6-wOMwS9Ef","executionInfo":{"status":"ok","timestamp":1743326591369,"user_tz":300,"elapsed":2374,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"61c23cee-9196-426f-a5c5-fe213aa54009"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Retrieved 100 papers.\n","21 papers matched the keywords.\n","12 papers have LLM in title.\n","Already downloaded: ./data/QA_corpus/CURLoRA:_Stable_LLM_Continual_Fine-Tuning_and_Catastrophic_Forgetting\n","__Mitigation.pdf\n","Already downloaded: ./data/QA_corpus/Balancing_Continuous_Pre-Training_and_Instruction_Fine-Tuning:\n","__Optimizing_Instruction-Following_in.pdf\n","Already downloaded: ./data/QA_corpus/Revisiting_Zeroth-Order_Optimization_for_Memory-Efficient_LLM\n","__Fine-Tuning:_A_Benchmark.pdf\n","Already downloaded: ./data/QA_corpus/Preference-Oriented_Supervised_Fine-Tuning:_Favoring_Target_Model_Over\n","__Aligned_Large_Language_Mode.pdf\n","Already downloaded: ./data/QA_corpus/FATE-LLM:_A_Industrial_Grade_Federated_Learning_Framework_for_Large\n","__Language_Models.pdf\n","Already downloaded: ./data/QA_corpus/Exploring_Advanced_Large_Language_Models_with_LLMsuite.pdf\n","Already downloaded: ./data/QA_corpus/Exploring_Design_Choices_for_Building_Language-Specific_LLMs.pdf\n","Downloading: MEGAnno+: A Human-LLM Collaborative Annotation System\n","Downloading: Large Language Model Supply Chain: Open Problems From the Security\n","  Perspective\n","Downloading: LLM for Everyone: Representing the Underrepresented in Large Language\n","  Models\n","Downloading: Speech Translation with Large Language Models: An Industrial Practice\n","Downloading: Boosting Large Language Model for Speech Synthesis: An Empirical Study\n"]}]},{"cell_type":"markdown","source":["## Step 4: Refining Further"],"metadata":{"id":"bveO6vnSgtrn"}},{"cell_type":"code","source":["query = \"llm fine-tuning\"\n","keywords = [\n","    \"LoRA\", \"QLoRA\", \"low-rank adaptation\", \"parameter-efficient\",\n","    \"efficient fine-tuning\", \"supervised fine-tuning\", \"adapter\",\n","    \"SFT\", \"instruction tuning\", \"continued pretraining\",\n","    \"PEFT\", \"alignment tuning\"\n","]\n","\n","papers = search_arxiv(query=query, max_results=50)\n","print(f\"Retrieved {len(papers)} papers.\")\n","\n","filtered = filter_papers(papers, keywords)\n","print(f\"{len(filtered)} papers matched the keywords.\")\n","\n","# Keep only those where title mentions LLM or Large Language Model\n","filtered = [\n","    paper for paper in filtered\n","    if (\"llm\" in paper['title'].lower()) or (\"large language model\" in paper['title'].lower())\n","]\n","print(f\"{len(filtered)} papers have LLM in title.\")\n","\n","download_dir = \"./data/QA_corpus\"\n","download_papers(filtered, download_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70VxUnGog1ME","executionInfo":{"status":"ok","timestamp":1743379583435,"user_tz":300,"elapsed":1066,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"e6db7497-d2eb-41fc-bf99-27b82b2ce6b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Retrieved 50 papers.\n","14 papers matched the keywords.\n","4 papers have LLM in title.\n","Already downloaded: ./data/QA_corpus/Revisiting_Zeroth-Order_Optimization_for_Memory-Efficient_LLM\n","__Fine-Tuning:_A_Benchmark.pdf\n","Already downloaded: ./data/QA_corpus/Balancing_Continuous_Pre-Training_and_Instruction_Fine-Tuning:\n","__Optimizing_Instruction-Following_in.pdf\n","Already downloaded: ./data/QA_corpus/CURLoRA:_Stable_LLM_Continual_Fine-Tuning_and_Catastrophic_Forgetting\n","__Mitigation.pdf\n","Already downloaded: ./data/QA_corpus/Preference-Oriented_Supervised_Fine-Tuning:_Favoring_Target_Model_Over\n","__Aligned_Large_Language_Mode.pdf\n"]}]}]}