{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4",
   "authorship_tag": "ABX9TyMWu2Ab8OSc5TcXhGHmar7g"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Retrieval-Augmented Generation (RAG) Inference Pipeline\n",
    "\n",
    "This notebook implements a **full Retrieval-Augmented Generation (RAG) inference system** on a **fine-tuned Mistral 7B model** using a custom-built scientific corpus.\n",
    "\n",
    "The goal:  \n",
    "- To **retrieve the most relevant scientific text passages** for a user query  \n",
    "- To **construct a clean, context-rich prompt**  \n",
    "- To **generate grounded, intelligent answers** using the fine-tuned LLM\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does:\n",
    "\n",
    "1. **Load the Fine-Tuned Model**:\n",
    "   - Load the fine-tuned Mistral-7B-Instruct model (with LoRA adaptation) for high-quality semantic reasoning.\n",
    "\n",
    "2. **Load the FAISS Index + Chunk Metadata**:\n",
    "   - Load the pre-computed FAISS index containing dense semantic embeddings of scientific paper chunks.\n",
    "   - Load the associated `chunk_metadata.json` for human-readable titles and texts.\n",
    "\n",
    "3. **Define a Semantic Retriever**:\n",
    "   - Given a user query, embed it using a SentenceTransformer (BGE model).\n",
    "   - Perform a fast similarity search over the FAISS index to retrieve the top-k relevant text chunks.\n",
    "\n",
    "4. **Construct the RAG Prompt**:\n",
    "   - Assemble the retrieved excerpts cleanly into a structured prompt.\n",
    "   - Insert a clear role (\"You are an expert scientific assistant...\") and the user question at the end.\n",
    "\n",
    "5. **Run Model Inference**:\n",
    "   - Tokenize and feed the constructed prompt into the LLM.\n",
    "   - Generate an answer using greedy decoding (reproducible, deterministic outputs).\n",
    "\n",
    "6. **Inspect Retrieved Context (Optional)**:\n",
    "   - Print the top-k retrieved chunks to validate retrieval relevance.\n",
    "\n",
    "7. **Tested with Multiple Queries**:\n",
    "   - Verified the RAG pipeline with two real scientific questions to ensure retrieval grounding and model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Techniques Used:\n",
    "\n",
    "- **Retrieval-Augmented Generation (RAG) Architecture**\n",
    "- **Semantic Search with FAISS + Sentence-Transformers**\n",
    "- **Prompt Engineering for Scientific QA**\n",
    "- **LoRA Fine-Tuning Usage**\n",
    "- **Efficient FAISS Indexing and Retrieval**\n",
    "- **Professional Inference and Decoding Settings (greedy, max token budgeting)**\n"
   ],
   "metadata": {
    "id": "TizhAYX2ATMf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Mounting Google Drive"
   ],
   "metadata": {
    "id": "HWE0ix1BOcNd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzWNzlOPOO4s",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745865931804,
     "user_tz": 300,
     "elapsed": 20248,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "568c649b-ca89-4f68-a9f1-ddda99e5a8e6"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Navigate to the repo folder\n",
    "%cd /content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n",
    "\n",
    "# List repo contents\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Installing Dependencies and Importing Libraries"
   ],
   "metadata": {
    "id": "9ydzTizjO-Nx"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q sentence-transformers faiss-cpu transformers"
   ],
   "metadata": {
    "id": "TgzUhYpEO-1c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import login"
   ],
   "metadata": {
    "id": "cNX0nbBWPLMd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "login()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "9839c2e444484d2b8b04ad202d5ba4eb",
      "47f3a0527b5d43fc81efeac5c0267fcc",
      "1280e27b2e5a4bf0b098fea3cd4a2e47",
      "a76ab0914fe444a79472a0475cc8aa2f",
      "eb10047bd6654bc688b9de558c6ee10a",
      "36931d3ae4be40efb08370a29702c894",
      "e2da9b6c28544bafb3bc7043554b2e1e",
      "bff4265f5a6a4705a7e4f465a20ce101",
      "135655fe35aa46bd8ead56fca23a33df",
      "3eaeb0be4ed5448d87ddc1e5ed01eb16",
      "e17fde96f22c4e16b6567983e98a14b0",
      "a94e938e2f3749c5965b23b064d510ca",
      "ed7187117bf2478380c4859f8b0eda39",
      "54179326c1f248dcb1357a21ae467e06",
      "33bf6a1b9eca4288ae701e4ac3c1a013",
      "e7984881c10e4e709b237ffb00d31cef",
      "9d2493ffbf274556981adc6fbb826845",
      "cc594b9ba4544c34bf700cf79e053865",
      "04e2f832827c4a91b49c0db5b4057192",
      "10f055535197443eadd7db48dff96aaf"
     ]
    },
    "id": "ckILzailPW9i",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745866345224,
     "user_tz": 300,
     "elapsed": 318,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "15ec56e6-4c76-44d4-c3a7-10444b5160b4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Verifying GPU and Environment"
   ],
   "metadata": {
    "id": "-M87yMAMPZ6h"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"Using CPU\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78NMD_n9PdAg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745866361123,
     "user_tz": 300,
     "elapsed": 32,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "e97ed782-4120-4c5f-ad34-67d4a2fa94e6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Loading the FAISS index and Chunk Metadata\n",
    "\n",
    "### Load FAISS Index and Chunk Metadata\n",
    "\n",
    "In this section, we prepare the two essential components for semantic retrieval:\n",
    "\n",
    "1. **FAISS Index** (`faiss_index.bin`)\n",
    "   - A dense vector index containing semantic embeddings of all text chunks.\n",
    "   - Used to perform fast top-k nearest neighbor search given a user query embedding.\n",
    "   - Loaded with `faiss.read_index`, providing instant retrieval functionality.\n",
    "\n",
    "2. **Chunk Metadata** (`chunk_metadata.json`)\n",
    "   - A JSON mapping between each FAISS vector and its corresponding original text chunk.\n",
    "   - Contains important fields such as:\n",
    "     - `arxiv_id` (paper source)\n",
    "     - `chunk_id` (local identifier)\n",
    "     - `title` (paper title)\n",
    "     - `text` (actual content of the chunk)\n",
    "\n",
    "Both the FAISS index and the metadata are loaded into memory at this step to enable full semantic retrieval and reconstruction of meaningful context for the user’s question.\n",
    "\n",
    "**Outcome**:  \n",
    "- FAISS index with ~8724 vectors ready for search.  \n",
    "- Metadata dictionary with ~8724 entries for text reconstruction."
   ],
   "metadata": {
    "id": "80dYvSORSxna"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load FAISS index\n",
    "faiss_index_path = \"./data/rag_corpus/faiss_index.bin\"\n",
    "index = faiss.read_index(faiss_index_path)\n",
    "print(f\"Loaded FAISS index with {index.ntotal} vectors.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNZbJQV8Rokq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745866368667,
     "user_tz": 300,
     "elapsed": 3334,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "aca17364-eda3-4654-d6be-7dbf969a4e46"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load chunk metadata\n",
    "metadata_path = \"./data/rag_corpus/chunk_metadata.json\"\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    chunk_metadata = json.load(f)\n",
    "print(f\"Loaded metadata for {len(chunk_metadata)} chunks.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1hGe7NgS7hs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745866373990,
     "user_tz": 300,
     "elapsed": 1198,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "ffba1d6d-59b1-4b36-e234-4fcd267af442"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Loading the Fine-Tuned Model"
   ],
   "metadata": {
    "id": "towyWvGGTbqU"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load merged model and tokenizer\n",
    "model_path = \"./models/merged-finetuned-mistral\""
   ],
   "metadata": {
    "id": "KWmQI5k9TTBQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ],
   "metadata": {
    "id": "r7Op5rqSTzUL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "metadata": {
    "id": "yU48XnHQT30Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")"
   ],
   "metadata": {
    "id": "zs3iKHjFT4a4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "print(\"Model and tokenizer successfully loaded.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ue2Ey8liUJmD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745866763287,
     "user_tz": 300,
     "elapsed": 6,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "8550478c-6601-4357-b43f-c2c44ae9dfb7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: Define the Retriever Function\n",
    "\n",
    "### Defining the Semantic Retriever Function\n",
    "\n",
    "In this section, we define a function that enables **semantic retrieval** of the most relevant scientific text chunks given a user query.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Function Does:\n",
    "\n",
    "1. **Embeds the User Query**:\n",
    "   - The query is transformed into a dense 768-dimensional vector using the same `bge-base-en-v1.5` model used during chunking.\n",
    "   - Embeddings are normalized to ensure correct cosine similarity behavior during retrieval.\n",
    "\n",
    "2. **Performs FAISS Semantic Search**:\n",
    "   - The query embedding is searched against the FAISS index to find the top-k most similar chunk embeddings.\n",
    "   - FAISS search returns the indices of the top-k matches based on inner-product similarity.\n",
    "\n",
    "3. **Maps FAISS Results Back to Full Text**:\n",
    "   - The retrieved indices are used to gather the corresponding metadata (e.g., title, chunk text) for each matching chunk.\n",
    "\n",
    "---\n",
    "\n",
    "#### Outputs:\n",
    "\n",
    "- A **list of dictionaries**, each containing:\n",
    "  - `title` of the original paper\n",
    "  - `text` of the retrieved chunk\n",
    "  - `chunk_id` to uniquely identify the chunk\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Matters:\n",
    "\n",
    "- Retrieval-augmented generation (**RAG**) relies on fetching real-world grounding knowledge from a semantic database.\n",
    "- A well-designed retriever ensures that the LLM model has high-quality, contextually relevant information when answering questions.\n",
    "- This improves answer accuracy, reduces hallucination, and enables real scientific reasoning over the corpus.\n",
    "\n",
    "---\n",
    "\n",
    "**Outcome**:  \n",
    "We can now input any user question and retrieve the top-k most semantically relevant scientific excerpts, ready for answer generation."
   ],
   "metadata": {
    "id": "bo2ojf22XyWh"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load Query Encoder (same model as used during chunk embedding)\n",
    "embedder = SentenceTransformer(\"BAAI/bge-base-en-v1.5\", device=model.device)\n",
    "print(\"Query embedder loaded.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3863NXoFVpDB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745867925342,
     "user_tz": 300,
     "elapsed": 3746,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "e712ef90-ec31-41f8-fc71-e6e30d6bae12"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define Semantic Retriever Function\n",
    "def retrieve_relevant_chunks(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Embed the query, search FAISS, and retrieve top-k relevant chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): User question.\n",
    "        top_k (int): Number of top chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts containing 'title', 'text', and 'chunk_id'.\n",
    "    \"\"\"\n",
    "    # Step 1: Embed the query\n",
    "    query_embedding = embedder.encode(query, normalize_embeddings=True)\n",
    "    query_embedding = np.expand_dims(query_embedding, axis=0)  # FAISS expects (batch_size, dim)\n",
    "\n",
    "    # Step 2: Search FAISS\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    retrieved_chunks = []\n",
    "\n",
    "    # Step 3: Map FAISS results back to chunk texts\n",
    "    for idx in indices[0]:\n",
    "        if idx < len(chunk_metadata):\n",
    "            retrieved_chunks.append(chunk_metadata[idx])\n",
    "\n",
    "    return retrieved_chunks\n",
    "\n",
    "print(\"Retriever function defined successfully.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7Poe5LGYA5r",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745867929438,
     "user_tz": 300,
     "elapsed": 17,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "33bbbeba-f862-415e-b850-a4e60085e6c5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Prompt Construction and Inference Function\n",
    "\n",
    "In this section, we define a complete **Retrieval-Augmented Generation (RAG)** pipeline, where:\n",
    "\n",
    "- A **user query** is first used to retrieve the most relevant scientific text passages.\n",
    "- These passages are then assembled into a coherent **context prompt**.\n",
    "- The **fine-tuned LLM** uses this context to generate an intelligent, grounded answer.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Function Does:\n",
    "\n",
    "1. **Semantic Retrieval**:\n",
    "   - Uses the previously defined retriever function to fetch the top-k most semantically relevant chunks from the scientific corpus.\n",
    "\n",
    "2. **Context Assembly**:\n",
    "   - Gathers the retrieved chunks while respecting a **maximum context token limit** (default 2048 tokens).\n",
    "   - Ensures the prompt stays within the model's maximum sequence length (4096 tokens).\n",
    "\n",
    "3. **Prompt Construction**:\n",
    "   - Builds a structured prompt that:\n",
    "     - Introduces the model's role (\"You are an expert scientific assistant...\")\n",
    "     - Clearly separates the provided excerpts.\n",
    "     - Presents the user’s question cleanly.\n",
    "\n",
    "4. **Model Inference**:\n",
    "   - Tokenizes the prompt carefully with padding and truncation.\n",
    "   - Feeds the input into the fine-tuned LLM for generation.\n",
    "   - Uses **greedy decoding** (no sampling) with `temperature=0.0` for maximum stability and reproducibility.\n",
    "\n",
    "5. **Postprocessing**:\n",
    "   - Cleans the output by extracting only the portion after \"Answer:\", ensuring focused and professional answers.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Parameters:\n",
    "\n",
    "| Parameter | Default | Meaning |\n",
    "|-----------|---------|---------|\n",
    "| `top_k` | 5 | Number of passages to retrieve |\n",
    "| `max_context_tokens` | 2048 | Max tokens allowed for retrieved context |\n",
    "| `max_new_tokens` | 512 | Max tokens allowed for the generated answer |\n",
    "\n",
    "---\n",
    "\n",
    "#### Outputs:\n",
    "\n",
    "- A final, human-readable **answer** generated by the model, grounded in real scientific text excerpts.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "tz3WCBSF1oBs"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_answer_with_retrieval(query, top_k=5, max_context_tokens=2048, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Full RAG pipeline: retrieve relevant chunks → build prompt → generate answer.\n",
    "\n",
    "    Args:\n",
    "        query (str): User's question.\n",
    "        top_k (int): Number of chunks to retrieve.\n",
    "        max_context_tokens (int): Max tokens to allocate for context passages.\n",
    "        max_new_tokens (int): Max tokens the model can generate for the answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's generated answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Retrieve relevant context passages\n",
    "    retrieved_chunks = retrieve_relevant_chunks(query, top_k=top_k)\n",
    "\n",
    "    # Step 2: Assemble the context\n",
    "    context_blocks = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for chunk in retrieved_chunks:\n",
    "        chunk_text = f\"[Title: {chunk['title']}]\\n{chunk['text']}\\n\"\n",
    "        chunk_tokens = len(tokenizer.tokenize(chunk_text))\n",
    "\n",
    "        if total_tokens + chunk_tokens <= max_context_tokens:\n",
    "            context_blocks.append(chunk_text)\n",
    "            total_tokens += chunk_tokens\n",
    "        else:\n",
    "            break  # stop adding more chunks if token budget exceeded\n",
    "\n",
    "    assembled_context = \"\\n\\n\".join(context_blocks)\n",
    "\n",
    "    # Step 3: Build the final prompt\n",
    "    prompt = (\n",
    "        f\"You are an expert scientific assistant. Use the provided excerpts to answer the question.\\n\\n\"\n",
    "        f\"Excerpts:\\n{assembled_context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    # Step 4: Tokenize the full prompt\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=4096,  # absolute model context window\n",
    "        padding=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Step 5: Generate the answer\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # greedy decoding\n",
    "            top_p=1.0\n",
    "        )\n",
    "\n",
    "    # Step 6: Decode and return the generated answer\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the portion after \"Answer:\" (to be clean)\n",
    "    final_answer = decoded_output.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    return final_answer"
   ],
   "metadata": {
    "id": "w5kYDkq21uG0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745869222466,
     "user_tz": 300,
     "elapsed": 19,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Testing the Model with a Sample Question"
   ],
   "metadata": {
    "id": "QbNEmgGD5Bhx"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sample_question = \"What are the benefits of parameter-efficient fine-tuning methods?\"\n",
    "\n",
    "# Generate the answer\n",
    "rag_answer = generate_answer_with_retrieval(sample_question, top_k=5)\n",
    "\n",
    "# Display\n",
    "print(\"Question:\")\n",
    "print(sample_question)\n",
    "print(\"\\nModel Answer:\")\n",
    "print(rag_answer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvItcMSf1939",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745869254961,
     "user_tz": 300,
     "elapsed": 30285,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "e2dd752d-61c2-4c47-903a-38a26c6d068c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "7Er16_oyARwS"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Inspect Retrieved Chunks for a Query\n",
    "\n",
    "retrieved_chunks = retrieve_relevant_chunks(sample_question, top_k=5)\n",
    "\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(f\"Title: {chunk['title']}\")\n",
    "    print(f\"Text excerpt:\\n{chunk['text'][:500]}...\")  # Print only the first 500 characters\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ia6hwklW5pZB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745869596458,
     "user_tz": 300,
     "elapsed": 44,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "5fd9ee4e-79c3-450a-8cd7-ea4e87ca46d7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sample_question_2 = \"How does LoRA improve the efficiency of fine-tuning large language models?\"\n",
    "\n",
    "rag_answer_2 = generate_answer_with_retrieval(sample_question_2, top_k=5)\n",
    "\n",
    "print(\"Question:\")\n",
    "print(sample_question_2)\n",
    "print(\"\\nModel Answer:\")\n",
    "print(rag_answer_2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHpRPU7Z7-Xs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745870427758,
     "user_tz": 300,
     "elapsed": 14381,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "aede3f18-6e16-4dfb-ca2e-b449d09e803b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Inspect Retrieved Chunks for a Query\n",
    "\n",
    "retrieved_chunks_2 = retrieve_relevant_chunks(sample_question_2, top_k=5)\n",
    "\n",
    "for i, chunk in enumerate(retrieved_chunks_2):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(f\"Title: {chunk['title']}\")\n",
    "    print(f\"Text excerpt:\\n{chunk['text'][:500]}...\")  # Print only the first 500 characters\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IO-77Cj2_F06",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745870471094,
     "user_tz": 300,
     "elapsed": 6,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    },
    "outputId": "e06c458d-175a-4a5c-d4d6-abd1a96ec5af"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 9: Fixing Metadata"
   ],
   "metadata": {
    "id": "Er6i2fTxCYcg"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pip install nbformat --quiet"
   ],
   "metadata": {
    "id": "y5MsCPNg_T5A",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745871441349,
     "user_tz": 300,
     "elapsed": 2716,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount=True)"
   ],
   "metadata": {
    "id": "24ybNyW9CfZ2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import nbformat\n",
    "import os, json, pathlib"
   ],
   "metadata": {
    "id": "BprGLsjeCmIU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "nb_path = pathlib.Path(\"/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/notebooks/14_rag_retrieval_and_inference.ipynb\")   # adjust if filename differs\n",
    "nb = json.loads(nb_path.read_text())\n",
    "\n",
    "# Delete the troublesome metadata\n",
    "nb.get(\"metadata\", {}).pop(\"widgets\", None)\n",
    "\n",
    "# (optional but helpful) strip cell outputs\n",
    "for cell in nb[\"cells\"]:\n",
    "    cell[\"outputs\"] = []\n",
    "    cell[\"execution_count\"] = None\n",
    "\n",
    "nb_path.write_text(json.dumps(nb, indent=1, ensure_ascii=False))\n",
    "print(\"Notebook cleaned.\")"
   ],
   "metadata": {
    "id": "J8qV4aqUCrWa"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}