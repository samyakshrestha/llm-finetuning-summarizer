{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNgAm4KtCv1GQVJZv7m0g/n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Step 1: Mounting Google Drive"],"metadata":{"id":"Na1CXwGoqc9T"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gA9ToC4IqNM7","executionInfo":{"status":"ok","timestamp":1743401097311,"user_tz":300,"elapsed":25781,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"1bebbe35-5930-44a4-f0a3-399ae9275aaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n","data  deployment  LICENSE  notebooks  project_plan.md  README.md  scripts\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Navigate to the repo folder\n","%cd /content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n","\n","# List repo contents\n","!ls"]},{"cell_type":"markdown","source":["## Step 2: Installing PyMuPDF"],"metadata":{"id":"ZFs1nu9bza70"}},{"cell_type":"code","source":["!pip install -q pymupdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bRC8nbZLzRkx","executionInfo":{"status":"ok","timestamp":1743401163274,"user_tz":300,"elapsed":6027,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"a8cb53a4-1554-4b56-e80f-a367b5acb775"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["## Step 3: Importing Libraries"],"metadata":{"id":"UJ9n9v3Ez_GE"}},{"cell_type":"code","source":["import os\n","import fitz  # PyMuPDF\n","import json\n","from IPython.display import display, Markdown"],"metadata":{"id":"Qo0NrtoR0DHq","executionInfo":{"status":"ok","timestamp":1743401323652,"user_tz":300,"elapsed":181,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Step 4: Setting Paths"],"metadata":{"id":"_uIcvMl60ULj"}},{"cell_type":"code","source":["BASE_DIR = \"/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\"\n","PDF_DIR = os.path.join(BASE_DIR, \"data\", \"QA_corpus\")\n","QA_DIR = os.path.join(BASE_DIR, \"qa_pairs\")\n","\n","os.makedirs(QA_DIR, exist_ok=True)"],"metadata":{"id":"6tSm2kqP0Qtp","executionInfo":{"status":"ok","timestamp":1743401561221,"user_tz":300,"elapsed":4,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Step 5: Defining Utility Functions\n","\n","This section defines three helper functions to extract text from PDFs, display text cleanly, and save question-answer pairs.\n","\n","**extract_text_from_pdf**: This function takes the filepath of a PDF and extracts text from its first max_pages. It uses the fitz library (PyMuPDF) to open the PDF and iterates through the specified pages, appending the extracted text to a string, which is then returned.\n","\n","**display_text_as_markdown**: This function takes a text string and displays it in a user-friendly format using Markdown within the Jupyter Notebook. It removes leading/trailing whitespace and wraps the text in triple backticks to create a code block, enhancing readability.\n","\n","**save_qa_pairs**: This function takes a `filename` and `qa_pairs` (a list or dictionary) and saves the question-answer pairs to a JSON file. It opens the file in write mode, uses `json.dump` to write the data in JSON format with indentation, and prints a confirmation message.\n","\n","\n","\n"],"metadata":{"id":"TD_FbB5q1VSK"}},{"cell_type":"code","source":["def extract_text_from_pdf(filepath, max_pages=4):\n","    \"\"\"Extracts text from the first few pages of the PDF.\"\"\"\n","    text = \"\"\n","    with fitz.open(filepath) as doc:\n","        for page in doc[:max_pages]:\n","            text += page.get_text()\n","    return text\n","\n","def display_text_as_markdown(text):\n","    \"\"\"Display large blocks of text cleanly.\"\"\"\n","    display(Markdown(\"```\\n\" + text.strip() + \"\\n```\"))\n","\n","def save_qa_pairs(filename, qa_pairs):\n","    \"\"\"Save QA pairs to a JSON file.\"\"\"\n","    with open(filename, 'w') as f:\n","        json.dump(qa_pairs, f, indent=2)\n","    print(f\"Saved {len(qa_pairs)} pairs to {filename}\")"],"metadata":{"id":"xhvHKXh71KwT","executionInfo":{"status":"ok","timestamp":1743401988051,"user_tz":300,"elapsed":5,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Step 6: Listing PDFs in QA Corpus Folder\n","\n","This code snippet lists all PDF files in a specific folder, displays them to the user, allows the user to select one, and then prepares the path information for further processing."],"metadata":{"id":"b2aLd4Lx37sV"}},{"cell_type":"code","source":["# List PDFs in QA corpus folder\n","pdf_files = sorted([f for f in os.listdir(PDF_DIR) if f.endswith(\".pdf\")])\n","\n","print(\"Available Papers:\")\n","for i, file in enumerate(pdf_files):\n","    print(f\"[{i}] {file}\")\n","\n","# Select your paper here\n","paper_index = 0  # Change this index to select a different paper\n","pdf_path = os.path.join(PDF_DIR, pdf_files[paper_index])\n","pdf_name = os.path.splitext(pdf_files[paper_index])[0]\n","\n","print(f\"\\n Selected: {pdf_files[paper_index]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcwOld_y2y9d","executionInfo":{"status":"ok","timestamp":1743402094786,"user_tz":300,"elapsed":326,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"28f8231e-3a0d-4832-9842-efa87120a63a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Available Papers:\n","[0] ADALORA:_ADAPTIVE_BUDGET_ALLOCATION_FOR_PARAMETER-EFFICIENT_FINE-TUNING.pdf\n","[1] AutoLoRA:_Automatically_Tuning_Matrix_Ranks_in_Low-Rank_Adaptation_Based_on_Meta_Learning.pdf\n","[2] Balancing_Continuous_Pre-Training_and_Instruction_Fine-Tuning:\n","__Optimizing_Instruction-Following_in.pdf\n","[3] CURLoRA:_Stable_LLM_Continual_Fine-Tuning_and_Catastrophic_Forgetting\n","__Mitigation.pdf\n","[4] DELIFT:_Data_Efficient_Language_model_Instruction_Fine_Tuning.pdf\n","[5] FINETUNED_LANGUAGE_MODELS_ARE_ZERO-SHOT_LEARNERS.pdf\n","[6] Few-Shot_Parameter-Efficient_Fine-Tuning_is_Better_and_Cheaper_than_In-Context_Learning.pdf\n","[7] Instruction_Tuning_for_Large_Language_Models:_A_Survey.pdf\n","[8] LLAMA-ADAPTER:_EFFICIENT_FINE-TUNING_OF_LARGE_LANGUAGE_MODELS_WITH_ZERO-INITIALIZED_ATTENTION.pdf\n","[9] LLM-Adapters:_An_Adapter_Family_for_Parameter-Efficient_Fine-Tuning_of_Large_Language_Models.pdf\n","[10] LORA:_LOW-RANK_ADAPTATION_OF_LARGE_LANGUAGE_MODELS.pdf\n","[11] LoRA_vs_Full_Fine-tuning:_An_Illusion_of_Equivalence.pdf\n","[12] Non-instructional_Fine-tuning:_Enabling_Instruction-Following\n","__Capabilities_in_Pre-trained_Language.pdf\n","[13] Parameter-Efficient_Fine-Tuning_Methods_for_Pretrained_Language_Models:_A_Critical _Review_and_Assessment.pdf\n","[14] Parameter-Efficient_Fine-Tuning_for_Large_Models: _A_Comprehensive_Survey.pdf\n","[15] Parameter-Efficient_Transfer_Learning_with_Diff_Pruning.pdf\n","[16] Preference-Oriented_Supervised_Fine-Tuning:_Favoring_Target_Model_Over\n","__Aligned_Large_Language_Mode.pdf\n","[17] QA-LORA:_QUANTIZATION-AWARE_LOW-RANK_ADAPTATION_OF_LARGE_LANGUAGE_MODELS.pdf\n","[18] QLORA:_Efficient_Finetuning_of_Quantized_LLMs.pdf\n","[19] Revisiting_Zeroth-Order_Optimization_for_Memory-Efficient_LLM\n","__Fine-Tuning:_A_Benchmark.pdf\n","[20] Revisiting_Zeroth-Order_Optimization_for_Memory-Efficient_LLM_Fine-Tuning:_A_Benchmark.pdf\n","[21] Scaling_Down_to_Scale_Up: _A_Guide_to_Parameter-Efficient_Fine-Tuning.pdf\n","[22] Targeted_Efficient_Fine-tuning:_Optimizing_Parameter_Updates_with\n","__Data-Driven_Sample_Selection.pdf\n","[23] The_Flan_Collection:_Designing_Data_and_Methods_for_Effective_Instruction_Tuning.pdf\n","\n"," Selected: ADALORA:_ADAPTIVE_BUDGET_ALLOCATION_FOR_PARAMETER-EFFICIENT_FINE-TUNING.pdf\n"]}]},{"cell_type":"markdown","source":["## Step 7: Displying Parsed Text"],"metadata":{"id":"GI_m6Ma84rXX"}},{"cell_type":"code","source":["parsed_text = extract_text_from_pdf(pdf_path, max_pages=4)\n","display_text_as_markdown(parsed_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"grii9ryn3M7f","executionInfo":{"status":"ok","timestamp":1743402501526,"user_tz":300,"elapsed":750,"user":{"displayName":"Samyak Shrestha (Caesar)","userId":"13083503381857072620"}},"outputId":"1bfbf636-8f93-4481-fe3d-67d45fb7b746"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"```\nPublished as a conference paper at ICLR 2023\nADALORA: ADAPTIVE BUDGET ALLOCATION FOR\nPARAMETER-EFFICIENT FINE-TUNING\nQingru Zhang†∗, Minshuo Chen‡, Alexander Bukharin†, Nikos Karampatziakis⋄,\nPengcheng He⋄, Yu Cheng⋄, Weizhu Chen⋄and Tuo Zhao†\n†Georgia Institute of Technology\n‡Princeton University\n⋄Microsoft Azure AI\n{qingru.zhang,abukharin3,tourzhao}@gatech.edu\nmc0750@princeton.edu\n{nikosk,penhe,yu.cheng,wzchen}@microsoft.com\nABSTRACT\nFine-tuning large pre-trained language models on downstream tasks has become\nan important paradigm in NLP. However, common practice fine-tunes all of the\nparameters in a pre-trained model, which becomes prohibitive when a large number\nof downstream tasks are present. Therefore, many fine-tuning methods are proposed\nto learn incremental updates of pre-trained weights in a parameter efficient way,\ne.g., low-rank increments. These methods often evenly distribute the budget\nof incremental updates across all pre-trained weight matrices, and overlook the\nvarying importance of different weight parameters. As a consequence, the fine-\ntuning performance is suboptimal. To bridge this gap, we propose AdaLoRA,\nwhich adaptively allocates the parameter budget among weight matrices according\nto their importance score. In particular, AdaLoRA parameterizes the incremental\nupdates in the form of singular value decomposition. Such a novel approach\nallows us to effectively prune the singular values of unimportant updates, which\nis essentially to reduce their parameter budget but circumvent intensive exact\nSVD computations. We conduct extensive experiments with several pre-trained\nmodels on natural language processing, question answering, and natural language\ngeneration to validate the effectiveness of AdaLoRA. Results demonstrate that\nAdaLoRA manifests notable improvement over baselines, especially in the low\nbudget settings. Our code is publicly available at https://github.com/\nQingruZhang/AdaLoRA.\n1\nINTRODUCTION\nPre-trained language models (PLMs) have manifested superior performance in various natural\nlanguage processing tasks (Devlin et al., 2019; Liu et al., 2019; He et al., 2021b; Radford et al.,\n2019; Brown et al., 2020). The most common way to adapt pre-trained models to down-stream\ntasks is to fine-tune all the parameters (full fine-tuning, Qiu et al. (2020); Raffel et al. (2020)).\nHowever, pre-trained models typically incurs large memory footprint. For example, BERT model\n(Devlin et al., 2019) consists up to 300 million parameters; T5 (Raffel et al., 2020) comprises up\nto 11 billion parameters and GPT-3 (Brown et al., 2020) contains up to 175 billion parameters.\nWhen building a NLP system upon these pre-trained models, we usually handle multiple tasks\nthat arrive simultaneously (Radford et al., 2019). Given a large number of down-stream tasks, full\nfine-tuning requires that each task maintains a separated copy of large models. The resulting memory\nconsumption is prohibitively expensive.\nTo address this issue, researchers have proposed two main lines of research to reduce the fine-tuning\nparameters, while maintaining or even improving the performance of PLMs. Specifically, one line\nof research focuses on adding small neural modules to PLMs and fine-tune only these modules for\neach task – the base model is kept frozen and shared across tasks. In this way, only a small number\nof task-specific parameters are introduced and updated, greatly enhancing the practicality of large\nmodels. For example, adapter tuning (Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2020;\n∗Work was done during Qingru Zhang’s internship at Microsoft Azure AI.\n1\narXiv:2303.10512v2  [cs.CL]  20 Dec 2023\nPublished as a conference paper at ICLR 2023\nWq\nWk\nWv\nWo\nWf1\nWf2\n88.50\n88.75\n89.00\n89.25\n89.50\n89.75\n90.00\nMNLI Matched Acc\n88.58\n88.98\n89.36 89.28\n89.91 89.99\n(a) Selected weight matrix\n1,2,3\n4,5,6\n7,8,9\n10,11,12\n78\n80\n82\n84\n86\n88\nMNLI Matched Acc\n77.87\n85.82\n88.15\n88.6\n(b) Selected layers\nFigure 1: Given the total trainable parameters as 0.28M, we apply LoRA only to selected weight matrices (left)\nor selected layers (right) of DeBERTaV3-base and compare the fine-tuning performance on MNLI-m. Figure 1a:\nwe only fine-tune a selected type of weight matrix of every transformer layer, including query/key/value\nprojection (Wq, Wk, Wv), output projection (Wo) in the self-attention, and two weight matrices (Wf1, Wf2) in\ntwo-layer FFNs. In Figure 1b, we apply LoRA to every weight matrix of the selected layers.\nHe et al., 2022) inserts small neural modules called adapters between the layers of the base model.\nPrefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) attach additional trainable\nprefix tokens to the input or hidden layers of the base model. These methods have shown to achieve\ncomparable performance to full fine-tuning, while only updating less than 1% of the original model\nparameters, significantly releasing the memory consumption.\nAnother line of research proposes to model the incremental update of the pre-trained weights in a\nparameter-efficient way, without modifying the model architecture (Zaken et al., 2021; Guo et al.,\n2020; Hu et al., 2022). Given a pre-trained weight matrix1 W (0), for example, diff pruning (Guo et al.,\n2020) models its incremental update ∆as a sparse matrix. Diff pruning initializes ∆as the same\ndimension as W (0) and then prunes ∆element-wise based on the magnitude of the entries. As such,\ndiff pruning can increase the parameter efficiency substantially by adaptively retaining important\nupdates and pruning unimportant ones. Nonetheless, diff pruning has several limitations. First, it\nrelies on low-level implementation to speed up the computation of unstructured sparse matrices,\nwhich is not well supported by existing deep learning frameworks. Therefore, we have to store ∆as\na dense matrix during training. Second, it needs to update every entry of ∆with their gradients and\nthen prune them. This results in similar computational cost as full fine-tuning (Guo et al., 2020).\nTo overcome these drawbacks, Hu et al. (2022) propose a method named LoRA, which parameterizes\n∆as a low-rank matrix by the product of two much smaller matrices:\nW = W (0) + ∆= W (0) + BA,\n(1)\nwhere W (0), ∆∈Rd1×d2, A ∈Rr×d2 and B ∈Rd1×r with r ≪{d1, d2}. During fine-tuning, only\nA and B are updated. The rank r is chosen to be much smaller than the dimension of W (e.g., r = 8\nwhen d1 = d2 = 1024). With less than 0.5% additional trainable parameters, the training overhead\ncan be reduced up to 70%, compared to full fine-tuning. However, LoRA achieves comparable or\neven better performance than full fine-tuning (Hu et al., 2022). Meanwhile, the product of two samll\nmatrices is more friendly to implement and deploy than unstructured sparse matrices in diff pruning.\nLoRA still has limitations as it prespecifies the rank r of each incremental matrix ∆identical. This\nignores the fact that the importance of weight matrices varies significantly across modules and layers\nwhen fine-tuning pre-trained models. To illustrate this point, we present an concrete example in\nFigure 1. We compare the performance of LoRA when fine-tuning specific modules or layers with\nthe same number of trainable parameters. Figure 1a shows that fine-tuning feed-forward networks\n(FFN) achieves better performance than self-attention modules. In addition, Figure 1b demonstrates\nthat weight matrices in top layers are more important than those in bottom layers.\nAdding more trainable parameters to the critical weight matrices can lead to better model performance.\nIn contrast, adding more parameters to those less important weight matrices yields very marginal\ngains or even hurt model performance. Given the parameter budget, i.e., the number of total trainable\nparameters, we always prefer to allocate more parameters to those important modules. Distributing\nthe budget evenly to all weight matrices/layers, like LoRA and other methods (e.g., adapter and prefix\ntuning), often gives suboptimal performance. To this end, a natural question is:\nHow can we allocate the parameter budget adaptively according to importance\nof modules to improve the performance of parameter-efficient fine-tuning?\n1Unless specified otherwise, we use W (0) to denote any pre-trained weight matrix.\n2\nPublished as a conference paper at ICLR 2023\nTo answer this question, we propose a new method – AdaLoRA (Adaptive Low-Rank Adaptation),\nwhich dynamically allocates the parameter budget among weight matrices during LoRA-alike fine-\ntuning. Specifically, AdaLoRA adjusts the rank of incremental matrices to control their budget.\nCritical incremental matrices are assigned with high rank such that they can capture more fine-grained\nand task-specific information. Less importance ones are pruned to have lower rank to prevent\noverfitting and save the computational budget. There are some methods to control the rank of matrices\nin the existing literature of matrix approximation (Cai et al., 2010; Koltchinskii et al., 2011; Toh &\nYun, 2010). Most of them directly compute singular value decomposition (SVD) of a matrix and\nthen truncate the smallest singular values. Such an operation can manipulate the rank explicitly\nand, more importantly, minimize the difference between the resulting matrix and the original matrix.\nHowever, for fine-tuning large models, it becomes prohibitively expensive to iteratively apply SVD\nfor a large number of high-dimensional weight matrices. Therefore, instead of computing SVD\nexactly, we parameterize ∆as ∆= PΛQ to mimic SVD. The diagonal matrix Λ contains singular\nvalues while the orthogonal matrices P and Q represent left/right singular vectors of ∆. To regularize\nthe orthogonality of P and Q, an additional penalty is added to training loss. Such a parameterization\navoids the intensive computations of SVD. Besides, another advantage is that we only need to drop the\nunimportant singular values while the singular vectors are maintained. This preserves the possibility\nof future recovery and stabilizes the training. See a detailed comparison to LoRA in Section 3.\nBased on our SVD parameterization, AdaLoRA dynamically adjusts the rank of ∆= PΛQ by\nimportance scoring. Specifically, we divide the incremental matrix PΛQ into triplets, where each\ntriplet Gi contains the i-th singular value and the corresponding singular vectors. To quantify the\nimportance of triplets, we propose a novel importance metric, which takes account of the contribution\nof every entry in Gi to the model performance (Sanh et al., 2020; Liang et al., 2021; Zhang et al.,\n2022). Triplets with low importance scores are granted low priority and hence the singular values are\nzeroed out. Triplets with high importance are retained for fine-tuning. Moreover, we also propose\na global budget scheduler to facilitate the training. In particular, we start from an initial parameter\nbudget, which is slightly higher than the final budget, and then gradually reduce it until matching\nthe target. Such a scheduler can improve the training stability and model performance. Please see\nSection 3 for a detailed description of our importance metric and budget scheduler.\nWe conduct extensive experiments on a wide range of tasks and models to demonstrate the effec-\ntiveness of AdaLoRA. Specifically, we evaluate the performance using DeBERTaV3-base (He et al.,\n2021a) on natural language understanding (GLUE, Wang et al. (2019)) and question answering\n(SQuADv1, Rajpurkar et al. (2016) and SQuADv2, Rajpurkar et al. (2018)) datasets. We also apply\nour methods to BART-large (Lewis et al., 2019) and evaluate the performance on natural language\ngeneration (XSum, Narayan et al. (2018) and CNN/DailyMail, Hermann et al. (2015)) tasks. We\nshow AdaLoRA consistently outperforms the baseline, especially under low budget settings. For\nexample, with less than 0.1% trainable parameters of full fine-tuning, AdaLoRA achieves a 1.2% F1\nimprovement on the SQuAD2.0 dataset compared with state-of-the-art approaches.\n2\nBACKGROUND\nTransformer-based Models. A typical transformer model consists of L stacked blocks, where each\nblock contains two submodules: a multi-head attention (MHA) and a fully connected FFN. Given the\ninput sequence X ∈Rn×d, MHA performs the attention function in parallel h heads:\nMHA (X) = Concat(head1, ..., headh)Wo,\nheadi = Softmax\n\u0010\nXWqi(XWki)⊤/\np\ndh\n\u0011\nXWvi,\nwhere Wo ∈Rd×d is an output projection and Wqi, Wki, Wvi ∈Rd×dh are query, key and value\nprojections of head i. dh is typically set to d/h. The other important module is a FFN which consists\nof two linear transformations with a ReLU activation in between: FFN(X) = ReLU(XWf1 +\nb1)Wf2 + b2, where Wf1 ∈Rd×dm and Wf2 ∈Rdm×d. Finally, a residual connection is used\nfollowed by a layer normalization (Ba et al., 2016).\nLow Rank Adaptation. LoRA (Hu et al., 2022) models the incremental update of the pre-trained\nweights by the product of two small matrices. For h = W (0)x, the modified forward pass is:\nh = W (0)x + ∆x = W (0)x + BAx,\n(2)\nwhere W (0), ∆∈Rd1×d2, A ∈Rr×d2 and B ∈Rd1×r with r ≪{d1, d2}. A typically adopts a\nrandom Gaussion initialization while B is initialized with zero to have ∆= 0 at the beginning of\n3\nPublished as a conference paper at ICLR 2023\ntraining. We further denote Ai∗as the i-th row of A, B∗i as the i-th column of B, and Gi = {Ai∗, B∗i}\nas the i-th doublet. Hu et al. (2022) only apply LoRA to query and value projections (i.e, Wq and\nWv) in the MHAs. He et al. (2022) extend it to weight matrices of FFNs (i.e, Wf1 and Wf2), leading\nto the performance improvement . Meanwhile, they propose a unified view of various efficient tuning\nmethods including adapter tuning, prefix tuning and LoRA.\n3\nADALORA METHOD\nOur method contains two important components: (i) SVD-based adaptation, which formulates\nthe incremental matrices in the form of singular value decomposition; (ii) Importance-aware rank\nallocation, which prunes redundant singular values based on our newly-designed importance metric.\n3.1\nSVD-BASED ADAPTATION\nAs mentioned in Section 1, we propose to parameterize the incremental updates of the pre-trained\nweight matrices in the form of singular value decomposition:\nW = W (0) + ∆= W (0) + PΛQ,\n(3)\nwhere P ∈Rd1×r and Q ∈Rr×d2 represent the left/right singular vectors of ∆and the diagonal\nmatrix Λ ∈Rr×r contains the singular values {λi}1≤i≤r with r ≪min(d1, d2). We further denote\nGi = {P∗i, λi, Qi∗} as the triplet containing the i-th singular value and vectors. In practice, since\nΛ is diagonal, we only need to save it as a vector in Rr. Λ is initialized with zero while P and Q\nadopt a random Gaussian initialization to ensure ∆= 0 at the beginning of training. To enforce the\northogonality of P and Q, i.e., P ⊤P = QQ⊤= I, we utilize the following regularizer2:\nR(P, Q) = ∥P ⊤P −I∥2\nF + ∥QQ⊤−I∥2\nF.\n(4)\nIn our method, Λ is iteratively pruned to adjust the rank after each gradient decent step. As mentioned\nin Section 1, one can directly compute SVD for every ∆to manipulate singular values. The\ncomputational complexity, however, is O(min(d1, d2)d1d2). It becomes extremely expensive to\niteratively apply SVD for a large number of high-dimensional incremental matrices. In contrast, our\nparameterization avoids intensive SVD computation, greatly releasing the computational overhead.\nWe remark that one can also apply structured pruning to LoRA to control the rank (i.e., prune BA\ndoublet-wise in (1)), whereas it has the following disadvantages. First, when a doublet is measured as\nunimportant, we have to prune all of its elements. It makes scarcely possible to reactivate the pruned\ndoublets as their entries are all zeroed out and not trained. In contrast, AdaLoRA only masks out\nthe singular values based on (3) while the singular vectors are always maintained. It preserves the\npotential of future recovery for the triplets dropped by mistake. Second, A and B of LoRA are not\northogonal, meaning the doublets can be dependent with each other. Discarding the doublets can\nincur larger variation from the original matrix than truncating the smallest singular values. Therefore,\nthe incremental matrices are often altered dramatically after each step of rank allocation, which\ncauses training instability and even hurts generalization. To demonstrate this point, we present an\nablation study in Section 4.4, which compares AdaLoRA with structured pruning for LoRA.\n3.2\nIMPORTANCE-AWARE RANK ALLOCATION\nWe apply the SVD-based adaptation (3) to every weight matrix including Wq, Wk, Wv, Wf1 and\nWf2 of each transformer layer. In order to control the budget, we iteratively prune singular values\nin correspondence to their importance score during the training. For clear reference, we use k to\nindex the incremental matrix, i.e., ∆k = PkΛkQk for k = 1, . . . , n, where n is the number of\nadapted weight matrices. We denote the i-th triplet of ∆k as Gk,i = {Pk,∗i, λk,i, Qk,i∗} and its\nimportance score as Sk,i. We further denote the parameter sets P = {Pk}n\nk=1, E = {Λk}n\nk=1,\nQ = {Qk}n\nk=1 and training cost as C(P, E, Q). With the regularization (4), the training objective\nis given by L(P, E, Q) = C(P, E, Q) + γ Pn\nk=1 R(Pk, Qk), where γ > 0 is the regularization\ncoefficient. At the t-th step, we first take a stochastic gradient step to update P (t)\nk , Λ(t)\nk\nand Q(t)\nk\nfor\nk = 1, . . . , n. Specifically, for Λ(t)\nk\n˜Λ(t)\nk\n= Λ(t)\nk −η∇ΛkL(P(t), E(t), Q(t)),\n(5)\n2We present the experiments in Appendix G to verify the effectiveness of the regularization.\n4\n```"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"bZN0ASu_4wJE"},"execution_count":null,"outputs":[]}]}