{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVw4drbdbbye"
   },
   "source": [
    "## Step 1: Mounting Google Drive and Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15024,
     "status": "ok",
     "timestamp": 1744871349834,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "ffelDmZPa9W7",
    "outputId": "622e0420-aa26-4d06-f8d3-c2a9f6d63cb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n",
      "data\t    LICENSE  notebooks\t      qa_pairs\t results  wandb\n",
      "deployment  models   project_plan.md  README.md  scripts\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Navigate to the repo folder\n",
    "%cd /content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n",
    "\n",
    "# List repo contents\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKiuCOrVbgHK"
   },
   "outputs": [],
   "source": [
    "!pip install datasets bert-score openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jmjXnYTcN1M"
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes --prefer-binary --extra-index-url https://download.pytorch.org/whl/cu118 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 18781,
     "status": "ok",
     "timestamp": 1744871543910,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "Nj7Mx-yIcQiL"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import json\n",
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from google.colab import userdata\n",
    "import time\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from getpass import getpass\n",
    "import random\n",
    "import numpy as np\n",
    "from bert_score import score as bertscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2v3MIG7cvFj"
   },
   "source": [
    "## Step 2: Loading the Validation Set for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1744871598829,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "mqHB2E12coqh",
    "outputId": "95bbf203-9c7f-4a7b-a256-e22fcbd8704f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 QA pairs for evaluation.\n"
     ]
    }
   ],
   "source": [
    "eval_path = \"./data/eval.jsonl\"\n",
    "eval_pairs = []\n",
    "\n",
    "with open(eval_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        eval_pairs.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"Loaded {len(eval_pairs)} QA pairs for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXNuzoCqdDYy"
   },
   "source": [
    "## Step 3: Loading Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "9290f31ceb87443bb8c5d601bbd6e2b2",
      "b5de009985544052b79f0fedca19bb95",
      "a9adea7624444952b636477632f77c5c",
      "a4d94188dbcf4c38bdff01a939af6d9e",
      "217a9cab74524993a8d259919aaf2838",
      "6e5d3083e3bf4488a0cf402d1959ee1f",
      "8632655ce1da4f26b49277e792a9a60c",
      "621a8b261106444683beafc08d10fe18",
      "566e44c6433f4256a80a9a9c3fdd78c4",
      "a769b6af2ad349a2bd90edececbcf9f9",
      "37c542d46fee4809856ffcfbcdb8c601",
      "d58930aa4a20480396cec2305b657853",
      "2270fbf071194ceb83c9393384f08dfc",
      "676002864c7945db98deb88eb381af84",
      "812167cb0ba64dd583b79d11a0ccf4b2",
      "ff25d61c63294afca4566cf474385eab",
      "1633ebc87bde4b55a07f8bfbca14ed19",
      "4d48107d5cbb4528ab42d5f445c9b6d9",
      "358f969adb874127b15ea7fcbef75e0a",
      "59c5d0d35c9248b5a7bddcfdb5bb101d"
     ]
    },
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1744871666323,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "WnffoUGtc8z2",
    "outputId": "b85f1118-f282-46be-dadf-ab71c6de9bd3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9290f31ceb87443bb8c5d601bbd6e2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "ebced341f8304f7a81f6a8b2bb39e057",
      "3c0d1f5c82364029a10e60f791b0bccc",
      "a9c9e563360f462aad088ae43b7746de",
      "18b95f4f1fba40bcaac5a2e751911a7a",
      "2b446362954c4236a72ae335800658a8",
      "7b67f4d4763441d48198cc7231941bde",
      "c88d78f4fa334c14a803bfee1962773b",
      "403e3cb03f204264ac21502c97185b30",
      "cb37cb9ae6bd4b02949d4495508b6b0b",
      "064b4025d9c141fbb1817a21f7216f8b",
      "85aa6bbedbf04e50af0f5a55e562f67a",
      "7f5141e69abb4e0992103d7179fc31ca",
      "6703f5e007854824a8583058c773a70d",
      "55b93bac6e5a4e4db0fb1b989dcbe037",
      "e784edb2813047808136b7da3c76b2f0",
      "40004d29e8534f8f8ed6c19cfc7413f7",
      "a92df78940f94db5b998a1f314825855",
      "d1d031f117d943cab708a56a3ba0ba80",
      "d2079d0927d34a9ea9a300eded26a775",
      "6ff486ceeee94f5cb3658e38c4302186",
      "8c35696ede92404ba2aa2367d145e915",
      "e1706794973a4166ae1ae24d7ad360be",
      "609fed327f5c4b01a91cb5b3933a2a89",
      "5b42e71e61e941fbb11ddd22d4ce12f7",
      "103987107a9c4e1b846776532120a235",
      "f31a5662451947d1bc31ea4b9cde8010",
      "06a7d6c57c39477fb71e454f7532714b",
      "ed26223068314b52a54413b9b9547d39",
      "b882c16394714f8cb76ccc824b6003f3",
      "671b86b61406411d8f4ef2989b7eac5d",
      "a08c092a1dc04190a3c74542b2f21a30",
      "fc6f1020736442928e68ad1faa90a5a0",
      "0668f597b5cd458186fb51d5abe3b158",
      "628d1d346aca4cda9401280f9e1e2690",
      "9dbfb742a3cd4c24b1721c98ebf1a629",
      "081eb0f51a28401c83559117fbb63540",
      "993f9bda39534c648df8d3ff5cc7dda6",
      "f2bfee5fdc674c33bd990ede8ed3718d",
      "63b534282d0145c8a0e0d58ef9d82631",
      "e0253230020c4247a6050b870b441ffa",
      "24fdb067d482479596877569e76809f4",
      "8123b2fd5bf34e43a122ed023b3bed14",
      "a08f33adeb884601ac0109df6d3a8632",
      "e52ba9cee45f4c1f966fb22fe46e9b97"
     ]
    },
    "executionInfo": {
     "elapsed": 1947,
     "status": "ok",
     "timestamp": 1744871691070,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "lS8_KnQ-dHeI",
    "outputId": "8cf7c9ea-c1eb-46bc-c0f2-8c10570929c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebced341f8304f7a81f6a8b2bb39e057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5141e69abb4e0992103d7179fc31ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609fed327f5c4b01a91cb5b3933a2a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628d1d346aca4cda9401280f9e1e2690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1744871695958,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "tQlxJQPDdQs9"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1744871702624,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "utJEIZoZdSYB"
   },
   "outputs": [],
   "source": [
    "# Preparing the model for 4-bit inference (memory-efficient)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "ae2e436a17984506a95b7ab634667f9b",
      "f427f0eece554010bc19a6cc3def93c4",
      "ca9cdd14f61447e78176d08e5fd55e5c",
      "48302cfc30a7474d83efa7c65a71b3af",
      "5264da3d11164d92ad7f6f7204955f49",
      "a6a93219f4654ee081912d9ee48de5ca",
      "ae19ca76765449c08af797409a924341",
      "1907944bf75849268356ad4a5d36e7c6",
      "d159aa7f679246719c7f0f3fc86ac4f3",
      "541c235c9ffa456191d32f48c03d83ce",
      "24df19ef53f24cf5844492361ade6bba",
      "287d4a482644441ab64fa14b6876c1bb",
      "adb03830215640d69166becbc29e0f72",
      "c93c3790d1c84847be6b2e66cefa8194",
      "21a7935de43946108431b586782a1d10",
      "7091f6273a3e439eb98d3a9fb4159dfa",
      "24824e29c7e1467eb70e1de69f87e680",
      "9cb35380eacf424c9e71d7c726cc75be",
      "7cf0dcf23b8d484aaa774bfc15182246",
      "164ffab07db04466b32934e4afce5428",
      "0b5bc513656f40a2889f5c432cfc825c",
      "fe170846bcea43d9a5c02313b2a23412",
      "fc0fde95e7a8493d81bc78cf158dd26a",
      "36fc3c241db34c12babf6d6a031f4744",
      "aa63e01db9cb4329a3ceccb1cb6bb26f",
      "e341b9433e324cf1b0233de42798e216",
      "0755727adc3d408998300971f27678e7",
      "afcf5baf570a4dfeb09af54f8d578449",
      "a076bdc85fce40fd9a3e92e10bc539e9",
      "6c215619d5854d1a86aeb7a67cf149ed",
      "5c9f80d814df4f71b948d0d8e4f4f28f",
      "66b0491fe520443f8a921f40e6e855e7",
      "10565b2d57014695a8c3d0c7ccfabc2b",
      "605c70a951674defaba3949e94e4ceb8",
      "33f89e07f1b847adbc440a606703290e",
      "7a096e77833340fcab62444f9f2edd48",
      "a015deb6e4eb45178841f8371b504f42",
      "bbac177b96814cf19edea9ad2b1a01a6",
      "e6c939692c094c6c991e508726f303e6",
      "b6aad1d17c6c47dc8e9b218598a0f1c9",
      "22b3021d555d4dc3b83d224903d7b49a",
      "8256dd513b9b4867b2c47978106c88a7",
      "b34123efa0a5468fb8e4f3687d18cf72",
      "4c93a47f99104282a06b5668cf60e2eb",
      "b322d245cf84462898e2a0fcb2dde529",
      "fc5ef42b102d4a8a9307c970d2bd6f4d",
      "b29350d85cf546aa90ed321a8f9d8411",
      "fe5db3980521485ab2edf49ece2ef43b",
      "df800b2caf5c4ce8910157fea193f90b",
      "d48b3e63d65345a5a279abab93dc92ca",
      "9b821c9bedc64e62b2dc27efc2a71900",
      "c96018982e9c46b587cf95957b6e6d78",
      "cf386c2729de4a53b047308f0f528821",
      "9fe2c6cfaf74417b8a2e6cfb4471d762",
      "2e49b85142c34f69848aaa1d1a507b8f",
      "09a5592d582345409efba484fef84fda",
      "48c03d03ca704c888f27a34ab2d6a395",
      "1cd2ae68862d4e118853ddf6eb563eb3",
      "0b6dff0363e643709ae7a24e9b785675",
      "58f62a690bda4aa2961b22753e94c4b1",
      "9db2f9928cba4a98bbc4540e3de1e3ca",
      "2191d1fb4f3343a18a3d13a9fba2874b",
      "52d33a99aba04340b425e836b3050cba",
      "830a62cef6d34d64816c6aedab1cd884",
      "59eb0707fb1a4e28be79fce6ec37bcfc",
      "389f83cf4ca24da290c66d6909c23b9a",
      "2f0fb233a2a44274bc99eeaeb4248174",
      "81d8c19ed214456aad49515986cc4f81",
      "86a2ec4162644f5daa3e21a4722c437e",
      "bcafe73299cf460c9f418102c41b484f",
      "2c70f3ef6ace43b590c50e67b592e28b",
      "6cdc5153d556462883b3c82f63f6f7b1",
      "6a5eefa386de4b56989274744c6d6eef",
      "87fa08d289fa4c198b17fdbbba74e04d",
      "1520b36c0cec43dd8eefc3edba37c0d4",
      "be39dd792aa94aa8b2f69e7ff5b2aed7",
      "bb0e95e534d94939a0a0ef92d1a5949d",
      "3aa81485d2394985883becc53b681cda",
      "610a726b5d7348c5a925747a3b02d51f",
      "5bfa712c3a8b45fda9f79a2ec7e34531",
      "30a244ba7ddd4efcbeafe4828fbacbb1",
      "29007396f9354ddfa50c984df171fa8a",
      "d9a35ccf0f1f4c579ca9afe474876c50",
      "e1dbaa1380ac4f5b8c0ec07e1eebf32c",
      "11757e210a4e46dd84c2f7910e9514be",
      "b778a2d0cff84d0aaef1ddf0a092b075",
      "d33ea96b4e354f05be54cf197078454e",
      "001e102810ff43aca4c0ececa727fa25"
     ]
    },
    "executionInfo": {
     "elapsed": 66835,
     "status": "ok",
     "timestamp": 1744871788561,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "9er3dCv8dUAF",
    "outputId": "0169bc57-1fd3-4967-df4a-e1ba0e4964aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2e436a17984506a95b7ab634667f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287d4a482644441ab64fa14b6876c1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0fde95e7a8493d81bc78cf158dd26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605c70a951674defaba3949e94e4ceb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b322d245cf84462898e2a0fcb2dde529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a5592d582345409efba484fef84fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0fb233a2a44274bc99eeaeb4248174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa81485d2394985883becc53b681cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSdo3YJpdxII"
   },
   "source": [
    "## Step 4: Generating Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1744871890885,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "EFAn-Dq4dYqF"
   },
   "outputs": [],
   "source": [
    "def generate_answer(question: str) -> str:\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False  # greedy decoding\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"Answer:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339732,
     "status": "ok",
     "timestamp": 1744872245965,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "DVIP7NFeeB87",
    "outputId": "6717369c-ab2d-419b-9ea0-361d1540ecd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for item in eval_pairs:\n",
    "    question = item[\"question\"]\n",
    "    reference = item[\"answer\"]\n",
    "    prediction = generate_answer(f\"Question: {question}\\nAnswer:\")\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"reference\": reference,\n",
    "        \"prediction\": prediction\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1744872290104,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "R5aEjGkUeFtW"
   },
   "outputs": [],
   "source": [
    "with open(\"eval_predictions_baseline.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1744872318098,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "a07mtQWtfjav",
    "outputId": "43f2a694-af4b-4b83-f799-5c1c9e5bbcbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ./data/evaluation/eval_predictions_closed_book_baseline.json\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./data/evaluation/eval_predictions_closed_book_baseline.json\"\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHpEs9A9fyHp"
   },
   "source": [
    "## Step 6: BLEU Score Evaluation\n",
    "\n",
    "In this section, we evaluate our fine-tuned model using the **BLEU (Bilingual Evaluation Understudy)** score, a standard metric for evaluating the quality of generated text by comparing it to a reference answer.\n",
    "\n",
    "### What is BLEU?\n",
    "BLEU measures *n-gram overlap* between the model's prediction and the reference answer:\n",
    "- **BLEU-1**: unigram overlap (word-level similarity)\n",
    "- **BLEU-2**: bigram overlap (2-word chunks)\n",
    "- **BLEU-3**: trigram overlap\n",
    "- **BLEU-4**: 4-gram overlap (more stringent)\n",
    "\n",
    "### Components of the Code:\n",
    "- `weights=(1, 0, 0, 0)`: Measures unigram overlap only (BLEU-1).\n",
    "- `smoothing_function=method1`: Prevents the BLEU score from dropping to 0 when there are no exact n-gram matches. This is useful for short or paraphrased responses.\n",
    "- We iterate over our evaluation dataset and compute BLEU-1 through BLEU-4 for each response.\n",
    "\n",
    "### Limitations:\n",
    "BLEU is a **surface-level** metric:\n",
    "- It penalizes paraphrasing.\n",
    "- It doesn't understand meaningâ€”only *form*.\n",
    "- It is useful for rough comparison, but **not sufficient alone** to assess model quality.\n",
    "\n",
    "Hence, we will also perform **qualitative evaluation** using *LLM-as-a-Judge* in the next step.\n",
    "\n",
    "### Results:\n",
    "Our average scores were:\n",
    "- BLEU-1: *e.g., 0.22*\n",
    "- BLEU-2: *e.g., 0.11*\n",
    "- BLEU-3: *e.g., 0.07*\n",
    "- BLEU-4: *e.g., 0.05*\n",
    "\n",
    "These low scores are expected, since:\n",
    "1. The evaluation was *closed-book* (no document context).\n",
    "2. The questions were from **papers published in 2025**, after the model's training cutoff.\n",
    "3. The model had not seen any of these papers during fine-tuning.\n",
    "\n",
    "**Conclusion**: BLEU gives us a sense of lexical similarity. In high-difficulty settings like this one, it must be supplemented with qualitative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1744872379252,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "UsVpiT8nfqNK"
   },
   "outputs": [],
   "source": [
    "# Load model predictions\n",
    "with open(\"eval_predictions_baseline.json\", \"r\") as f:\n",
    "    eval_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1744872403232,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "D7hmShytf5LI"
   },
   "outputs": [],
   "source": [
    "smooth = SmoothingFunction().method1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1744872411503,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "Ja55GI5of_Cm"
   },
   "outputs": [],
   "source": [
    "bleu_scores = {\n",
    "    \"BLEU-1\": [],\n",
    "    \"BLEU-2\": [],\n",
    "    \"BLEU-3\": [],\n",
    "    \"BLEU-4\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1744872430338,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "FQuyTDBdgBCv",
    "outputId": "0c6c4a94-5e36-487b-da4c-6b9f7ac9f821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Scores: {'BLEU-1': 0.0866, 'BLEU-2': 0.0354, 'BLEU-3': 0.0178, 'BLEU-4': 0.0096}\n"
     ]
    }
   ],
   "source": [
    "for item in eval_results:\n",
    "    reference = item[\"reference\"].split()\n",
    "    prediction = item[\"prediction\"].split()\n",
    "\n",
    "    bleu_scores[\"BLEU-1\"].append(sentence_bleu([reference], prediction, weights=(1, 0, 0, 0), smoothing_function=smooth))\n",
    "    bleu_scores[\"BLEU-2\"].append(sentence_bleu([reference], prediction, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth))\n",
    "    bleu_scores[\"BLEU-3\"].append(sentence_bleu([reference], prediction, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth))\n",
    "    bleu_scores[\"BLEU-4\"].append(sentence_bleu([reference], prediction, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth))\n",
    "\n",
    "# Compute average BLEU scores\n",
    "avg_bleu_scores = {metric: round(sum(scores)/len(scores), 4) for metric, scores in bleu_scores.items()}\n",
    "print(\"Average BLEU Scores:\", avg_bleu_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZdW8gfvgLyT"
   },
   "source": [
    "## Step 7: Using GPT-4o as LLM-as-a-Judge (OpenAI Evaluation)\n",
    "\n",
    "In this section, we use **GPT-4o**â€”a state-of-the-art model from OpenAIâ€”as a neutral third-party judge to evaluate the quality of our modelâ€™s predictions against ground truth answers. This is part of the **LLM-as-a-Judge** evaluation methodology, which is growing in popularity as a way to assess open-ended outputs where metrics like BLEU or ROUGE may fall short.\n",
    "\n",
    "**What this section does:**\n",
    "\n",
    "- Loads model predictions from `eval_predictions.json`\n",
    "- Uses a GPT-4o prompt that provides:\n",
    "  - The question\n",
    "  - The model's generated answer\n",
    "  - The reference (ground-truth) answer\n",
    "- Asks GPT-4o to score the generated answer on a **scale from 1 to 5**, considering relevance, correctness, completeness, and style\n",
    "- Stores all outputs in `gpt4o_judged_results.json` for analysis\n",
    "\n",
    "**Key Functions:**\n",
    "\n",
    "- `ask_gpt_judge()` â†’ Sends a prompt to GPT-4o via the OpenAI API and returns a numeric score\n",
    "- `judged_results` â†’ A list of evaluation records including the question, reference, model prediction, and GPT-4o's score\n",
    "- `np.mean()` â†’ Used at the end to compute the **average evaluation score** across all QA pairs\n",
    "\n",
    "**Why use GPT-4o?**\n",
    "\n",
    "Because LLMs are best judged by **other LLMs** capable of contextual understanding. GPT-4o has been shown to be highly consistent and reliable in comparative evaluations.\n",
    "\n",
    "This evaluation complements our BLEU score by offering a **semantic and qualitative assessment**, helping us better understand the strengths and weaknesses of our fine-tuned model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11276,
     "status": "ok",
     "timestamp": 1744872476786,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "5My0OrxdgFoz",
    "outputId": "78beda3f-1d88-4d35-d77b-04c990e53991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1744872486603,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "u0iycKfpgOPI"
   },
   "outputs": [],
   "source": [
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1744872494114,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "_6h9NiCfgTZZ"
   },
   "outputs": [],
   "source": [
    "# Load the API key from environment variable\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1744872561503,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "TAwG6cF9gVJQ"
   },
   "outputs": [],
   "source": [
    "def ask_gpt_judge(question, reference, prediction):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert model evaluator. Given a question, a reference answer, and a model-generated answer, judge how good the modelâ€™s answer is on a scale of 1 to 5. Use the following rubric:\n",
    "\n",
    "1 â€“ Completely irrelevant or hallucinated.\n",
    "2 â€“ Partially related but mostly inaccurate.\n",
    "3 â€“ Mostly accurate but missing key details.\n",
    "4 â€“ Accurate and mostly complete.\n",
    "5 â€“ Nearly identical in meaning to the reference.\n",
    "\n",
    "Be strict but fair. Output ONLY the number.\n",
    "\n",
    "Question: {question}\n",
    "Reference Answer: {reference}\n",
    "Model Prediction: {prediction}\n",
    "\n",
    "Score:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(\"Error during evaluation:\\n\")\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744872582431,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "hBhw8xIsglrS"
   },
   "outputs": [],
   "source": [
    "with open(\"eval_predictions_baseline.json\") as f:\n",
    "    eval_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49167,
     "status": "ok",
     "timestamp": 1744872640683,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "w8JAUu35gqx0",
    "outputId": "c6fac384-cce9-466a-f977-31d70a2f137c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 1/30\n",
      "Evaluating 2/30\n",
      "Evaluating 3/30\n",
      "Evaluating 4/30\n",
      "Evaluating 5/30\n",
      "Evaluating 6/30\n",
      "Evaluating 7/30\n",
      "Evaluating 8/30\n",
      "Evaluating 9/30\n",
      "Evaluating 10/30\n",
      "Evaluating 11/30\n",
      "Evaluating 12/30\n",
      "Evaluating 13/30\n",
      "Evaluating 14/30\n",
      "Evaluating 15/30\n",
      "Evaluating 16/30\n",
      "Evaluating 17/30\n",
      "Evaluating 18/30\n",
      "Evaluating 19/30\n",
      "Evaluating 20/30\n",
      "Evaluating 21/30\n",
      "Evaluating 22/30\n",
      "Evaluating 23/30\n",
      "Evaluating 24/30\n",
      "Evaluating 25/30\n",
      "Evaluating 26/30\n",
      "Evaluating 27/30\n",
      "Evaluating 28/30\n",
      "Evaluating 29/30\n",
      "Evaluating 30/30\n"
     ]
    }
   ],
   "source": [
    "judged_results = []\n",
    "\n",
    "for i, item in enumerate(results):\n",
    "    print(f\"Evaluating {i+1}/{len(results)}\")\n",
    "    score = ask_gpt_judge(item[\"question\"], item[\"reference\"], item[\"prediction\"])\n",
    "    if score:\n",
    "        judged_results.append({\n",
    "            \"question\": item[\"question\"],\n",
    "            \"reference\": item[\"reference\"],\n",
    "            \"prediction\": item[\"prediction\"],\n",
    "            \"gpt4o_score\": score\n",
    "        })\n",
    "    time.sleep(1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1744872684971,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "RbPmP5yjgtA6"
   },
   "outputs": [],
   "source": [
    "with open(\"gpt4o_judgments_baseline.json\", \"w\") as f:\n",
    "    json.dump(judged_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1744872708208,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "FtxA61tjhDz-",
    "outputId": "d3276935-0495-4f7d-f063-04e6e959d57a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: What is the primary innovation introduced by the LoRI method for parameter-efficient fine-tuning?\n",
      " Reference Answer: LoRI introduces a novel approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks, thereby significantly reducing trainable parameters while minimizing cross-task interference.\n",
      " Model Prediction: The LoRI (Learning Rate Inversion) method for parameter-efficient fine-tuning introduces a novel approach where the learning rate of the pre-trained model is inverted during fine-tuning. This means that instead of using a constant learning rate, the learning rate is set to the inverse of the number of training steps taken so far. This approach has been shown to improve the performance of fine-tuning and make it more stable.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does LoRI reduce the number of trainable parameters compared to traditional LoRA?\n",
      " Reference Answer: LoRI reduces the number of trainable parameters by keeping matrix A fixed as a random projection and sparsifying matrix B using task-specific masks, eliminating the need to train both matrices and reducing redundancy.\n",
      " Model Prediction: LoRA (Layer-wise Relevance Analysis) is a method for channeling knowledge from a large pre-trained model to a smaller model. In traditional LoRA, a linear layer is added to each layer of the pre-trained model, and the weights of these linear layers are learned during fine-tuning. This results in a large number of trainable parameters.\n",
      "\n",
      "LoRI (Layer-wise Relevance Inference) is a variant of LoRA that aims to further reduce the number of trainable parameters. Instead of learning the weights of the linear layers, LoRI infers them based on the relevance scores of the pre-trained model. This means that LoRI only requires the pre-trained model and the relevance scores to be stored, and no additional trainable parameters are needed. This makes LoRI more memory-efficient and faster to train compared to traditional LoRA.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why is sparsity in matrix B important in LoRI?\n",
      " Reference Answer: Sparsity in matrix B enables LoRI to retain only the most critical elements necessary for adaptation, reducing parameter count and mitigating cross-task interference during adapter merging and continual learning.\n",
      " Model Prediction: In LoRI (Localized Regularized Iterative Hard Thresholding), sparsity in matrix B is important because it allows for the identification of a small number of significant variables (or features) in the linear model. This is particularly useful in high-dimensional data analysis, where the number of variables can be much larger than the number of observations. By promoting sparsity, LoRI can help to reduce overfitting and improve the interpretability of the model. The sparse solution obtained by LoRI can be used to identify the most important variables that contribute significantly to the response variable, and these can then be used for further analysis or decision making.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does LoRI improve the process of merging adapters in multi-task scenarios?\n",
      " Reference Answer: LoRI enables more effective adapter merging by using fixed, randomly initialized projection matrices A, which maps task-specific adapters into approximately orthogonal subspaces, thus reducing parameter interference.\n",
      " Model Prediction: LoRI (Learning-based Oligonucleotide Read-In) improves the process of merging adapters in multi-task scenarios by using a learning-based approach to identify and merge adapter sequences. In multi-task scenarios, where multiple samples are being sequenced simultaneously, the presence of multiple adapter sequences can complicate the analysis process. LoRI addresses this issue by learning a model of adapter sequences from a training dataset, and then using this model to identify and merge adapter sequences in new, unseen samples. This approach allows for more accurate and efficient adapter trimming, even in the presence of multiple adapter sequences.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What mechanism does LoRI use to mitigate catastrophic forgetting in continual learning?\n",
      " Reference Answer: LoRI mitigates catastrophic forgetting by applying task-specific sparse masks to matrix B, which isolates parameter updates across tasks and preserves knowledge from previous adaptations, including safety alignment.\n",
      " Model Prediction: LoRI (Learning with Replay Interval) mitigates catastrophic forgetting in continual learning by periodically revisiting old data during training. This revisiting, or replay, helps the model to maintain its knowledge of previous tasks, reducing the forgetting effect. The replay interval, which is a hyperparameter, determines how often the old data is revisited. This approach allows the model to adapt to new tasks while retaining its knowledge of previous ones.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: On what benchmark did LoRI with 90% sparsity in B outperform LoRA, and by how much?\n",
      " Reference Answer: LoRI with 90% sparsity in B outperformed LoRA by 17.3% on the HumanEval benchmark using the Llama-3 model.\n",
      " Model Prediction: LoRI with 90% sparsity in B outperformed LoRA on the GLUE benchmark, specifically on the SST-2 and MRPC tasks. The improvement was not specified in the question, but it's important to note that the exact improvement can vary depending on the specific model configurations and hyperparameters used.\n",
      "\n",
      "LoRI (Layer-wise Relevance Inference) is a method for interpreting transformer models by identifying the most important words in each layer of the model for a given input. It does this by pruning the weights of the model and then retraining it to maintain performance while reducing the number of parameters.\n",
      "\n",
      "LoRA (Layer-wise Relevance Attention) is a similar method, but it uses attention mechanisms to identify the most important words in each layer.\n",
      "\n",
      "The GLUE (General Language Understanding Evaluation) benchmark is a collection of nine natural language understanding tasks, including SST-2 (Sentiment Analysis on Stanford Splits) and MRPC (Microsoft Research Paraphrase Corpus). These tasks are used to evaluate the performance of models on a variety of language understanding tasks.\n",
      "\n",
      "By using sparsity in the B layers (the output layers\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does LoRI compare to full fine-tuning and other PEFT methods in terms of performance and efficiency?\n",
      " Reference Answer: LoRI matches or outperforms full fine-tuning and other PEFT methods across multiple domains while using up to 95% fewer trainable parameters than LoRA, demonstrating both high performance and high efficiency.\n",
      " Model Prediction: LoRA (Layer-wise Relevance Analysis) is a parameter-efficient transfer learning (PETL) method that is similar to LoRa (Layer-wise Relevance Analysis) but for models. It is designed to adapt pre-trained models to new tasks with minimal additional parameters.\n",
      "\n",
      "In terms of performance, LoRA has been shown to perform competitively with other PETL methods such as TinyBERT, DynaBERT, and PET, and even outperform them in some cases. However, the exact performance can depend on the specific task, dataset, and pre-trained model being used.\n",
      "\n",
      "In terms of efficiency, LoRA is generally more efficient than full fine-tuning because it requires fewer parameters to be learned. This can make it a more practical choice for tasks where computational resources are limited.\n",
      "\n",
      "Compared to other PEFT methods, the exact performance and efficiency of LoRA can vary. For example, methods like Proximal Adaptation (ProxAd) and Lottery Tickets Hypothesis (LTH) can sometimes achieve better performance, but at the cost of more parameters and potentially higher computational requirements.\n",
      "\n",
      "Ultimately, the choice between LoRA\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What types of tasks were used to evaluate LoRI's effectiveness?\n",
      " Reference Answer: LoRI was evaluated on a diverse set of tasks, including natural language understanding, mathematical reasoning, code generation, and safety alignment.\n",
      " Model Prediction: To evaluate LoRI's effectiveness, various tasks were employed, including:\n",
      "\n",
      "1. Text Summarization: LoRI was used to generate summaries from long documents, comparing the results with human-written summaries and other summarization models.\n",
      "\n",
      "2. Text Classification: LoRI was tested on various text classification tasks, such as sentiment analysis, topic classification, and spam detection.\n",
      "\n",
      "3. Text Generation: LoRI was used to generate coherent and contextually relevant text, such as writing essays, stories, and poetry.\n",
      "\n",
      "4. Text Editing: LoRI was employed to edit and correct errors in text, such as grammar correction, style correction, and fact-checking.\n",
      "\n",
      "5. Text Translation: LoRI was used to translate text from one language to another, comparing the results with human translations and other translation models.\n",
      "\n",
      "6. Text Style Transfer: LoRI was used to transfer the style of one text to another, such as writing a news article in the style of a novel or a poem in the style of a news article.\n",
      "\n",
      "7. Text Summarization with Extraction: LoRI was used to generate summaries with specific information extracted from the original text,\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What potential future directions do the authors propose for extending LoRI?\n",
      " Reference Answer: The authors suggest exploring structured sparsity patterns like block sparsity or head pruning and adapting LoRI to multi-modal models such as diffusion and vision-language systems.\n",
      " Model Prediction: The authors propose several potential future directions for extending LoRI:\n",
      "\n",
      "1. Incorporating more complex models: The authors suggest that LoRI could be extended to incorporate more complex models, such as deep learning models, to improve its ability to handle more complex and nuanced data.\n",
      "\n",
      "2. Improving interpretability: The authors note that while LoRI is designed to be interpretable, there is still room for improvement in this area. They suggest that future work could focus on developing methods to make the model's decisions more transparent and understandable.\n",
      "\n",
      "3. Scaling up to larger datasets: The authors acknowledge that LoRI is currently limited in its ability to handle large datasets. They suggest that future work could focus on developing methods to scale up LoRI to handle larger datasets.\n",
      "\n",
      "4. Applying LoRI to other domains: The authors suggest that LoRI could be applied to other domains, such as natural language processing, image recognition, and recommendation systems.\n",
      "\n",
      "5. Integrating LoRI with other techniques: The authors suggest that LoRI could be integrated with other techniques, such as reinforcement learning, to further improve its performance.\n",
      "\n",
      "6. Evaluating LoRI on more diverse datasets: The authors note that Lo\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What is the broader significance of LoRI in the context of PEFT and LLM deployment?\n",
      " Reference Answer: LoRI provides a lightweight, modular, and scalable solution for adapting LLMs with minimal overhead, making it particularly suited for multi-task learning, safety-critical alignment, and efficient deployment on resource-constrained hardware.\n",
      " Model Prediction: LoRa (Long Range) is a wireless communication protocol designed for low-power, long-range, and low-data-rate applications. In the context of Physical Endpoint Function Templates (PEFT) and Large Language Models (LLMs), LoRa can be significant in the following ways:\n",
      "\n",
      "1. IoT Device Communication: LoRa can be used to connect IoT devices that are part of a PEFT or LLM deployment. These devices can collect data, send alerts, or perform actions based on the instructions from the LLM.\n",
      "\n",
      "2. Low-Power Requirements: LoRa's low-power consumption makes it ideal for devices that are battery-powered and need to operate for extended periods without needing to be recharged. This is particularly useful in remote or hard-to-reach locations where replacing batteries can be challenging.\n",
      "\n",
      "3. Long-Range Coverage: LoRa's long-range capabilities can help in deploying PEFTs and LLMs in large geographical areas where traditional communication methods might not be feasible or cost-effective.\n",
      "\n",
      "4. Cost-Effective: LoRa devices and networks are generally less expensive to deploy and maintain\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What are the core limitations of traditional LoRA methods that ElaLoRA seeks to address?\n",
      " Reference Answer: ElaLoRA addresses two key limitations of traditional LoRA: the fixed rank allocation across layers, which overlooks the layer-specific importance, and the inability to adapt ranks dynamically during training, which can lead to suboptimal parameter efficiency.\n",
      " Model Prediction: Traditional LoRa methods have several core limitations that ElaLoRA aims to address:\n",
      "\n",
      "1. Limited Data Rate: Traditional LoRa has a limited data rate, which can be insufficient for certain IoT applications that require real-time, high-volume data transmission.\n",
      "\n",
      "2. Interference: LoRa networks can be susceptible to interference from other devices operating in the same frequency band, leading to decreased network performance and increased error rates.\n",
      "\n",
      "3. Range Limitations: The range of traditional LoRa is limited, especially in urban environments where buildings and other obstacles can block signals.\n",
      "\n",
      "4. Battery Life: While LoRa devices are known for their long battery life, they can still consume significant power during data transmission, especially in high-data-rate applications.\n",
      "\n",
      "5. Security: Traditional LoRa lacks robust security measures, making it vulnerable to eavesdropping and data tampering.\n",
      "\n",
      "6. Scalability: As the number of devices in a LoRa network increases, the network can become congested, leading to decreased performance and increased error rates.\n",
      "\n",
      "ElaLoRA addresses these limitations by using a combination of LoR\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Describe the three core components of the ElaLoRA framework.\n",
      " Reference Answer: ElaLoRA's architecture consists of: (1) an SVD-based adaptation strategy for matrix decomposition, (2) an importance score calculation mechanism based on loss gradients to assess rank relevance, and (3) a dynamic rank learning algorithm that reallocates ranks periodically during training to optimize layer-wise adaptation.\n",
      " Model Prediction: The ElaLoRA framework, a novel approach for low-resource African language (LRL) processing, consists of three core components:\n",
      "\n",
      "1. Language-specific pre-training: This involves training a language model on a large corpus of text specific to the LRL. The model learns the unique characteristics and patterns of the language.\n",
      "\n",
      "2. Cross-lingual transfer learning: After pre-training on the LRL corpus, the model is fine-tuned on a multilingual corpus, such as WikiText-103, to learn more general language patterns. This cross-lingual learning helps to mitigate the data scarcity issue in LRLs.\n",
      "\n",
      "3. Language-specific post-training: Finally, the model is further fine-tuned on task-specific LRL data, such as a small dataset of labeled sentences for a specific NLP task (e.g., named entity recognition, part-of-speech tagging, etc.). This step ensures that the model is adapted to the specific requirements of the target LRL and the task at hand.\n",
      "\n",
      "By combining these three components, the ElaLoRA framework aims to create effective NLP models for L\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does ElaLoRAâ€™s adaptive strategy improve performance under limited parameter budgets?\n",
      " Reference Answer: ElaLoRA reallocates computational resources to the most critical layers by pruning less important ranks and expanding ranks in essential layers, thus achieving higher performance even under smaller parameter budgetsâ€”for example, outperforming other PEFT methods with r=2 compared to their r=4 settings.\n",
      " Model Prediction: ElaLoRA's adaptive strategy improves performance under limited parameter budgets by dynamically adjusting the number of parameters based on the data. This is achieved through a two-stage training process: first, a coarse-grained model is trained with a large number of parameters, and then a fine-grained model is trained with a smaller number of parameters. The coarse-grained model captures the general trends in the data, while the fine-grained model focuses on the details. This approach allows ElaLoRA to achieve good performance with a smaller number of parameters, making it more efficient in terms of computational resources.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: In what way does ElaLoRA achieve better task alignment during fine-tuning?\n",
      " Reference Answer: ElaLoRA uses gradient-derived importance scores to identify which layers contribute most to task-specific learning, allowing the model to allocate more capacity to those layers and thus improving task alignment and learning efficiency.\n",
      " Model Prediction: ElaLoRA achieves better task alignment during fine-tuning by using a two-stage training process. In the first stage, it pre-trains the model on a large corpus of data with a language modeling objective. In the second stage, it fine-tunes the model on a smaller dataset specific to the task at hand, such as text classification or question answering. This two-stage approach allows the model to learn a general language representation in the first stage, which can then be fine-tuned to perform well on specific tasks in the second stage. This results in better task alignment during fine-tuning compared to models that are only fine-tuned on the task-specific dataset without pre-training.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What experimental evidence supports the superiority of ElaLoRA over other PEFT methods?\n",
      " Reference Answer: Experiments across NLU, NLG, and vision benchmarks show that ElaLoRA consistently outperforms state-of-the-art PEFT methods in accuracy, particularly under constrained parameter budgets, and demonstrates better GLUE benchmark performance even with fewer trainable parameters.\n",
      " Model Prediction: ElaLoRA, a PEFT (Prompt-based Efficient Transformer) method, has shown promising results in various experiments. Here are some key points that support its superiority over other PEFT methods:\n",
      "\n",
      "1. **Efficiency**: ElaLoRA significantly reduces the number of parameters and computational cost compared to other PEFT methods, making it more efficient for large-scale applications.\n",
      "\n",
      "2. **Performance**: In several benchmarks, ElaLoRA has demonstrated competitive or superior performance to other PEFT methods, such as LoRA and T5-Data2Vec, in terms of perplexity and accuracy on various tasks, including text generation, translation, and question answering.\n",
      "\n",
      "3. **Generalization**: ElaLoRA has shown strong generalization capabilities, performing well on tasks it was not explicitly trained on, which is a desirable property for a PEFT method.\n",
      "\n",
      "4. **Adaptability**: ElaLoRA can be easily adapted to various pretrained models, making it a flexible choice for different applications.\n",
      "\n",
      "5. **Interpretability**: ElaLoRA provides a clear and interpretable way to understand the learned representations, as it directly modifies the prompts rather\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why is ElaLoRA particularly well-suited for resource-constrained environments?\n",
      " Reference Answer: ElaLoRA's dynamic pruning and expansion mechanism ensures that only the most essential ranks are trained, reducing memory usage and computational cost while maintaining high performance, making it ideal for low-resource scenarios.\n",
      " Model Prediction: ElaLoRA is designed to be lightweight and efficient, making it well-suited for resource-constrained environments. It uses a compact model architecture, and it can be run on devices with limited computational resources. Additionally, it is designed to be efficient in terms of memory usage, which is another important consideration for resource-constrained environments.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How does the final rank distribution in ElaLoRA reflect its adaptive learning process?\n",
      " Reference Answer: ElaLoRAâ€™s final rank distribution reveals that higher ranks are allocated to layers deemed more important via importance scores, confirming that the model dynamically concentrates learning capacity on the most impactful parts of the network.\n",
      " Model Prediction: The final rank distribution in ElaLoRA, an adaptive language model, can provide insights into its learning process. A well-adapted model should have a relatively even distribution, where the top-ranked models are consistently accurate and the lower-ranked models are less accurate but still useful. This distribution can change over time as the model continues to learn and improve. However, it's important to note that the exact distribution can vary depending on the specific task, dataset, and evaluation metrics used.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What are the broader implications of ElaLoRAâ€™s design for the future of fine-tuning large models?\n",
      " Reference Answer: ElaLoRAâ€™s design shows that adaptive, importance-based rank allocation can significantly improve parameter efficiency without sacrificing accuracy, suggesting a paradigm shift toward more intelligent and resource-aware fine-tuning strategies.\n",
      " Model Prediction: ElaLoRA's design has several significant implications for the future of fine-tuning large models:\n",
      "\n",
      "1. Efficiency: ElaLoRA's efficient design allows for faster fine-tuning, which can save time and resources, especially when working with large models or large datasets.\n",
      "\n",
      "2. Adaptability: The ability to adapt the model to specific tasks or domains without significant loss in performance can make models more versatile and useful in a wider range of applications.\n",
      "\n",
      "3. Accessibility: By reducing the computational requirements for fine-tuning, ElaLoRA could make large language models more accessible to researchers and organizations with limited computational resources.\n",
      "\n",
      "4. Generalization: The approach could lead to models that generalize better across tasks and domains, as the model learns to focus on the most relevant features for a given task.\n",
      "\n",
      "5. Ethical considerations: As with any AI technology, the ability to fine-tune large models more efficiently could have ethical implications. It's important to ensure that these models are used responsibly, to avoid potential misuse or bias.\n",
      "\n",
      "6. Research and development: The success of ElaLoRA could spur further research into efficient and\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What distinguishes ElaLoRA from prior dynamic rank methods like AdaLoRA or IncreLoRA?\n",
      " Reference Answer: While AdaLoRA and IncreLoRA either prune or expand ranks, ElaLoRA is the first to implement both pruning and expansion dynamically during training, offering a more flexible and principled mechanism for allocating parameter capacity.\n",
      " Model Prediction: ElaLoRA, AdaLoRA, and IncreLoRA are all dynamic rank methods for language models, but they have some key differences:\n",
      "\n",
      "1. ElaLoRA: ElaLoRA (Efficiently Learning Adaptive Language-specific Ranks) is a method that learns adaptive ranks for each language during pretraining. It does this by using a language-specific rank for each token in the vocabulary, which is learned during pretraining. ElaLoRA also uses a language-agnostic rank for tokens that are not specific to a language. This allows ElaLoRA to adapt to the specific characteristics of each language, such as the frequency of certain characters or the length of sentences.\n",
      "\n",
      "2. AdaLoRA: AdaLoRA (Adaptive Language-specific Ranks) is a method that learns adaptive ranks for each language during fine-tuning. It does this by adjusting the ranks of the tokens in the vocabulary based on the specific task and dataset being used for fine-tuning. AdaLoRA uses a language-agnostic rank for tokens that are not specific to a language.\n",
      "\n",
      "3. IncreLoRA: IncreLoRA (Incremental Language\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why is parameter-efficient fine-tuning increasingly important in the LLM landscape?\n",
      " Reference Answer: As LLMs grow in size, full fine-tuning becomes prohibitively expensive, especially for domain-specific or low-resource settings. PEFT methods like ElaLoRA offer a practical solution by enabling adaptation with minimal compute and storage costs.\n",
      " Model Prediction: Parameter-efficient fine-tuning (PEFT) is becoming increasingly important in the large language model (LLM) landscape for several reasons:\n",
      "\n",
      "1. **Efficiency**: PEFT allows for fine-tuning large models with fewer computational resources compared to traditional fine-tuning methods. This is crucial as the size of LLMs continues to grow, making training and fine-tuning them computationally expensive.\n",
      "\n",
      "2. **Adaptability**: PEFT allows for more targeted adaptation of models to specific tasks or domains. This is because it modifies only a small portion of the model parameters, rather than the entire model, which can lead to overfitting if the data is not representative.\n",
      "\n",
      "3. **Versatility**: PEFT can be combined with various techniques, such as prompt engineering, few-shot learning, and model cards, to further improve the performance of LLMs.\n",
      "\n",
      "4. **Ethical considerations**: PEFT can help address ethical concerns related to the carbon footprint of training large models, as it reduces the computational resources required.\n",
      "\n",
      "5. **Scalability**: As the size of LLMs continues to increase, PEFT provides a scalable solution for fine-tuning these\n",
      " GPT-4o Evaluation: 4\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What is the primary goal of the study presented in 'Beyond QA Pairs'?\n",
      " Reference Answer: The study aims to assess the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain-specific facts into LLMs, focusing on the impact of QA pair categorization and synthetic dataset generation techniques.\n",
      " Model Prediction: The primary goal of the study presented in 'Beyond QA Pairs' is to develop a more effective approach for question answering (QA) tasks by moving beyond the traditional question-answer pair format. The study proposes a new method called \"Knowledge-Grounded Question Answering\" (KGQA), which leverages knowledge graphs to improve the performance of QA systems. The KGQA approach aims to better capture the semantic relationships between questions and answers, leading to more accurate and contextually relevant responses.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How are QA pairs categorized in this study, and what is the purpose of this categorization?\n",
      " Reference Answer: QA pairs are classified into â€˜Factualâ€™ and â€˜Conceptualâ€™ categories using a BERT-based classifier. The purpose is to investigate how the nature of QA pairs influences the effectiveness of PEFT.\n",
      " Model Prediction: In this study, Quality Assurance (QA) pairs are categorized based on the type of QA task they perform. The purpose of this categorization is to understand the distribution of QA tasks across different domains and to analyze the performance of QA systems in each domain. The categories used in this study include:\n",
      "\n",
      "1. Factual QA: These QA pairs are designed to test the system's ability to answer factual questions, such as questions about dates, names, and specific details.\n",
      "\n",
      "2. Commonsense QA: These QA pairs test the system's ability to answer questions that require commonsense knowledge, such as questions about causality, comparisons, and reasoning.\n",
      "\n",
      "3. Opinion QA: These QA pairs test the system's ability to answer questions that require subjective judgments, such as questions about preferences, opinions, and emotions.\n",
      "\n",
      "4. Complex QA: These QA pairs test the system's ability to answer complex questions that require multiple steps to answer, such as questions that involve multiple entities, conditions, and relationships.\n",
      "\n",
      "5. Conversational QA: These QA pairs test the system's ability to carry on a conversation,\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What were the findings regarding models trained on conceptual vs factual QA datasets?\n",
      " Reference Answer: Models fine-tuned on conceptual datasets consistently outperformed those trained on factual datasets across multiple evaluations.\n",
      " Model Prediction: Models trained on conceptual QA datasets, such as those based on natural language inference tasks, tend to focus on understanding relationships and reasoning abilities. On the other hand, models trained on factual QA datasets, like those based on factual knowledge bases, excel at retrieving specific facts and answering direct questions. The choice between these two types of datasets depends on the specific application requirements. For example, a model designed for a chatbot might benefit more from a factual QA dataset, while a model for a more complex task like text summarization might benefit more from a conceptual QA dataset.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Which synthetic dataset generation techniques are evaluated in this work, and which one performs better?\n",
      " Reference Answer: The paper evaluates D-RAG and D-Naive synthetic data generation methods. D-Naive outperformed D-RAG in fine-tuning effectiveness, largely due to better retrieval performance.\n",
      " Model Prediction: In this work, we evaluate four synthetic dataset generation techniques:\n",
      "\n",
      "1. GAN-based (GAN-Synthetic): Generative Adversarial Networks (GANs) are used to generate synthetic data that mimics the distribution of the real data.\n",
      "\n",
      "2. VAE-based (VAE-Synthetic): Variational Autoencoders (VAEs) are used to generate synthetic data by learning a probabilistic latent representation of the data.\n",
      "\n",
      "3. Autoencoder-based (AE-Synthetic): Autoencoders are used to learn an efficient representation of the data and then generate new data by passing noise through the encoder and decoder.\n",
      "\n",
      "4. MMD-based (MMD-Synthetic): Maximum Mean Discrepancy (MMD) is used to measure the distance between the real and synthetic data distributions, and synthetic data is generated to minimize this distance.\n",
      "\n",
      "The performance of these techniques is evaluated based on the following metrics:\n",
      "\n",
      "1. Data Utility: The ability of the synthetic data to improve the performance of a machine learning model when used as additional training data.\n",
      "\n",
      "2. Data Fidelity: The similarity\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What was the significance of the product recommendation task in the data center domain?\n",
      " Reference Answer: The task served as a practical demonstration showing that a Llama-2 7B model fine-tuned with PEFT on just 1,000 instruction-based QA pairs significantly outperformed the baseline in generating product recommendations.\n",
      " Model Prediction: The product recommendation task in the data center domain was significant as it aimed to predict the most suitable hardware products for a specific data center based on its existing infrastructure, workload requirements, and budget constraints. This task was crucial for data center managers to make informed decisions about hardware upgrades or new purchases, optimize resource utilization, and ensure the overall efficiency and performance of the data center. The recommendation system could also help in reducing costs by suggesting cost-effective solutions and minimizing the risk of hardware compatibility issues.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: Why do the authors argue that PEFT may not be optimal for factual embedding?\n",
      " Reference Answer: The study shows that while PEFT is effective for instruction tuning, it struggles with embedding factual information as effectively, likely due to its limited parameter update scope.\n",
      " Model Prediction: The authors argue that PEFT (Projection Embedding for Text) may not be optimal for factual embedding due to several reasons:\n",
      "\n",
      "1. PEFT is designed for learning continuous representations of words, but it does not explicitly model the relationships between words and facts.\n",
      "\n",
      "2. PEFT uses a shared embedding space for both words and facts, which can lead to ambiguity and confusion, especially when multiple facts share similar words.\n",
      "\n",
      "3. PEFT does not handle missing or incomplete information well. If a fact is not fully represented in the text, PEFT may not be able to accurately embed it.\n",
      "\n",
      "4. PEFT does not consider the context in which words and facts appear, which can be crucial for understanding their meaning and relationships.\n",
      "\n",
      "5. PEFT does not explicitly model the uncertainty or confidence in the facts it embeds, which can lead to errors in downstream tasks that require this information.\n",
      "\n",
      "6. PEFT does not scale well to large datasets, which can limit its usefulness in real-world applications.\n",
      "\n",
      "Therefore, while PEFT has been useful for some tasks, it may not be the best choice for factual embedding, especially in complex and uncertain environments.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What conclusions do the authors draw about the volume versus quality of QA data in PEFT?\n",
      " Reference Answer: They conclude that sheer quantity of QA pairs is insufficient; quality and conceptual depth are far more critical for successful PEFT.\n",
      " Model Prediction: The authors conclude that the volume of QA data in PEFT is not directly correlated with the quality of the QA data. They suggest that the quality of QA data is more dependent on factors such as the diversity and complexity of the questions, the quality of the annotations, and the effectiveness of the QA system. They also note that the volume of QA data can be a useful indicator of the system's performance, but it should not be the only factor considered when evaluating the quality of the QA data in PEFT.\n",
      " GPT-4o Evaluation: 3\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What limitations of D-RAG were identified in the study?\n",
      " Reference Answer: D-RAG's limitations were attributed to the poor performance of its underlying vector database retriever, leading to suboptimal training data quality.\n",
      " Model Prediction: In the study, several limitations of D-RAG (Differential RNA Accessibility Analysis) were identified:\n",
      "\n",
      "1. Dependence on high-quality data: D-RAG requires high-quality ATAC-seq and RNA-seq data, which can be challenging to obtain, especially for non-model organisms or tissues with low cellular complexity.\n",
      "\n",
      "2. Limited resolution: D-RAG identifies differentially accessible regions (DARs) between two conditions, but it does not provide information about the specific nucleotides within these regions that are responsible for the differences.\n",
      "\n",
      "3. Sensitivity to noise: D-RAG is sensitive to noise in the data, which can lead to false positives or false negatives. This is particularly problematic when working with low-coverage data.\n",
      "\n",
      "4. Assumption of linearity: D-RAG assumes that the relationship between RNA expression and DNA accessibility is linear, which may not always be the case.\n",
      "\n",
      "5. Lack of functional validation: While D-RAG can identify potential regulatory elements, it does not provide functional validation of these elements. This means that the identified DARs may not necessarily be\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n",
      " Question: How do the authors suggest future research should improve PEFT for fact embedding?\n",
      " Reference Answer: Future research should explore improvements in retrieval systems used by D-RAG, and consider more refined QA classification and data generation strategies.\n",
      " Model Prediction: The authors suggest several directions for future research to improve PEFT for fact embedding:\n",
      "\n",
      "1. Improve the quality of the pre-trained language model: The authors suggest using larger models, more diverse training data, and more sophisticated training methods to improve the quality of the pre-trained language model.\n",
      "\n",
      "2. Improve the quality of the factual knowledge base: The authors suggest using more diverse and high-quality factual knowledge bases, and developing methods to automatically generate and update the knowledge base.\n",
      "\n",
      "3. Improve the quality of the factual embeddings: The authors suggest developing methods to improve the quality of the factual embeddings, such as using more sophisticated embedding methods, and incorporating more contextual information into the embeddings.\n",
      "\n",
      "4. Improve the quality of the factual reasoning: The authors suggest developing methods to improve the quality of the factual reasoning, such as using more sophisticated reasoning algorithms, and incorporating more contextual information into the reasoning process.\n",
      "\n",
      "5. Improve the robustness of the PEFT system: The authors suggest developing methods to improve the robustness of the PEFT system, such as using more diverse and challenging test cases, and developing methods to handle ambiguous or conflicting information.\n",
      " GPT-4o Evaluation: 2\n",
      "--------------------------------------------------------------------------------\n",
      " Question: What is the key insight this paper contributes to the field of LLM fine-tuning?\n",
      " Reference Answer: The paper highlights that PEFT's success hinges more on dataset compositionâ€”especially the conceptual quality of QA pairsâ€”than on volume alone, and that careful use-case targeting is essential.\n",
      " Model Prediction: The key insight this paper contributes to the field of LLM fine-tuning is the importance of using a diverse set of prompts during the fine-tuning process to improve the model's ability to generalize and handle a wide range of tasks. The authors demonstrate that using a diverse set of prompts can lead to improved performance on a variety of downstream tasks, compared to fine-tuning on a single task or a small set of prompts. This finding suggests that a more comprehensive approach to fine-tuning, which exposes the model to a wide range of prompts, can lead to more versatile and capable models.\n",
      " GPT-4o Evaluation: 1\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sample in judged_results:\n",
    "    print(\" Question:\", sample[\"question\"])\n",
    "    print(\" Reference Answer:\", sample[\"reference\"])\n",
    "    print(\" Model Prediction:\", sample[\"prediction\"])\n",
    "    print(\" GPT-4o Evaluation:\", sample[\"gpt4o_score\"])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744872768560,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "Ke6MfKSvhJec",
    "outputId": "da5f810a-0a42-4600-df37-efa296544833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GPT-4o Evaluation Score: 1.57 out of 5\n"
     ]
    }
   ],
   "source": [
    "scores = [int(res[\"gpt4o_score\"]) for res in judged_results if res[\"gpt4o_score\"].isdigit()]\n",
    "average_score = np.mean(scores)\n",
    "print(f\"Average GPT-4o Evaluation Score: {average_score:.2f} out of 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744872796825,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "2_X5pvlQhYNv",
    "outputId": "fed2c6ca-3267-4196-b649-d4ef88a138bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judged results saved to ./data/evaluation/eval_gpt4o_judgments_closed_book_baseline.json\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./data/evaluation/eval_gpt4o_judgments_closed_book_baseline.json\"\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(judged_results, f, indent=2)\n",
    "\n",
    "print(f\"Judged results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F90r2izMhi3z"
   },
   "source": [
    "## Step 8: Evaluating with BERTScore (Semantic Similarity Metric)\n",
    "\n",
    "In this section, we evaluate the semantic similarity between the modelâ€™s predictions and the ground truth answers using **BERTScore**, a metric that leverages contextual embeddings from large pretrained models (like BERT) to assess the *meaning* of the outputs.\n",
    "\n",
    "Unlike BLEU, which only considers surface-level n-gram overlap, BERTScore measures how semantically close the answers areâ€”even when the phrasing differs.\n",
    "\n",
    "### Interpretation:\n",
    "- **BERTScore F1** reflects the degree of **semantic overlap** between model output and human-labeled answer.\n",
    "- A score closer to **1.0** indicates stronger alignment of meaning.\n",
    "- This metric is especially useful in open-ended QA or summarization settings where **exact matching isn't expected**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1744872822037,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "QeQZ6bTxhfH1"
   },
   "outputs": [],
   "source": [
    "# Replace `results` with `judged_results` if needed\n",
    "predictions = [item[\"prediction\"] for item in results]\n",
    "references = [item[\"reference\"] for item in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWDFZuh-hlRL"
   },
   "outputs": [],
   "source": [
    "P, R, F1 = bertscore(predictions, references, lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744872858741,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "uPY_k8fHhnFf",
    "outputId": "89d5ee4a-619a-4f0b-e7fe-923c8a67f2e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: -0.0061\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Precision: {P.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1744872866119,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "NKj5K_pAhuO3",
    "outputId": "462c8cb2-1fef-4f1f-a338-13502120e5ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall: 0.1956\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Recall: {R.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744872875557,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "UXdnTXHchwCi",
    "outputId": "5c784376-2e90-4110-d2fc-52d87bfecf86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BERTScore (F1): 0.0935\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average BERTScore (F1): {F1.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeQowDDVi-Ma"
   },
   "source": [
    "## Step 9: Fixing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2420,
     "status": "ok",
     "timestamp": 1744873208982,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "j3Q2BRz6hyWX"
   },
   "outputs": [],
   "source": [
    "pip install nbformat --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_C_3wTsjDKp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP885y0fK/TGnEsfwIOCes/",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
