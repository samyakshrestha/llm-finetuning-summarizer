{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKJQF-aqu0G3"
   },
   "source": [
    "## Step 1: Mounting Google Drive and Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1744265928962,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "JDHYBsqHurDz",
    "outputId": "1de2ea9c-7ef5-4d2b-c736-b6a7e205eb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n",
      "data\t    LICENSE  notebooks\t      qa_pairs\t results  wandb\n",
      "deployment  models   project_plan.md  README.md  scripts\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Navigate to the repo folder\n",
    "%cd /content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer\n",
    "\n",
    "# List repo contents\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4bwMj_5MU2m"
   },
   "source": [
    "## Step 2: Installing Dependencies and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81306,
     "status": "ok",
     "timestamp": 1744263973253,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "kgxwAaJsRNgh",
    "outputId": "7af2594c-5f1d-40b2-9477-5a63867f1e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes --prefer-binary --extra-index-url https://download.pytorch.org/whl/cu118 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4517,
     "status": "ok",
     "timestamp": 1744263977767,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "ZSLU485Cu4KR",
    "outputId": "b89c37c1-73ac-4f12-920f-2d7b09ffdd9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m368.6/491.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate peft datasets wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCFGTnJ9MaI1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from datasets import load_from_disk\n",
    "from huggingface_hub import login, HfApi, HfFolder, notebook_login\n",
    "import wandb\n",
    "import math\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 17764,
     "status": "ok",
     "timestamp": 1744264021378,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "QHaQ1e3clZ6m",
    "outputId": "87f6de20-a3c5-4cad-ea41-e518135278db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamyakshrestha\u001b[0m (\u001b[33msamyakshrestha-university-of-texas-at-dallas\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "dfc4b6704784450fb658971605968136",
      "5797f45939404d4dbabf4e032a8f322c",
      "24fd9a89e39f485f936711a4a307bb42",
      "42211380d5db4be293e402d35c915ce7",
      "922f0c69b1bc433c8fd5f9adba80af5c",
      "aaa2331c03fb4a50af6ed1d5b2c4118e",
      "82c90b88084c43c78c827bace593352e",
      "483cd80915b2468db03998f1324bde05",
      "fd02ad278cfc417b8b8009d204221424",
      "0daa87cd5a874bef848cbe0ac5edf5a1",
      "f32cf8386fc44a5fabe0252bda88817d",
      "bedbea77bfe94649ac64b675cacceaec",
      "946a9f45842848cbba811cf7c9121419",
      "05e48cd43e944c6db46ce3a13fd1c599",
      "49d87d92c63c4292b54365e18bb2b099",
      "0c8e4ede9fc34f13ba1b7f77d3d212b0",
      "152a851608994946a3d425a59f57277b",
      "a873d5bf10e246d59489fee104e6b434",
      "2b611f5080204d14a9c52696e600794b",
      "3d98270a18744a3fbf8d6aefc5315b54"
     ]
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1744264029834,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "V00LqIcjpST1",
    "outputId": "4c2c4300-11e6-48b7-e53e-6a778a1e5dca"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc4b6704784450fb658971605968136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A6zP24QNsCQ"
   },
   "source": [
    "## Step 3: Loading Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RmI7FRrNchu"
   },
   "outputs": [],
   "source": [
    "data_path = \"./data/tokenized_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3476,
     "status": "ok",
     "timestamp": 1744264051689,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "iyv-MgA6N7mC",
    "outputId": "a985a302-11da-4383-acc4-24cf2a253833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 210\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 24\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(data_path)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDMxvt7fhPXG"
   },
   "source": [
    "## Step 4: Verifying GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1744264053925,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "shV4KnQeN776",
    "outputId": "bac8eaee-c01f-4c47-d3c7-975762f659d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: NVIDIA A100-SXM4-40GB\n",
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability and set device\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU detected: {device_name}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not detected. Using CPU instead.\")\n",
    "\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXefyJYHtJep"
   },
   "source": [
    "## Step 5: Configuring LoRA  for PEFT (Parameter-Efficient Fine-Tuning)\n",
    "\n",
    "In this step, we define the configuration for **LoRA (Low-Rank Adaptation)** using the `LoraConfig` class from the `peft` library. LoRA is a widely adopted technique in fine-tuning large language models efficiently by injecting trainable low-rank matrices into specific parts of the transformer architecture.\n",
    "\n",
    "Key configuration parameters:\n",
    "- `r=8`: The rank of the LoRA update matrices. A commonly used value that balances adaptation capacity with compute efficiency.\n",
    "- `lora_alpha=16`: Scaling factor that helps stabilize training. The effective weight update is scaled by `alpha / r`.\n",
    "- `target_modules=[\"q_proj\", \"v_proj\"]`: These are the projection layers in the self-attention mechanism where LoRA is applied.\n",
    "- `bias=\"none\"`: We do not adapt any bias terms to preserve memory efficiency and avoid unintended interactions.\n",
    "- `lora_dropout=0.05`: Introduces mild regularization to prevent overfitting in low-data regimes.\n",
    "- `task_type=\"CAUSAL_LM\"`: Indicates that we are fine-tuning a causal language model (e.g., Mistral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEmfgXmUhsD3"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,        # This is a text generation task\n",
    "    r=8,                                 # Rank of the low-rank matrices\n",
    "    lora_alpha=16,                       # Scaling factor\n",
    "    lora_dropout=0.05,                   # Dropout to prevent overfitting\n",
    "    bias=\"none\",                         # Do not update bias terms\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Modules to apply LoRA on (common in transformers)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHMC6ka1nZQ-"
   },
   "source": [
    "## Step 6: Initialzing Model with qLoRA and PEFT\n",
    "\n",
    "In this section, we prepare our base model for parameter-efficient fine-tuning using the **qLoRA** framework. This allows us to fine-tune large language models (LLMs) in resource-constrained environments by combining two powerful strategies:\n",
    "\n",
    "1. **Quantized Loading (qLoRA)**  \n",
    "   Using the `BitsAndBytesConfig`, we load the base model in **4-bit precision** via the `bitsandbytes` library. This drastically reduces memory usage while preserving performance. We specify:\n",
    "   - `load_in_4bit=True`: Activates 4-bit quantization.\n",
    "   - `bnb_4bit_compute_dtype=torch.float16`: Uses half-precision during computation for faster GPU execution.\n",
    "   - `bnb_4bit_use_double_quant=True`: Applies an extra layer of quantization to improve compression.\n",
    "   - `bnb_4bit_quant_type=\"nf4\"`: Uses NormalFloat-4 (NF4), an information-theoretic quantization format optimized for language models.\n",
    "\n",
    "2. **LoRA Adapter Injection (PEFT)**  \n",
    "   We then apply `get_peft_model()` to the quantized base model with a custom `LoraConfig`, which:\n",
    "   - Inserts low-rank matrices into selected attention layers (here: `q_proj` and `v_proj`)\n",
    "   - Trains only a small number of parameters (∼0.05%), freezing the rest\n",
    "   - Supports causal language modeling (`task_type=\"CAUSAL_LM\"`), suitable for instruction-following tasks\n",
    "\n",
    "> **Why this matters**: This step transforms a massive pretrained model into a lightweight, fine-tunable system without sacrificing generalization. The result is a **LoRA-injected, quantization-aware LLM** ready for efficient training on our domain-specific QA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYv1JBRhhMks"
   },
   "outputs": [],
   "source": [
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AED2CInOo1Cg"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395,
     "referenced_widgets": [
      "0386c159097044369edf0b53e27cbfaf",
      "672123019444447b8783513f5b0ba882",
      "5377da98e798466b96d687a9697c8bf9",
      "698e08e701184b3d901828013130f501",
      "088e0de35bbc422d8cd9c387d24c8a33",
      "8f5484801fd449cdbe6e85ef20de01ab",
      "232d57320e1048a3ae8b646332cfec36",
      "99783ce411314ad19a45922622115613",
      "8a9000b465ba46869abbd89b26a84b41",
      "7b29a3ca520f48aba632323996d77cbe",
      "b32c5871b52f474082bdc6881d3dc43b",
      "552723aaaf7d45649caae4de7d206aef",
      "c83da7b62d104ca4beacc86834de60cf",
      "790967a6963a4b3e91d0f07e1e12aeb4",
      "1bed2119f96e4d97847743fb456fbc25",
      "88952456f9144e949ed8ed40f3eb27c7",
      "fb225cda27a1452f993a00da475fe310",
      "c4b8d2f83d3a48aeac1b4cb5d355a57e",
      "0696588e52cb47309fd1595453aeef90",
      "08a946e28a6b4f56849df97a064b2ab7",
      "3c5f61a62b52441f978e7a98222ebe30",
      "cd0c669e7d684fa2b780b8cbeda1a122",
      "5a2d5ee962ee43aa8a127dc2cd4d9b89",
      "bfffa6af39b84c779ff23eb111140ddb",
      "ef0e19280d4b46c4be871a93587c896f",
      "dd622445d1dd49a7ba6e0285422728f2",
      "5053f500bb194797802bd193d6a57dca",
      "8964ee4b4fde41ce8e924b9afa66fb34",
      "74174cc1494149eb9e7fbaa34fe0ca7b",
      "3e041dae45e2485b8a5fcd0d4cd2520b",
      "33119acf5e424710a66fc9712b15a38a",
      "7c13c858b279490ab3d98d61ae0427c2",
      "3abc9341b4b84ae585b67fcafc9cd6c6",
      "8534e4717eb54ec2bc7280e632a1097f",
      "0f4933c6ac5d43a697696b6086a1df91",
      "e7fed9c467264d8ea662398a06f636a7",
      "0727cc3ea47e45a394bc8096bfb8fc94",
      "544a1a015eca49f1b3e158cda90e6e51",
      "5c0594678b22416291fc125a7621c990",
      "aabe64f5af764e55b849f75c30891cc8",
      "f039c905ca1f4265b27b99fae4de6ac7",
      "f1b873e2a6864c2c88da3ca3e113e662",
      "04304e64e94f4024882c547321272397",
      "43a2f76414954416811caf7cf1a05799",
      "8686b8417d4942468f8add539a05f7c7",
      "803d367461e44a2285c8f8e158c09603",
      "7c625ba1979b4f3483ddf79204838a83",
      "1efecdf092d54958bf6383a6b47808c0",
      "73af03d3393149ceab926274c025a944",
      "1745a249f30445ffba1d6f497005370d",
      "d9fe8b43cb9f41b3b9a9f61a746c66f2",
      "16481fe404d24f79ba65800677830068",
      "942c420ad0314210a393e50ac0aba804",
      "dc3bb58d45ff44ae87d9da7d0ac8536e",
      "e6a9b9d48934440180c433e36e22228a",
      "dd769a2e9512477b92cbeb3de39f48ec",
      "d3dddb46a4644d26b45978cd22578813",
      "b94526a747524878b3658ba384d826a9",
      "6e26c61acc36434f83957547fc4d763f",
      "d68b3681dd544a8c9f777182028d6bd6",
      "00b0ea8b5b6a42c89d8710e175506ffd",
      "ae2a14b4c8394093808254e70af4d4dd",
      "7dd32c40a7264878b6eb9b0431216b7f",
      "0105ad62714f435189b95d796b7b5580",
      "a443dbf17a4f46fbaf5a73f04e2a46d7",
      "b292b5ac52624cd088bbb6f319f48030",
      "d35c529709a84567afb75d49d2f232b6",
      "280b70f2f81640ac95d896c02cefe5d0",
      "55e4a974aedb44a595fe0b48be840b58",
      "aec1e3666ebd45c08e8b867e8bbcd070",
      "404ae00ed8c44e9cacf2327978d5a116",
      "8aba163c560847d7af38d875dec8170f",
      "0f6c03d2be1e44369ce4c9018d0d65ab",
      "58b4da7d242b4e4cbe984d6347ceb0ec",
      "e3bbd875919c4234b1c1ef2ea86cf1f8",
      "156cca51633c428bbf74b3f6f659e74e",
      "8a944de316dc4c70a4783f9ed9e4c073",
      "d5ebc8c48b634513bf368691f6cea88e",
      "fab3fcdfba364376a161d909effaca2d",
      "40bbe8f2786f4c898e999822a18b9fb6",
      "c4a9548f3470448ab449915492012e80",
      "9e215223bee843f48373ac2b8260a793",
      "cc04051b0a2e4dd1b6fd45867a11f948",
      "c1b12f5c6c7c4448b36be7d7e4332d3d",
      "a3d25d67cb4c410e89a100232d8b7fb3",
      "6f043f5d38d542de8f7ff5b37e7d3ddc",
      "9c94d216d0e14d24b4de72d0e5f4cca8",
      "951971c919194cd1939ea42e5fbfd6bb"
     ]
    },
    "executionInfo": {
     "elapsed": 49744,
     "status": "ok",
     "timestamp": 1744264109129,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "u6BjILp4o3at",
    "outputId": "218e60b2-54df-4705-f4e1-e2624b9cd7c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0386c159097044369edf0b53e27cbfaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552723aaaf7d45649caae4de7d206aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2d5ee962ee43aa8a127dc2cd4d9b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8534e4717eb54ec2bc7280e632a1097f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8686b8417d4942468f8add539a05f7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd769a2e9512477b92cbeb3de39f48ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35c529709a84567afb75d49d2f232b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ebc8c48b634513bf368691f6cea88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model with quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically puts layers on the GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgbATzT1o8Zb"
   },
   "outputs": [],
   "source": [
    "# Prepare the model for k-bit training (important!)\n",
    "base_model = prepare_model_for_kbit_training(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QiBV-UxUIQ8"
   },
   "outputs": [],
   "source": [
    "# Inject LoRA adapters into the base model\n",
    "model = get_peft_model(base_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcvCToXD7ykK"
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzHLOCp77sqD"
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1744264115792,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "RFHKPsp3UKrS",
    "outputId": "a60383bd-b16d-47f9-8821-81b090b4c0ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32768, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send the model to the correct device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744264117387,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "6C5ZWgHqUMOo",
    "outputId": "a2fc2404-4c2e-4b8c-8f30-f064e2e8771e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,251,431,424 || trainable%: 0.0470\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTH5_pPlVJ6v"
   },
   "source": [
    "## Step 7: Defining TrainingArguments for Supervised Fine-Tuning\n",
    "\n",
    "Here, we configure the `TrainingArguments` object, which tells the Hugging Face `Trainer` how to perform training. These arguments specify:\n",
    "\n",
    "- The output directory for saving model checkpoints and logs\n",
    "- Batch size per device and gradient accumulation to simulate larger batches\n",
    "- Total number of epochs (passes over the dataset)\n",
    "- Evaluation and logging frequency\n",
    "- Learning rate and scheduler strategy\n",
    "- Mixed precision training (implicitly enabled by bitsandbytes)\n",
    "\n",
    "These are tuned for training with a small dataset using LoRA adapters on a quantized (4-bit) model. All training will be tracked under the `./results/finetuned-mistral` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBfuJLppZBjU"
   },
   "outputs": [],
   "source": [
    "output_dir = \"./results/finetuned-mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-rUsUK1USZx"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=6,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"finetune-mistral-lora\",\n",
    "    bf16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUaTQOawzqZ9"
   },
   "source": [
    "## Step 8: Adding Perplexity as an Evaluation Metric\n",
    "\n",
    "To complement training and validation loss, we log **perplexity**, a widely used metric in language modeling that intuitively reflects how \"surprised\" the model is by the correct output. Lower perplexity indicates better predictive performance.\n",
    "\n",
    "This step defines a `compute_metrics` function, which is passed to the `Trainer` to automatically compute and log perplexity at each evaluation step.\n",
    "\n",
    "Key points:\n",
    "- We use the model's logits and ground truth labels to compute cross-entropy loss.\n",
    "- From the loss, we derive **perplexity** as `exp(loss)`.\n",
    "- This does **not affect training**, and is only run during evaluation.\n",
    "- The results are logged both in the notebook and in **Weights & Biases (wandb)** for real-time visualization.\n",
    "\n",
    "This is a standard best practice in instruction fine-tuning, and is fully compatible with LoRA + 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaxzqFzBz1Er"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred: EvalPrediction) -> Dict:\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    # CrossEntropy loss expects shape (batch_size * seq_len, vocab_size)\n",
    "    # and labels shape (batch_size * seq_len)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1)\n",
    "    )\n",
    "\n",
    "    # Convert to perplexity\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "    return {\n",
    "        \"eval_loss\": loss.item(),\n",
    "        \"eval_perplexity\": perplexity.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDx-VXIdphoi"
   },
   "source": [
    "## Step 9: Initializing the Trainer\n",
    "\n",
    "In this step, we instantiate the HuggingFace `Trainer` class to orchestrate the fine-tuning process. We pass in:\n",
    "\n",
    "- The **LoRA-augmented model**\n",
    "- The **training arguments** (including learning rate, batch size, scheduler, logging, etc.)\n",
    "- Our **tokenized dataset**, with training and validation splits\n",
    "- An **early stopping callback** to terminate training if no improvement is seen for several epochs\n",
    "\n",
    "This setup ensures modular, device-agnostic training with built-in support for checkpointing, evaluation, logging, and optimizer scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1744264123665,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "ktwR1pfoZkp3",
    "outputId": "cd530fde-ae44-47f5-f975-c0639476ddee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=6)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 320715,
     "status": "ok",
     "timestamp": 1744264445399,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "ejNk-MWVqA_P",
    "outputId": "348d8d9a-80a0-4497-9c03-9d266a4310fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/wandb/run-20250410_054844-nlg4ovkm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/huggingface/runs/nlg4ovkm' target=\"_blank\">finetune-mistral-lora</a></strong> to <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/huggingface' target=\"_blank\">https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/huggingface/runs/nlg4ovkm' target=\"_blank\">https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/huggingface/runs/nlg4ovkm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 05:12, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.375700</td>\n",
       "      <td>0.374699</td>\n",
       "      <td>1.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.357300</td>\n",
       "      <td>0.304837</td>\n",
       "      <td>1.356403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.276400</td>\n",
       "      <td>0.290156</td>\n",
       "      <td>1.336635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.286021</td>\n",
       "      <td>1.331119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.285445</td>\n",
       "      <td>1.330353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=78, training_loss=0.45214030681512296, metrics={'train_runtime': 320.6777, 'train_samples_per_second': 3.929, 'train_steps_per_second': 0.243, 'total_flos': 2.5755886798503936e+16, 'train_loss': 0.45214030681512296, 'epoch': 5.60377358490566})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rabbT_3C-LRQ"
   },
   "source": [
    "## Step 10: Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "da54f8cb708f47a38cec8d7ec3d51f00",
      "b02053e310224dc2b58cc87ea26e07ef",
      "720793481fb640018740fa2141775b32",
      "a8a105577a3e444ba69118c4040cc045",
      "3d85db61a7b342cb8e3a144fa8106b63",
      "6775698acfea4f43852cdf50e6099b59",
      "d04f5b6ce0734db88a66c19171ba447d",
      "61235631df8c469d9720749a9d108283",
      "fb53c70dce0a4ab6abedd522f91ad1a2",
      "f63b7699a61e4025b41bc7c747843c64",
      "fcb159f45f764e508b1b8df630e39317",
      "6802c46e12484e628d47f10e1db79fc1",
      "08864ebc9b2440d8920d075287446ed3",
      "e4dce8eeeb924fb0b9779ad1ee0c4490",
      "c93172e533c341de8708425eae72c1b5",
      "df9ec42052624557adbc90f0a918682a",
      "153c5f5242a24b3b8b6b5cb70ab5ae64",
      "213b2de73e3e4410a9fc4b5bd95a3af3",
      "fb5e7b08e73349629315eafbe650abb5",
      "ef97d43a0daf4ac594418017e5975df3",
      "b9e5d255982849b3bbc0b9aec46a8ce9",
      "b951f418bbf24448856f18fa0c6ddd76",
      "612920e3b0e24e70aec625bdb178ffc9",
      "2d67cbe3a5e5404f9d8f884e5b668239",
      "815ff65c37484a1299b52f7b282e0cda",
      "794ab1bcba6d4d1e95f18cb031979e59",
      "0ecb0c65580c4a938c5cfc1eb16c3ae2",
      "b9963a0e0cbe4766b9a62b2b59987575",
      "a5d8226a8da3436989f37041f5196d4a",
      "25a092cca1f64a9db44a2444d4af43c0",
      "43275b83f89040dca5aa8dbfb32c7191",
      "4ac48769da9a4f408ed23384946b0d43",
      "86bb9f338e714b33afb17137024b30d7",
      "da81025f9e9243bdbb5ffe583f8cf612",
      "0465532237c54fbe8e7b9e239206cd8f",
      "ae080bf647b24ebcabfd678cce87447a",
      "2ebb5804d49d4d69985de4340c25d138",
      "9e09ec98c9f14b929b8fb009190d3b15",
      "6f82b28ccd9340ed8832c2e4b977109c",
      "8891c91113a644c8b03ddc6d87c5b2ce",
      "d1880b46fcf64facac6af0c6d0efe056",
      "781b656304ea4e5e9af3e9cf0b528734",
      "864fdcfc13254093a0c5b07d6e9d4ea1",
      "1975d0bd74ef4e1e973b871563237f5b"
     ]
    },
    "executionInfo": {
     "elapsed": 1395,
     "status": "ok",
     "timestamp": 1744264451146,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "o6HtOOSs-0i8",
    "outputId": "f4e5cec2-354a-48ee-94bb-6b6bd668563b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da54f8cb708f47a38cec8d7ec3d51f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6802c46e12484e628d47f10e1db79fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612920e3b0e24e70aec625bdb178ffc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da81025f9e9243bdbb5ffe583f8cf612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3607,
     "status": "ok",
     "timestamp": 1744264455115,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "Nnq0dUa1-J1g",
    "outputId": "79906ff3-156e-478e-aad5-5c4a68545ed4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/finetuned-mistral/tokenizer_config.json',\n",
       " './models/finetuned-mistral/special_tokens_map.json',\n",
       " './models/finetuned-mistral/tokenizer.model',\n",
       " './models/finetuned-mistral/added_tokens.json',\n",
       " './models/finetuned-mistral/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./models/finetuned-mistral\"\n",
    "\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLA8ikR-2iNN"
   },
   "source": [
    "## Step 11: Running Inference to Verify Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4587,
     "status": "ok",
     "timestamp": 1744264463357,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "m5xHGtVLrxTH",
    "outputId": "ee660130-6e4c-4702-f03e-bd4ad48a222b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "What is the benefit of using LoRA for fine-tuning large language models?\n",
      "\n",
      "### Answer:\n",
      "LoRA enables selective and sparse fine-tuning, allowing updates to be focused on specific task-relevant dimensions, reducing computational requirements and memory footprint, especially for tasks with limited data. Additionally, it maintains a consistent behavioral profile, preserving model integrity.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)  # No device here!\n",
    "\n",
    "instruction = \"### Question:\\nWhat is the benefit of using LoRA for fine-tuning large language models?\\n\\n### Answer:\\n\"\n",
    "output = pipe(instruction, max_new_tokens=150, do_sample=True, temperature=0.7)\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2614,
     "status": "ok",
     "timestamp": 1744266203814,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "7iUt2LjSS3aM"
   },
   "outputs": [],
   "source": [
    "pip install nbformat --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_imYCsAXkBq"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 193,
     "status": "error",
     "timestamp": 1744266209536,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "drzmX_BXWk_a",
    "outputId": "5d706f53-fd9e-46aa-df18-4892e45e2933"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/notebooks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d1785c2c6ea7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# List the notebook directory to confirm the file exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/notebooks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/notebooks'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# List the notebook directory to confirm the file exists\n",
    "os.listdir(\"/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "executionInfo": {
     "elapsed": 938,
     "status": "error",
     "timestamp": 1744265996002,
     "user": {
      "displayName": "Samyak Shrestha (Caesar)",
      "userId": "13083503381857072620"
     },
     "user_tz": 300
    },
    "id": "wIRSfrlOVBaO",
    "outputId": "af9a10aa-80c1-4d1f-c968-65b7a72ca411"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-60fe0b1126c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebook_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"widgets\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: PTH123\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = \"/content/drive/MyDrive/llm-finetuning-project/llm-finetuning-summarizer/notebooks/06_finetuning.ipynb\"\n",
    "\n",
    "with open(notebook_path, \"r\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "with open(notebook_path, \"w\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Notebook fixed and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnOsppvbWkBA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3gX4EYbVUQM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM+ERU2uHQoCTX8qCeq/qFA",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
