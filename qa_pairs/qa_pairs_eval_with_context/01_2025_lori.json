{
    "paper_id": "01_2025_lori",
    "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation",
    "qa_pairs": [
        {
            "question": "What is the primary innovation introduced by the LoRI method for parameter-efficient fine-tuning?",
            "answer": "LoRI introduces a novel approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks, thereby significantly reducing trainable parameters while minimizing cross-task interference.",
            "context": "Abstract:\n\nLow-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n\nIntroduction:\n\nLarge language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n\nRecent studies have shown that delta parameters \u2013 the differences between fine-tuned and pretrained model weights \u2013 exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model\u2019s knowledge during adaptation.\n\nMulti-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n\nBeyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n\nTo evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to \u2013 or better than \u2013 full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n\n\nConclusion:\n\nIn this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains \u2013 including natural language understanding, mathematical reasoning, code generation, and safety alignment \u2013 while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n\nFuture Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns \u2013 such as block sparsity, head pruning, or group-wise masking \u2013 which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning."
        },
        {
            "question": "How does LoRI reduce the number of trainable parameters compared to traditional LoRA?",
            "answer": "LoRI reduces the number of trainable parameters by keeping matrix A fixed as a random projection and sparsifying matrix B using task-specific masks, eliminating the need to train both matrices and reducing redundancy.",
            "context": "Abstract:\n\nLow-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n\nIntroduction:\n\nLarge language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n\nRecent studies have shown that delta parameters \u2013 the differences between fine-tuned and pretrained model weights \u2013 exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model\u2019s knowledge during adaptation.\n\nMulti-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n\nBeyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n\nTo evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to \u2013 or better than \u2013 full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n\n\nConclusion:\n\nIn this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains \u2013 including natural language understanding, mathematical reasoning, code generation, and safety alignment \u2013 while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n\nFuture Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns \u2013 such as block sparsity, head pruning, or group-wise masking \u2013 which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning."
        },
        {
            "question": "Why is sparsity in matrix B important in LoRI?",
            "answer": "Sparsity in matrix B enables LoRI to retain only the most critical elements necessary for adaptation, reducing parameter count and mitigating cross-task interference during adapter merging and continual learning.",
            "context": "Abstract:\n\nLow-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n\nIntroduction:\n\nLarge language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n\nRecent studies have shown that delta parameters \u2013 the differences between fine-tuned and pretrained model weights \u2013 exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model\u2019s knowledge during adaptation.\n\nMulti-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n\nBeyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n\nTo evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to \u2013 or better than \u2013 full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n\n\nConclusion:\n\nIn this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains \u2013 including natural language understanding, mathematical reasoning, code generation, and safety alignment \u2013 while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n\nFuture Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns \u2013 such as block sparsity, head pruning, or group-wise masking \u2013 which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning."
        },
        {
            "question": "How does LoRI improve the process of merging adapters in multi-task scenarios?",
            "answer": "LoRI enables more effective adapter merging by using fixed, randomly initialized projection matrices A, which maps task-specific adapters into approximately orthogonal subspaces, thus reducing parameter interference.",
            "context": "Abstract:\n\nLow-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n\nIntroduction:\n\nLarge language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n\nRecent studies have shown that delta parameters \u2013 the differences between fine-tuned and pretrained model weights \u2013 exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model\u2019s knowledge during adaptation.\n\nMulti-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n\nBeyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n\nTo evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to \u2013 or better than \u2013 full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n\n\nConclusion:\n\nIn this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains \u2013 including natural language understanding, mathematical reasoning, code generation, and safety alignment \u2013 while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n\nFuture Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns \u2013 such as block sparsity, head pruning, or group-wise masking \u2013 which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning."
        },
        {
            "question": "What mechanism does LoRI use to mitigate catastrophic forgetting in continual learning?",
            "answer": "LoRI mitigates catastrophic forgetting by applying task-specific sparse masks to matrix B, which isolates parameter updates across tasks and preserves knowledge from previous adaptations, including safety alignment.",
            "context": "Abstract:\n\nLow-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n\nIntroduction:\n\nLarge language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n\nRecent studies have shown that delta parameters \u2013 the differences between fine-tuned and pretrained model weights \u2013 exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model\u2019s knowledge during adaptation.\n\nMulti-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n\nBeyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n\nTo evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to \u2013 or better than \u2013 full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n\n\nConclusion:\n\nIn this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains \u2013 including natural language understanding, mathematical reasoning, code generation, and safety alignment \u2013 while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n\nFuture Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns \u2013 such as block sparsity, head pruning, or group-wise masking \u2013 which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning."
        },
        {
            "question": "On what benchmark did LoRI with 90% sparsity in B outperform LoRA, and by how much?",
            "answer": "LoRI with 90% sparsity in B outperformed LoRA by 17.3% on the HumanEval benchmark using the Llama-3 model.",
            "context": "Abstract:\n\nLow-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n\nIntroduction:\n\nLarge language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n\nRecent studies have shown that delta parameters \u2013 the differences between fine-tuned and pretrained model weights \u2013 exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model\u2019s knowledge during adaptation.\n\nMulti-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n\nBeyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n\nTo evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to \u2013 or better than \u2013 full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n\n\nConclusion:\n\nIn this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains \u2013 including natural language understanding, mathematical reasoning, code generation, and safety alignment \u2013 while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n\nFuture Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns \u2013 such as block sparsity, head pruning, or group-wise masking \u2013 which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning."
        },
        {
            "question": "How does LoRI compare to full fine-tuning and other PEFT methods in terms of performance and efficiency?",
            "answer": "LoRI matches or outperforms full fine-tuning and other PEFT methods across multiple domains while using up to 95% fewer trainable parameters than LoRA, demonstrating both high performance and high efficiency.",
            "context": "Abstract:\n\nLow-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n\nIntroduction:\n\nLarge language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n\nRecent studies have shown that delta parameters \u2013 the differences between fine-tuned and pretrained model weights \u2013 exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model\u2019s knowledge during adaptation.\n\nMulti-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n\nBeyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n\nTo evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to \u2013 or better than \u2013 full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n\n\nConclusion:\n\nIn this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains \u2013 including natural language understanding, mathematical reasoning, code generation, and safety alignment \u2013 while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n\nFuture Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns \u2013 such as block sparsity, head pruning, or group-wise masking \u2013 which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning."
        },
        {
            "question": "What types of tasks were used to evaluate LoRI's effectiveness?",
            "answer": "LoRI was evaluated on a diverse set of tasks, including natural language understanding, mathematical reasoning, code generation, and safety alignment.",
            "context": "Abstract:\n\nLow-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n\nIntroduction:\n\nLarge language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n\nRecent studies have shown that delta parameters \u2013 the differences between fine-tuned and pretrained model weights \u2013 exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model\u2019s knowledge during adaptation.\n\nMulti-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n\nBeyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n\nTo evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to \u2013 or better than \u2013 full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n\n\nConclusion:\n\nIn this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains \u2013 including natural language understanding, mathematical reasoning, code generation, and safety alignment \u2013 while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n\nFuture Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns \u2013 such as block sparsity, head pruning, or group-wise masking \u2013 which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning."
        },
        {
            "question": "What potential future directions do the authors propose for extending LoRI?",
            "answer": "The authors suggest exploring structured sparsity patterns like block sparsity or head pruning and adapting LoRI to multi-modal models such as diffusion and vision-language systems.",
            "context": "Abstract:\n\nLow-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n\nIntroduction:\n\nLarge language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n\nRecent studies have shown that delta parameters \u2013 the differences between fine-tuned and pretrained model weights \u2013 exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model\u2019s knowledge during adaptation.\n\nMulti-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n\nBeyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n\nTo evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to \u2013 or better than \u2013 full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n\n\nConclusion:\n\nIn this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains \u2013 including natural language understanding, mathematical reasoning, code generation, and safety alignment \u2013 while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n\nFuture Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns \u2013 such as block sparsity, head pruning, or group-wise masking \u2013 which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning."
        },
        {
            "question": "What is the broader significance of LoRI in the context of PEFT and LLM deployment?",
            "answer": "LoRI provides a lightweight, modular, and scalable solution for adapting LLMs with minimal overhead, making it particularly suited for multi-task learning, safety-critical alignment, and efficient deployment on resource-constrained hardware.",
            "context": "Abstract:\n\nLow-Rank Adaptation (LoRA) has emerged as a popular parameter- efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices A as random projections and sparsifies the matrices B using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference.\n\nIntroduction:\n\nLarge language models (LLMs) have transformed deep learning, showcasing remarkable capabilities across various domains. However, their deployment remains computationally demanding, particularly when fine-tuning is required to adapt to downstream tasks or align with human preferences. To mitigate the high resource costs, researchers have developed a range of parameter-efficient fine-tuning (PEFT) techniques. Among these techniques, LoRA has gained widespread adoption. Nevertheless, LoRA still introduces notable memory overhead, particularly in large-scale models. Consequently, recent research has focused on further optimizing LoRA by reducing the number of trainable parameters without compromising performance.\n\nRecent studies have shown that delta parameters \u2013 the differences between fine-tuned and pretrained model weights \u2013 exhibit significant redundancy. Motivated by the effectiveness of random projections and the observed redundancy in delta parameters, we propose LoRA with Reduced Interference (LoRI). LoRI keeps the low-rank matrices A fixed as random projections, while training the matrices B using task-specific sparse masks. To retain the most critical elements of B, LoRI performs a calibration process to extract sparse masks by selecting the highest-magnitude elements across all layers and projections. As shown in Figure 1(a), LoRI maintains performance even with 90% sparsity in B while keeping A frozen. This demonstrates that adaptation does not require updating A, and that B has considerable redundancy. By applying more constrained updates than LoRA, LoRI significantly reduces the number of trainable parameters while better preserving the pretrained model\u2019s knowledge during adaptation.\n\nMulti-task learning is essential for enabling versatile models with multi-task capabilities, which is traditionally performed via joint training on a combination of task-specific datasets. However, training large models on this data mixture is prohibitively expensive in terms of time and compute. Model merging is a training-free alternative for building powerful models by combining existing ones. This approach is well-suited for merging LoRA adapters to enable multi-task capabilities within a single LoRA. However, as shown in Figure 1(b), directly merging heterogeneous LoRAs often results in parameter interference, leading to degraded performance in the merged LoRA compared to single-task LoRAs. Additionally, many existing merging methods require trial-and-error to identify the optimal method for a specific combination of tasks. LoRI tackles these challenges by enabling adapter merging without manual selection of merging methods. By using fixed, randomly initialized projection A, LoRI maps task-specific adapters into approximately orthogonal subspaces, thereby reducing interference when merging multiple LoRIs.\n\nBeyond multi-tasking, safety-critical scenarios require that each newly introduced adapter enhances model capabilities while preserving the safety alignment of the pretrained base model. LoRI provides a lightweight continual learning approach for adapting models while preserving safety, where training is performed sequentially across tasks. The strategy involves first fine-tuning an adapter on safety data to establish alignment, followed by separate adaptation to each downstream task. However, as illustrated in Figure 1(c), continual learning often leads to catastrophic forgetting, wherein the adaptation to new tasks substantially compromises previously acquired knowledge. LoRI mitigates forgetting by leveraging the sparsity of matrices B through task-specific masks. This isolation of parameter updates across tasks facilitates continual learning with minimal interference, preserving both safety and task effectiveness.\n\nTo evaluate the effectiveness of LoRI, we conduct extensive experiments across a diverse suite of benchmarks spanning natural language understanding (NLU), mathematical reasoning, code generation, and safety alignment tasks. Using Llama-3-8B and Mistral-7B as base models, our results show that LoRI achieves performance comparable to \u2013 or better than \u2013 full fine-tuning (FFT), LoRA, and other PEFT methods, while using up to 95% fewer trainable parameters than LoRA. Notably, LoRI with 90% sparsity in B surpasses LoRA by 17.3% on HumanEval with Llama-3. Beyond single-task adaptation, we evaluate LoRI in multi-task settings, including adapter merging and continual learning scenarios. Concatenated merging of LoRI adapters consistently outperforms LoRA adapters overall, closely matching the performance of single-task LoRA baseline. In continual learning, LoRI significantly outperforms LoRA in mitigating catastrophic forgetting of safety alignment, while maintaining strong performance on downstream tasks.\n\n\nConclusion:\n\nIn this work, we introduced LoRI, a simple yet effective approach to parameter-efficient fine-tuning (PEFT) that substantially reduces trainable parameters while minimizing cross-task interference. By freezing the projection matrices A as random projections and sparsifying B using task-specific masks, LoRI achieves strong single-task performance across diverse domains \u2013 including natural language understanding, mathematical reasoning, code generation, and safety alignment \u2013 while reducing trainable parameters by up to 95% compared to LoRA. Furthermore, LoRI enables training-free adapter merging with minimal performance degradation, and supports continual learning with significantly reduced catastrophic forgetting. It also provides a lightweight approach to building safety adapters that preserve the safety alignment of the base model.\n\nFuture Work. We identify several promising avenues for extending this work. While LoRI currently leverages unstructured magnitude-based sparsity, future research can explore structured sparsity patterns \u2013 such as block sparsity, head pruning, or group-wise masking \u2013 which may offer better hardware compatibility. Additionally, although this study focuses on LLMs, the core design of LoRI is modality-agnostic. Extending LoRI to diffusion and vision-language models for multi-modal generation is a promising direction, given the growing impact of adapter-based fine-tuning."
        }
    ]
}